{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b3d3f1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-13 13:14:50.213046: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-05-13 13:14:50.214206: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-05-13 13:14:50.238608: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-05-13 13:14:50.239031: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-05-13 13:14:50.621665: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "/usr/lib/python3/dist-packages/requests/__init__.py:89: RequestsDependencyWarning: urllib3 (2.2.1) or chardet (5.2.0) doesn't match a supported version!\n",
      "  warnings.warn(\"urllib3 ({}) or chardet ({}) doesn't match a supported \"\n",
      "2024-05-13 13:14:51.613310: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-05-13 13:14:51.614559: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1960] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/mreza/.local/lib/python3.8/site-packages/tensorflow/python/util/deprecation.py:648: calling map_fn_v2 (from tensorflow.python.ops.map_fn) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use fn_output_signature instead\n",
      "File extension detected: \n",
      "File extension detected: \n",
      "Epoch 1/4\n",
      "File extension detected: \n",
      "Step 1/2500, Loss: 0.9999650120735168\n",
      "Step 101/2500, Loss: 0.05711154267191887\n",
      "Step 201/2500, Loss: 0.879167914390564\n",
      "Step 301/2500, Loss: 0.0753590390086174\n",
      "Step 401/2500, Loss: 0.7836889028549194\n",
      "Step 501/2500, Loss: 0.8054177165031433\n",
      "Step 601/2500, Loss: 0.1198170855641365\n",
      "Step 701/2500, Loss: 0.7977041602134705\n",
      "Step 801/2500, Loss: 0.8363164067268372\n",
      "Step 901/2500, Loss: 0.027730371803045273\n",
      "Step 1001/2500, Loss: 0.7641609907150269\n",
      "Step 1101/2500, Loss: 0.8026394844055176\n",
      "Step 1201/2500, Loss: 0.019849468022584915\n",
      "Step 1301/2500, Loss: 0.713286817073822\n",
      "Step 1401/2500, Loss: 0.06444819271564484\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import LSTM, Dense, RepeatVector, TimeDistributed, Layer, MultiHeadAttention\n",
    "from tensorflow.keras.layers import Bidirectional, Dropout\n",
    "from tensorflow.keras.layers import Masking, Input, Lambda\n",
    "import tensorflow_probability as tfp\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.losses import mse\n",
    "from numpy.fft import fft\n",
    "from scipy.stats import skew, kurtosis \n",
    "import pandas as pd\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import struct\n",
    "import glob\n",
    "from sklearn.metrics import confusion_matrix, roc_auc_score, roc_curve, classification_report, accuracy_score\n",
    "\n",
    "# Custom loss function\n",
    "def custom_loss(y_true, y_pred):\n",
    "    mse = tf.reduce_mean(tf.square(y_true - y_pred))\n",
    "    y_pred_clipped = tf.clip_by_value(y_pred, 1e-9, 1.0)  # Clip values to avoid log(0)\n",
    "    entropy_reg = -tf.reduce_mean(y_pred_clipped * tf.math.log(y_pred_clipped))\n",
    "    lambda_entropy = 0.01\n",
    "    return mse + lambda_entropy * entropy_reg\n",
    "\n",
    "def count_lines(filepath):\n",
    "    with open(filepath, 'rb') as f:\n",
    "        return sum(1 for _ in f)\n",
    "\n",
    "class DataGenerator:        \n",
    "    def __init__(self, filepath, batch_size, sequence_length, max_samples=None, for_training=True):\n",
    "        self.filepath = filepath\n",
    "        self.batch_size = batch_size\n",
    "        self.sequence_length = sequence_length\n",
    "        self.max_samples = max_samples\n",
    "        self.for_training = for_training\n",
    "        self.samples = []\n",
    "        self.binary_file = open(self.filepath, 'rb')  # Initialize the binary_file here\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.total_samples_processed = 0\n",
    "        _, self.file_extension = os.path.splitext(self.filepath)\n",
    "        print(f\"File extension detected: {self.file_extension}\")  # Add this line\n",
    "\n",
    "\n",
    "    def __iter__(self):\n",
    "        self.binary_file.seek(0)  # reset file pointer\n",
    "        self.samples = []\n",
    "        return self\n",
    "    \n",
    "    def close(self):\n",
    "        if not self.binary_file.closed:\n",
    "            self.binary_file.close()\n",
    "\n",
    "    def process_data1(self, samples):\n",
    "        real_parts = []\n",
    "        imag_parts = []\n",
    "        for sample in samples:\n",
    "            try:\n",
    "                cnum = complex(sample.replace('j', 'j'))\n",
    "                real_parts.append(np.real(cnum))\n",
    "                imag_parts.append(np.imag(cnum))\n",
    "            except ValueError:\n",
    "                continue\n",
    "\n",
    "        real_parts = (real_parts - np.mean(real_parts)) / np.std(real_parts)\n",
    "        imag_parts = (imag_parts - np.mean(imag_parts)) / np.std(imag_parts)\n",
    "\n",
    "        X = [list(zip(real_parts[i:i+self.sequence_length], imag_parts[i:i+self.sequence_length])) for i in range(len(real_parts) - self.sequence_length)]\n",
    "        return np.array(X)\n",
    "    def process_data2(self, samples):\n",
    "        # Convert samples list to a NumPy array and check the total number of samples\n",
    "        samples_array = np.array(samples, dtype=np.complex64)\n",
    "        total_samples = samples_array.size\n",
    "\n",
    "        # Ensure that the total number of samples matches self.batch_size * self.sequence_length\n",
    "        if total_samples != self.batch_size * self.sequence_length:\n",
    "            # Handle this scenario: you might want to raise an error or handle it in some way\n",
    "            raise ValueError(\"Total number of samples does not match batch_size * sequence_length\")\n",
    "\n",
    "        # Check for invalid values in samples_array before processing\n",
    "        if np.isnan(samples_array).any() or np.isinf(samples_array).any():\n",
    "            print(f\"Invalid values found in samples_array: {samples_array}\")\n",
    "        # Reshape the samples array\n",
    "        samples_array = samples_array.reshape(self.batch_size, self.sequence_length)\n",
    "        print('samples_array.shape:', samples_array.shape)\n",
    "\n",
    "        # Apply FFT to convert time-domain signals into frequency domain\n",
    "        samples_fft = fft(samples_array)\n",
    "        print('samples_fft.shape:', samples_fft.shape)\n",
    "\n",
    "        # Extract real and imaginary parts\n",
    "        real_parts = np.real(samples_fft)\n",
    "        imag_parts = np.imag(samples_fft)\n",
    "\n",
    "#         # Normalize the real and imaginary parts\n",
    "#         real_parts = (real_parts - np.mean(real_parts, axis=1, keepdims=True)) / np.std(real_parts, axis=1, keepdims=True)\n",
    "#         imag_parts = (imag_parts - np.mean(imag_parts, axis=1, keepdims=True)) / np.std(imag_parts, axis=1, keepdims=True)\n",
    "        \n",
    "        # Normalize the real and imaginary parts\n",
    "        epsilon = 1e-10\n",
    "        real_parts_mean = np.mean(real_parts, axis=1, keepdims=True)\n",
    "        real_parts_std = np.std(real_parts, axis=1, keepdims=True)\n",
    "        real_parts_std[real_parts_std == 0] = epsilon  # Avoid division by zero\n",
    "        real_parts = (real_parts - real_parts_mean) / real_parts_std\n",
    "\n",
    "        imag_parts_mean = np.mean(imag_parts, axis=1, keepdims=True)\n",
    "        imag_parts_std = np.std(imag_parts, axis=1, keepdims=True)\n",
    "        imag_parts_std[imag_parts_std == 0] = epsilon  # Avoid division by zero\n",
    "        imag_parts = (imag_parts - imag_parts_mean) / imag_parts_std\n",
    "\n",
    "        # Extract statistical features from the real and imaginary parts\n",
    "        features = np.column_stack((\n",
    "            np.mean(real_parts, axis=1),\n",
    "            np.std(real_parts, axis=1),\n",
    "            skew(real_parts, axis=1),\n",
    "            kurtosis(real_parts, axis=1),\n",
    "            np.mean(imag_parts, axis=1),\n",
    "            np.std(imag_parts, axis=1),\n",
    "            skew(imag_parts, axis=1),\n",
    "            kurtosis(imag_parts, axis=1)\n",
    "        ))\n",
    "\n",
    "        # Reshape features to match the input shape of the model\n",
    "        X = features.reshape(-1, self.sequence_length, features.shape[1])\n",
    "        return X\n",
    "\n",
    "    def __next__(self):\n",
    "        chunksize = self.batch_size * self.sequence_length\n",
    "        global totalMagnitude  # Access the global variable\n",
    "        global totalnumberofsamples  # Access the global variable\n",
    "        \n",
    "        #if self.file_extension == '.dat':        \n",
    "        samples = []\n",
    "        while True:\n",
    "            binary_data = self.binary_file.read(8)\n",
    "            if not binary_data:\n",
    "                break  # End of file\n",
    "            decoded_data = struct.unpack('ff', binary_data)\n",
    "            \n",
    "            # Skip samples that are exactly zero (0 + 0j)\n",
    "            if decoded_data[0] == 0 and decoded_data[1] == 0:\n",
    "                continue\n",
    "\n",
    "            # Convert the binary data to a complex number string\n",
    "            decoded_line = f\"{decoded_data[0]}+{decoded_data[1]}j\\n\" if decoded_data[1] >= 0 else f\"{decoded_data[0]}{decoded_data[1]}j\\n\"\n",
    "            samples.append(decoded_line)\n",
    "\n",
    "            # Check if we have enough samples for a batch\n",
    "            if len(samples) == chunksize:\n",
    "                X_chunk = self.process_data1(samples)\n",
    "                #X_chunk = self.process_data2(samples)\n",
    "                if self.for_training:\n",
    "                    return X_chunk, X_chunk\n",
    "                else:\n",
    "                    return X_chunk\n",
    "                # Clear samples for the next batch (optional, depends on your logic)\n",
    "                samples = []\n",
    "#------------------------------------------------------------------------------------------------------\n",
    "# Minimum Entropy Coupling (MEC) Functions\n",
    "# def mec_kocaoglu_np(p, q):\n",
    "#     \"\"\"\n",
    "#     Compute the joint distribution matrix with minimal entropy between two given distributions.\n",
    "#     \"\"\"\n",
    "#     p = tf.cast(p, tf.float64) / tf.reduce_sum(p)\n",
    "#     q = tf.cast(q, tf.float64) / tf.reduce_sum(q)\n",
    "#     J = tf.zeros((tf.size(q), tf.size(p)), dtype=tf.float64)\n",
    "#     M = tf.stack([p, q], axis=0)\n",
    "#     r = tf.reduce_min(tf.reduce_max(M, axis=1))\n",
    "#     #print('Input shapes to mec_kocaoglu_np:', p.shape, q.shape)\n",
    "#     def body(r, M, J):\n",
    "#         a_i = tf.argmax(M, axis=1)\n",
    "#         r_updated = tf.reduce_min(tf.reduce_max(M, axis=1))\n",
    "#         update_values = tf.stack([r, r])\n",
    "#         # ensure tensors have same datatype\n",
    "#         a_i = tf.cast(a_i, dtype=tf.int32)\n",
    "#         indices_range = tf.range(tf.size(a_i), dtype=tf.int32)\n",
    "#         #prepare indices for scatter update\n",
    "#         indices = tf.stack([indices_range, a_i], axis=1)\n",
    "#         #now update\n",
    "#         M_updates = tf.scatter_nd(indices, -update_values, tf.shape(M))\n",
    "#         M = M + M_updates\n",
    "#         J_updates = tf.scatter_nd(indices, [r, r], tf.shape(J))\n",
    "#         J = J + J_updates\n",
    "#         return r_updated, M, J\n",
    "\n",
    "#     def condition(r, M, J):\n",
    "#         return r > 0\n",
    "#     r, M, J = tf.while_loop(condition, body, loop_vars=[r, M, J])\n",
    "#     return J\n",
    "\n",
    "# def apply_mec_to_data(data, num_bins=25, latent_dim=25):\n",
    "#     print('data.shape in apply mec:', data.shape)\n",
    "#     \"\"\"\n",
    "#     Apply the MEC transformation to each sample in the data using tf.map_fn.\n",
    "#     \"\"\"\n",
    "#     def process_sample(sample):\n",
    "#         min_val = tf.reduce_min(sample)\n",
    "#         max_val = tf.reduce_max(sample)\n",
    "#         sample_distribution = tf.histogram_fixed_width(sample, [min_val, max_val], nbins=num_bins)\n",
    "#         sample_distribution = tf.cast(sample_distribution, tf.float64)\n",
    "#         sum_distribution = tf.cast(tf.reduce_sum(sample_distribution), tf.float64)\n",
    "#         sample_distribution /= sum_distribution\n",
    "\n",
    "#         mec_transformed = mec_kocaoglu_np(sample_distribution, sample_distribution)\n",
    "\n",
    "#         # Flatten the 2D to 1D\n",
    "#         if len(mec_transformed.shape) > 1:\n",
    "#             transformed_sample = tf.reshape(mec_transformed, [-1])\n",
    "\n",
    "#         # slice/pad to match the latent_dim\n",
    "#         if transformed_sample.shape[0] > latent_dim:\n",
    "#             transformed_sample = transformed_sample[:latent_dim]\n",
    "#         elif transformed_sample.shape[0] < latent_dim:\n",
    "#             padding = tf.zeros(latent_dim - transformed_sample.shape[0], dtype=tf.float64)\n",
    "#             transformed_sample = tf.concat([transformed_sample, padding], axis=0)\n",
    "\n",
    "#         return tf.reshape(transformed_sample, (latent_dim,))\n",
    "#     # apply function to each sample in the batch\n",
    "#     transformed_batch = tf.map_fn(process_sample, data, dtype=tf.float64, parallel_iterations=10)\n",
    "\n",
    "#     return transformed_batch\n",
    "def mec_kocaoglu_np(p, q):\n",
    "    \"\"\"\n",
    "    Compute the joint distribution matrix with minimal entropy between two given distributions.\n",
    "    \"\"\"\n",
    "    p = tf.cast(p, tf.float64) / tf.reduce_sum(p)\n",
    "    q = tf.cast(q, tf.float64) / tf.reduce_sum(q)\n",
    "    J = tf.zeros((tf.size(q), tf.size(p)), dtype=tf.float64)\n",
    "    M = tf.stack([p, q], axis=0)\n",
    "    r = tf.reduce_min(tf.reduce_max(M, axis=1))\n",
    "    #print('Input shapes to mec_kocaoglu_np:', p.shape, q.shape)\n",
    "    def body(r, M, J):\n",
    "        a_i = tf.argmax(M, axis=1)\n",
    "        r_updated = tf.reduce_min(tf.reduce_max(M, axis=1))\n",
    "        update_values = tf.stack([r, r])\n",
    "        # ensure tensors have same datatype\n",
    "        a_i = tf.cast(a_i, dtype=tf.int32)\n",
    "        indices_range = tf.range(tf.size(a_i), dtype=tf.int32)\n",
    "        #prepare indices for scatter update\n",
    "        indices = tf.stack([indices_range, a_i], axis=1)\n",
    "        #now update\n",
    "        M_updates = tf.scatter_nd(indices, -update_values, tf.shape(M))\n",
    "        M = M + M_updates\n",
    "        J_updates = tf.scatter_nd(indices, [r, r], tf.shape(J))\n",
    "        J = J + J_updates\n",
    "        return r_updated, M, J\n",
    "\n",
    "    def condition(r, M, J):\n",
    "        return r > 0\n",
    "    r, M, J = tf.while_loop(condition, body, loop_vars=[r, M, J])\n",
    "    return J\n",
    "\n",
    "def apply_mec_to_data(z_mean, z, num_bins=25, latent_dim=25):\n",
    "    \"\"\"\n",
    "    Apply the MEC transformation to each pair of samples in z_mean and z using tf.map_fn.\n",
    "    \"\"\"\n",
    "    def process_pair(pair):\n",
    "        #z_mean_sample, z_sample = pair\n",
    "        z_mean_sample = pair[0]\n",
    "        z_sample = pair[1]\n",
    "        \n",
    "        # Dynamically calculate the range and number of bins\n",
    "        min_val = tf.reduce_min([z_mean_sample, z_sample])\n",
    "        max_val = tf.reduce_max([z_mean_sample, z_sample])\n",
    "        num_bins_dynamic = tf.cast(tf.sqrt(tf.size(z_mean_sample, out_type=tf.float32)), tf.int32)\n",
    "\n",
    "        # Calculate histogram for z_mean_sample\n",
    "        min_val_z_mean = tf.reduce_min(z_mean_sample)\n",
    "        max_val_z_mean = tf.reduce_max(z_mean_sample)\n",
    "        z_mean_distribution = tf.histogram_fixed_width(z_mean_sample, [min_val_z_mean, max_val_z_mean], nbins=num_bins)\n",
    "        #z_mean_distribution = tf.histogram_fixed_width(z_mean_sample, [min_val, max_val], nbins=num_bins_dynamic)\n",
    "        z_mean_distribution = tf.cast(z_mean_distribution, tf.float64)\n",
    "        z_mean_distribution = tf.cast(z_mean_distribution, tf.float64) / tf.reduce_sum(z_mean_distribution)\n",
    "\n",
    "        # Calculate histogram for z_sample\n",
    "        min_val_z = tf.reduce_min(z_sample)\n",
    "        max_val_z = tf.reduce_max(z_sample)\n",
    "        z_distribution = tf.histogram_fixed_width(z_sample, [min_val_z, max_val_z], nbins=num_bins)\n",
    "        #z_distribution = tf.histogram_fixed_width(z_sample, [min_val, max_val], nbins=num_bins_dynamic)\n",
    "        z_distribution = tf.cast(z_distribution, tf.float64)\n",
    "        z_distribution = tf.cast(z_sample, tf.float64)\n",
    "        z_distribution = tf.cast(z_distribution, tf.float64) / tf.reduce_sum(z_distribution)\n",
    "\n",
    "        # Apply MEC using the distributions\n",
    "        mec_transformed = mec_kocaoglu_np(z_mean_distribution, z_distribution)\n",
    "\n",
    "        # Flatten the 2D to 1D\n",
    "        if len(mec_transformed.shape) > 1:\n",
    "            transformed_pair = tf.reshape(mec_transformed, [-1])\n",
    "\n",
    "        # Slice/pad to match the latent_dim\n",
    "        if transformed_pair.shape[0] > latent_dim:\n",
    "            transformed_pair = transformed_pair[:latent_dim]\n",
    "        elif transformed_pair.shape[0] < latent_dim:\n",
    "            padding = tf.zeros(latent_dim - transformed_pair.shape[0], dtype=tf.float64)\n",
    "            transformed_pair = tf.concat([transformed_pair, padding], axis=0)\n",
    "\n",
    "        return tf.reshape(transformed_pair, (latent_dim,))\n",
    "\n",
    "    # Stack z_mean and z to create pairs for map_fn\n",
    "    pairs = tf.stack([z_mean, z], axis=1)\n",
    "    # Apply function to each pair in the batch\n",
    "    transformed_batch = tf.map_fn(process_pair, pairs, dtype=tf.float64, parallel_iterations=10)\n",
    "    return transformed_batch\n",
    "\n",
    "def process_latent_variables(z_mean, z):\n",
    "    z_transformed = apply_mec_to_data(z_mean, z)\n",
    "    #print('Output of MEC transformation shape:', z_transformed.shape)\n",
    "    return z_transformed\n",
    "\n",
    "# def process_latent_variables(z):\n",
    "#     z_transformed = apply_mec_to_data(z)\n",
    "#     #print('Output of MEC transformation shape:', z_transformed.shape)\n",
    "#     return z_transformed\n",
    "#self Attention LSTM Autoencoder Model\n",
    "class SelfAttentionLayer(Layer):\n",
    "    def __init__(self, num_heads, key_dim):\n",
    "        super(SelfAttentionLayer, self).__init__()\n",
    "        self.multi_head_attention = MultiHeadAttention(num_heads=num_heads, key_dim=key_dim)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return self.multi_head_attention(inputs, inputs, inputs)\n",
    "# Variational Autoencoder (VAE) Class\n",
    "class VAE:\n",
    "    def __init__(self, sequence_length, feature_dim, original_dim, intermediate_dim, latent_dim,\n",
    "                 epsilon_std=0.1, dropout_rate=0.2):\n",
    "        self.sequence_length = sequence_length\n",
    "        self.feature_dim = feature_dim\n",
    "        self.original_dim = original_dim\n",
    "        self.intermediate_dim = intermediate_dim\n",
    "        self.latent_dim = latent_dim\n",
    "        self.epsilon_std = epsilon_std\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.encoder = None\n",
    "        self.decoder = None\n",
    "        self.vae = None\n",
    "        self._build()\n",
    "        #self._build2()\n",
    "\n",
    "    def _sampling3(self, args):\n",
    "        z_mean, _ = args  # Ignore z_log_var for simplicity\n",
    "        #tf.print('z_mean:', z_mean)\n",
    "        z_mean_transformed = process_latent_variables(z_mean)\n",
    "        #tf.print('z_mean_transformed:', z_mean_transformed)\n",
    "        epsilon_std = 0.1  # Use a fixed small std deviation to reduce variability\n",
    "        #epsilon = K.random_normal(shape=K.shape(z_mean), mean=0., stddev=epsilon_std)\n",
    "        epsilon = K.random_normal(shape=K.shape(z_mean_transformed), mean=0., stddev=epsilon_std)\n",
    "        #epsilon = process_latent_variables(epsilon)\n",
    "        #tf.print('epsilon_value:', epsilon)\n",
    "        epsilon = tf.cast(epsilon, 'float64')  # Cast epsilon to float64\n",
    "        \n",
    "        #a = z_mean_transformed + epsilon\n",
    "        #tf.print('a:', a)\n",
    "        #return a\n",
    "        return z_mean_transformed\n",
    "        #return z_mean + epsilon\n",
    "    def _sampling(self, args):\n",
    "        z_mean, z_log_var = args\n",
    "        batch = K.shape(z_mean)[0]\n",
    "        dim = K.int_shape(z_mean)[1]\n",
    "        epsilon = K.random_normal(shape=(batch, dim), mean=0., stddev=self.epsilon_std)\n",
    "        output = z_mean + K.exp(0.5 * z_log_var) * epsilon\n",
    "        return output\n",
    "    def _sampling33(self, args):\n",
    "        z_mean, z_log_var = args\n",
    "        batch = K.shape(z_mean)[0]\n",
    "        dim = K.int_shape(z_mean)[1]\n",
    "        epsilon = K.random_normal(shape=(batch, dim), mean=0., stddev=self.epsilon_std)\n",
    "        # Standard reparameterization trick\n",
    "        z = z_mean + K.exp(0.5 * z_log_var) * epsilon\n",
    "        # Consider z_mean as x and z as y for MEC application\n",
    "        z_mec = apply_mec_to_data(z_mean, z)\n",
    "        # Convert z_mec to float32 to match z_mean and z_log_var\n",
    "        z_mec = tf.cast(z_mec, dtype=tf.float32)\n",
    "        z_mec = z_mean + K.exp(0.5 * z_log_var) * z_mec\n",
    "        return z_mec\n",
    "    \n",
    "#     def _sampling4(self, args):\n",
    "#         z_mean, z_log_var = args\n",
    "#         z_mean_transformed = process_latent_variables(z_mean)\n",
    "#         z_log_var_transformed = process_latent_variables(z_log_var)\n",
    "#         batch = K.shape(z_mean_transformed)[0]\n",
    "#         dim = K.int_shape(z_mean_transformed)[1]\n",
    "#         z_mean_transformed = tf.cast(z_mean_transformed, tf.float32)\n",
    "#         z_log_var_transformed = tf.cast(z_log_var_transformed, tf.float32)\n",
    "\n",
    "#         epsilon = K.random_normal(shape=(batch, dim), mean=0., stddev=self.epsilon_std)\n",
    "#         output = z_mean_transformed + K.exp(0.5 * z_log_var_transformed) * epsilon\n",
    "#         return output\n",
    "        \n",
    "    def _build(self):\n",
    "        # Encoder\n",
    "        #model.add(LSTM(50, activation='relu', input_shape=(sequence_length, 8), \n",
    "                       #return_sequences=True)\n",
    "        inputs = Input(shape=(self.sequence_length, self.feature_dim), name='encoder_input')\n",
    "        #x = Bidirectional(LSTM(self.intermediate_dim, activation='tanh', return_sequences=True))(inputs)\n",
    "        x = LSTM(self.original_dim, activation='tanh', return_sequences=True)(inputs)\n",
    "        #x = Dropout(self.dropout_rate)(x)\n",
    "        self_attention = SelfAttentionLayer(num_heads=2, key_dim=self.original_dim)  # Adjust num_heads and key_dim as needed\n",
    "        x = self_attention(x)\n",
    "        x = LSTM(self.intermediate_dim, activation='tanh', return_sequences=False)(x)\n",
    "        #x = Dropout(self.dropout_rate)(x)\n",
    "\n",
    "        z_mean = Dense(self.latent_dim, name='z_mean')(x)\n",
    "        z_log_var = Dense(self.latent_dim, name='z_log_var')(x)\n",
    "        \n",
    "        z = Lambda(self._sampling33, output_shape=(self.latent_dim,))([z_mean, z_log_var])\n",
    "        # Initiate Encoder\n",
    "        self.encoder = Model(inputs, [z_mean, z_log_var, z], name='encoder')  \n",
    "        # Decoder\n",
    "        latent_inputs = Input(shape=(self.latent_dim,), name='z_sampling')\n",
    "        x = RepeatVector(self.sequence_length)(latent_inputs)\n",
    "        x = LSTM(self.intermediate_dim, activation='tanh', return_sequences=True)(x)\n",
    "        #x = Dropout(self.dropout_rate)(x)\n",
    "        x = LSTM(self.original_dim, activation='tanh', return_sequences=True)(x)\n",
    "        #x = Dropout(self.dropout_rate)(x)\n",
    "        final_activation = 'sigmoid' #'linear' if data is not normalized\n",
    "        #outputs = TimeDistributed(Dense(self.original_dim, activation=final_activation))(x)\n",
    "        outputs = TimeDistributed(Dense(self.feature_dim))(x)\n",
    "        # Instantiate the decoder model\n",
    "        self.decoder = Model(latent_inputs, outputs, name='decoder')\n",
    "        # VAE model\n",
    "        outputs = self.decoder(self.encoder(inputs)[2])\n",
    "        self.vae = Model(inputs, outputs, name='vae_mlp')\n",
    "\n",
    "    def vae_loss(self, x, x_decoded_mean, z_mean, z_log_var):\n",
    "        mse = tf.reduce_mean(tf.square(x - x_decoded_mean), axis=(1, 2))\n",
    "        xent_loss = mse\n",
    "        kl_loss = -0.5 * K.sum(1 + z_log_var - K.square(z_mean) - K.exp(z_log_var), axis=-1)\n",
    "        return K.mean(xent_loss + kl_loss)\n",
    "    def simplified_vae_loss(self, x, x_decoded_mean, z_mean):\n",
    "        mse = tf.reduce_mean(tf.square(x - x_decoded_mean), axis=(1, 2))\n",
    "        mean_loss = tf.reduce_mean(tf.square(z_mean))\n",
    "        total_loss = mse + mean_loss\n",
    "        return K.mean(total_loss)\n",
    "    \n",
    "    def compile(self, learning_rate=0.001, optimizer='adam'):\n",
    "        if optimizer == 'adam':\n",
    "            optimizer = Adam(learning_rate=learning_rate)\n",
    "\n",
    "        def vae_loss_wrapper(x, x_decoded_mean):\n",
    "            z_mean, z_log_var, _ = self.encoder(x)\n",
    "            return self.vae_loss(x, x_decoded_mean, z_mean, z_log_var)\n",
    "\n",
    "        self.vae.compile(optimizer=optimizer, loss=vae_loss_wrapper)\n",
    "\n",
    "    def compile2(self, learning_rate=0.0005, optimizer='adam'):\n",
    "        if optimizer == 'adam':\n",
    "            optimizer = Adam(learning_rate=learning_rate)\n",
    "\n",
    "        # Define a wrapper function for the custom loss\n",
    "        def custom_loss_wrapper(x, x_decoded_mean):\n",
    "            z_mean, z_log_var, _ = self.encoder(x)\n",
    "            return self.custom_loss(x, x_decoded_mean, z_mean, z_log_var)\n",
    "\n",
    "        self.vae.compile(optimizer=optimizer, loss=custom_loss_wrapper)\n",
    "    def compile_custom(self, learning_rate=0.001, optimizer='adam'):\n",
    "        if optimizer == 'adam':\n",
    "            optimizer = Adam(learning_rate=learning_rate)\n",
    "        def custom_loss_wrapper(x, x_decoded_mean):\n",
    "            z_mean, _, _ = self.encoder(x)\n",
    "            return self.simplified_vae_loss(x, x_decoded_mean, z_mean)\n",
    "\n",
    "        self.vae.compile(optimizer=optimizer, loss=custom_loss_wrapper)\n",
    "    def compile_simple_autoencoder(self, learning_rate=0.005, optimizer='adam'):\n",
    "        if optimizer == 'adam':\n",
    "            optimizer = Adam(learning_rate=learning_rate)\n",
    "        \n",
    "        # Simple reconstruction loss\n",
    "        self.vae.compile(optimizer=optimizer, loss='mse')\n",
    "\n",
    "# Instantiate and Compile the VAE\n",
    "sequence_length = 10\n",
    "feature_dim = 2\n",
    "original_dim = 50\n",
    "intermediate_dim = 50\n",
    "latent_dim = 25\n",
    "\n",
    "vae_model = VAE(sequence_length, feature_dim, original_dim, intermediate_dim, latent_dim)\n",
    "#vae_model.vae.compile(optimizer='adam', loss=vae_model.vae_loss)\n",
    "#vae_model.compile2(learning_rate=0.005)\n",
    "vae_model.compile_simple_autoencoder(learning_rate=0.005)\n",
    "#vae_model.compile_custom(learning_rate=0.005)\n",
    "\n",
    "# Model Training\n",
    "batch_size = 40\n",
    "max_train_samples = 1000000\n",
    "train_steps = max_train_samples // (batch_size * sequence_length)\n",
    "max_samples = 1000000  # Maximum samples to read (or None to read all)\n",
    "max_test_samples = 1000000\n",
    "\n",
    "pure_file_pattern = '/home/mreza/5G accelerator/ID_MEC/data generator/pure_data/pure_iq_samples_*.csv'\n",
    "mixed_file_pattern = '/home/mreza/5G accelerator/ID_MEC/data generator/mixed_data/mixed_iq_samples_*.csv'\n",
    "pure_file_new = '/home/mreza/5G accelerator/ID_MEC/data generator/New Data-Collection/rx_IQ_pure'\n",
    "mixed_file_new = '/home/mreza/5G accelerator/ID_MEC/data generator/New Data-Collection/rx_IQ_MIX'\n",
    "pure_file_old = '/home/mreza/5G accelerator/IQ_samples/data collected/5G_DL_IQ_no_jamming_0924.dat'\n",
    "mixed_file_old = '/home/mreza/5G accelerator/IQ_samples/data collected/5G_DL_IQ_with_periodic_jamming_0928_02.dat'\n",
    "# Example file patterns\n",
    "#pure_file_pattern = 'C:\\\\Users\\\\Mohammadreza\\\\Desktop\\\\My Class\\\\Proj-DC\\\\My Works\\\\My Papers\\\\intrusion\\\\data generator\\\\pure_data\\\\pure_iq_samples_*.csv'\n",
    "#mixed_file_pattern = 'C:\\\\Users\\\\Mohammadreza\\\\Desktop\\\\My Class\\\\Proj-DC\\\\My Works\\\\My Papers\\\\intrusion\\\\data generator\\\\mixed_data\\\\mixed_iq_samples_*.csv'\n",
    "\n",
    "# Data Generator Instances\n",
    "# train_gen_instance = CSVDataGenerator(pure_file_pattern, batch_size, sequence_length, \n",
    "#                                       max_train_samples, for_training=True)\n",
    "# combined_gen_instance = CSVDataGenerator(mixed_file_pattern, batch_size, sequence_length, \n",
    "#                                          max_test_samples, for_training=False)\n",
    "train_gen_instance = DataGenerator(pure_file_new,batch_size=batch_size, sequence_length=sequence_length, \n",
    "                                   max_samples=max_train_samples, for_training=True)\n",
    "combined_gen_instance = DataGenerator(mixed_file_new,batch_size=batch_size, sequence_length=sequence_length, \n",
    "                                      for_training=False)\n",
    "\n",
    "num_epochs = 4  # You can adjust the number of epochs as needed\n",
    "steps_per_epoch = train_steps  # Assuming one epoch processes all the data\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
    "    train_gen_instance.reset()  # Reset the generator at the beginning of each epoch\n",
    "    for step in range(steps_per_epoch):\n",
    "        try:\n",
    "            X_chunk, Y_chunk = next(train_gen_instance)\n",
    "        except StopIteration:\n",
    "            train_gen_instance.reset()  # Reset the generator when it runs out of data\n",
    "            X_chunk, Y_chunk = next(train_gen_instance)\n",
    "\n",
    "        #loss = model.train_on_batch(X_chunk, Y_chunk)\n",
    "        loss = vae_model.vae.train_on_batch(X_chunk, Y_chunk)\n",
    "        #print(f\"Step {step + 1}/{steps_per_epoch}\", end='\\r')\n",
    "        if step % 100 == 0:\n",
    "            print(f\"Step {step + 1}/{train_steps}, Loss: {loss}\")\n",
    "    print()\n",
    "\n",
    "num_predictions = 100  # or any other large number\n",
    "print(f\"Number of predictions to be performed: {num_predictions}\")\n",
    "\n",
    "\n",
    "reconstruction_errors = []\n",
    "all_X_chunk_test = []\n",
    "all_X_chunk_pred = []\n",
    "all_intrusion_flags = []\n",
    "try:\n",
    "    for _ in range(num_predictions):\n",
    "        print('prediction number:', _)\n",
    "        X_chunk_test = next(combined_gen_instance)\n",
    "        #X_chunk_pred = model.predict(X_chunk_test)\n",
    "        X_chunk_pred = vae_model.vae.predict(X_chunk_test)\n",
    "        chunk_errors = np.mean(np.square(X_chunk_test - X_chunk_pred), axis=1)\n",
    "        reconstruction_errors.extend(chunk_errors)        \n",
    "        all_X_chunk_test.append(X_chunk_test)\n",
    "        all_X_chunk_pred.append(X_chunk_pred)\n",
    "except StopIteration:\n",
    "    print(\"All samples processed.\")\n",
    "\n",
    "\n",
    "reconstruction_error = np.array(reconstruction_errors)\n",
    "print('reconstruction_error.shape:', reconstruction_error.shape)\n",
    "print('Number of NaNs in reconstruction_error:', np.isnan(reconstruction_error).sum())\n",
    "max_error_per_sequence = reconstruction_error.max(axis=1) # Max error for each sequence\n",
    "print('max_error_per_sequence:', max_error_per_sequence)\n",
    "\n",
    "print('max_error_per_sequence.shape:', max_error_per_sequence.shape)\n",
    "\n",
    "threshold1 = np.percentile(max_error_per_sequence, 98)\n",
    "print('threshold1:', threshold1)\n",
    "threshold2 = np.percentile(reconstruction_error, 95)\n",
    "print('threshold percentile:', threshold2)\n",
    "\n",
    "is_intrusion_detected = max_error_per_sequence > threshold1  # Boolean array for sequences\n",
    "print('len(is_intrusion_detected):', len(is_intrusion_detected))\n",
    "print('is_intrusion_detected.shape:', is_intrusion_detected.shape)\n",
    "\n",
    "#is_intrusion_detected2 = error_per_sequence > threshold1\n",
    "\n",
    "num_total_sequences = len(max_error_per_sequence)\n",
    "num_total_sequences2 = num_predictions * batch_size - num_predictions\n",
    "print('num_total_sequences:', num_total_sequences)\n",
    "print('num_total_sequences2:', num_total_sequences2)\n",
    "\n",
    "#---------------------------------------finish 111-----------------------------------\n",
    "flat_error_per_sequence = max_error_per_sequence.flatten()\n",
    "#flat_error_per_sequence2 = error_per_sequence.flatten()\n",
    "# Determine if intrusion detected for each sequence\n",
    "for error in flat_error_per_sequence:\n",
    "    all_intrusion_flags.append(error > threshold1)    \n",
    "all_X_chunk_test = np.concatenate(all_X_chunk_test, axis=0)\n",
    "all_X_chunk_pred = np.concatenate(all_X_chunk_pred, axis=0)\n",
    "\n",
    "#save_path = 'C:\\\\Users\\\\Mohammadreza\\\\Desktop\\\\My Class\\\\Proj-DC\\\\My Works\\\\My Papers\\\\intrusion\\\\data generator\\\\intrusion_detected'\n",
    "#plot_with_intrusions8(all_X_chunk_test, all_X_chunk_pred, all_intrusion_flags, sequence_length, save_path)\n",
    "\n",
    "jamming_detected = reconstruction_error > threshold1\n",
    "train_gen_instance.close()\n",
    "combined_gen_instance.close()\n",
    "#Table\n",
    "flattened_jamming_detected = jamming_detected.flatten()\n",
    "real_part_detected = jamming_detected[:, 0]\n",
    "imag_part_detected = jamming_detected[:, 1]\n",
    "\n",
    "real_true_count = np.sum(real_part_detected)\n",
    "real_false_count = len(real_part_detected) - real_true_count\n",
    "\n",
    "imag_true_count = np.sum(imag_part_detected)\n",
    "imag_false_count = len(imag_part_detected) - imag_true_count\n",
    "# Overall\n",
    "overall_true_count = np.sum(flattened_jamming_detected)\n",
    "overall_false_count = len(flattened_jamming_detected) - overall_true_count\n",
    "# Table-DataFrame\n",
    "df = pd.DataFrame({\n",
    "    'Part': ['Real', 'Imaginary', 'Overall'],\n",
    "    'True Count': [real_true_count, imag_true_count, overall_true_count],\n",
    "    'False Count': [real_false_count, imag_false_count, overall_false_count]\n",
    "})\n",
    "print(df)\n",
    "num_jamming_detected = np.sum(jamming_detected)\n",
    "print(f\"Number of jamming sequences detected: {num_jamming_detected} out of {len(flattened_jamming_detected)} sequences\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50dd4f2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define the number of sequences to plot together\n",
    "n = 9  # Change this to desired number of sequences\n",
    "sample_length = sequence_length * n\n",
    "\n",
    "# Select a random starting sequence for plotting\n",
    "sequence_index = np.random.choice(len(X_chunk_test) - n + 1)\n",
    "\n",
    "# Extract and concatenate the original and reconstructed samples\n",
    "original_sample = np.concatenate(X_chunk_test[sequence_index:sequence_index + n])\n",
    "reconstructed_sample = np.concatenate(X_chunk_pred[sequence_index:sequence_index + n])\n",
    "\n",
    "# Plot concatenated sequences\n",
    "plt.figure(figsize=(14, 6))\n",
    "plt.plot(original_sample[:, 0], 'b-', label='Original Real Part')\n",
    "plt.plot(reconstructed_sample[:, 0], 'r--', label='Reconstructed Real Part')\n",
    "# plt.plot(original_sample[:, 1], 'orange', label='Original Real STD')\n",
    "# plt.plot(reconstructed_sample[:, 1], 'orange', label='Reconstructed Real STD', linestyle='--')\n",
    "\n",
    "plt.plot(original_sample[:, 1], 'y-', label='Original Imaginary Part')\n",
    "plt.plot(reconstructed_sample[:, 1], 'g--', label='Reconstructed Imaginary Part')\n",
    "# plt.plot(original_sample[:, 5], 'pink', label='Original Imaginary STD')\n",
    "# plt.plot(reconstructed_sample[:, 5], 'pink', label='Reconstructed Imaginary STD', linestyle='--')\n",
    "plt.title(f'Original vs Reconstructed IQ Data for {n} Sequences of Length {sequence_length}')\n",
    "plt.xlabel('Time Steps')\n",
    "plt.ylabel('Amplitude')\n",
    "plt.legend()\n",
    "# plt.savefig('9-Original vs Reconstructed IQ Data for {n} Sequences of Length {sequence_length}.png')\n",
    "# plt.close()\n",
    "plt.show()\n",
    "\n",
    "# Repeat for n = 9\n",
    "n = 2  # Change this to desired number of sequences\n",
    "sequence_index = np.random.choice(len(X_chunk_test) - n + 1)\n",
    "original_sample = np.concatenate(X_chunk_test[sequence_index:sequence_index + n])\n",
    "reconstructed_sample = np.concatenate(X_chunk_pred[sequence_index:sequence_index + n])\n",
    "\n",
    "plt.figure(figsize=(14, 6))\n",
    "plt.plot(original_sample[:, 0], 'b-', label='Original Real Part')\n",
    "plt.plot(reconstructed_sample[:, 0], 'r--', label='Reconstructed Real Part')\n",
    "plt.plot(original_sample[:, 1], 'g-', label='Original Imaginary Part')\n",
    "plt.plot(reconstructed_sample[:, 1], 'y--', label='Reconstructed Imaginary Part')\n",
    "plt.title(f'Original vs Reconstructed IQ Data for {n} Sequences of Length {sequence_length}')\n",
    "plt.xlabel('Time Steps')\n",
    "plt.ylabel('Amplitude')\n",
    "plt.legend()\n",
    "# plt.savefig('11-Original vs Reconstructed IQ Data for {n} Sequences of Length {sequence_length}.png')\n",
    "# plt.close()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "372d49c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# reconstruction error\n",
    "plt.figure(figsize=(14, 6))\n",
    "plt.plot(reconstruction_error, label='Reconstruction Error')\n",
    "plt.axhline(y=threshold2, color='r', linestyle='--', label='Threshold')\n",
    "plt.title('Reconstruction Error with Threshold')\n",
    "plt.xlabel('Sequence Number')\n",
    "plt.ylabel('Reconstruction Error')\n",
    "plt.legend()\n",
    "# plt.savefig('1-Reconstruction Error with Threshold.png')\n",
    "# plt.close()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4348bb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flatten the reconstruction_error to 1D\n",
    "reconstruction_error_flat = reconstruction_error.flatten()\n",
    "# reconstruction error\n",
    "plt.figure(figsize=(14, 6))\n",
    "plt.plot(reconstruction_error_flat, label='Reconstruction Error')\n",
    "plt.axhline(y=threshold2, color='r', linestyle='--', label='Threshold')\n",
    "plt.title('Reconstruction Error with Threshold')\n",
    "plt.xlabel('Sequence Number')\n",
    "plt.ylabel('Reconstruction Error')\n",
    "plt.legend()\n",
    "# plt.savefig('1-Reconstruction Error with Threshold.png')\n",
    "# plt.close()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab0e1a56",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Histogram of Reconstruction Errors:\n",
    "plt.figure(figsize=(14, 6))\n",
    "plt.hist(reconstruction_error, bins=50, alpha=0.75)\n",
    "plt.axvline(x=threshold2, color='r', linestyle='--', label='Threshold')\n",
    "plt.title('Histogram of Reconstruction Errors')\n",
    "plt.xlabel('Reconstruction Error')\n",
    "plt.ylabel('Frequency')\n",
    "plt.legend()\n",
    "# plt.savefig('4-Histogram of Reconstruction Errors.png')\n",
    "# plt.close()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39a86f70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flatten the reconstruction_error to 1D\n",
    "reconstruction_error_flat = reconstruction_error.flatten()\n",
    "#Histogram of Reconstruction Errors:\n",
    "plt.figure(figsize=(14, 6))\n",
    "plt.hist(reconstruction_error_flat, bins=50, alpha=0.75)\n",
    "plt.axvline(x=threshold2, color='r', linestyle='--', label='Threshold')\n",
    "plt.title('Histogram of Reconstruction Errors')\n",
    "plt.xlabel('Reconstruction Error')\n",
    "plt.ylabel('Frequency')\n",
    "plt.legend()\n",
    "# plt.savefig('4-Histogram of Reconstruction Errors.png')\n",
    "# plt.close()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c4005b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Time Series Plot of IQ Samples:\n",
    "sample_index = np.random.choice(len(X_chunk_test))\n",
    "original_sample = X_chunk_test[sample_index]\n",
    "reconstructed_sample = X_chunk_pred[sample_index]\n",
    "\n",
    "plt.figure(figsize=(14, 6))\n",
    "plt.plot(original_sample[:, 0], 'b-', label='Original Real Part')\n",
    "plt.plot(reconstructed_sample[:, 0], 'r--', label='Reconstructed Real Part')\n",
    "plt.plot(original_sample[:, 1], 'g-', label='Original Imaginary Part')\n",
    "plt.plot(reconstructed_sample[:, 1], 'y--', label='Reconstructed Imaginary Part')\n",
    "plt.title('Original vs Reconstructed IQ Data')\n",
    "plt.xlabel('Time Steps')\n",
    "plt.ylabel('Amplitude')\n",
    "plt.legend()\n",
    "# plt.savefig('5-Original vs Reconstructed IQ Data.png')\n",
    "# plt.close()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01e87830",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scatter Plot of Reconstruction Errors vs. Real and Imaginary Parts:\n",
    "avg_real = np.mean(X_chunk_test, axis=1)[:, 0]\n",
    "avg_imag = np.mean(X_chunk_test, axis=1)[:, 1]\n",
    "\n",
    "last_errors = np.mean(reconstruction_errors[-len(X_chunk_test):], axis=1)\n",
    "\n",
    "print(\"Shape of avg_real:\", avg_real.shape)\n",
    "print(\"Shape of avg_imag:\", avg_imag.shape)\n",
    "print(\"Shape of last_errors:\", len(last_errors))\n",
    "\n",
    "\n",
    "plt.figure(figsize=(14, 6))\n",
    "plt.scatter(avg_real, last_errors, label='Real Part', alpha=0.5)\n",
    "plt.axhline(y=threshold2, color='r', linestyle='--', label='Threshold')\n",
    "plt.title('Reconstruction Error vs. Average Real Part')\n",
    "plt.xlabel('Average Amplitude')\n",
    "plt.ylabel('Reconstruction Error')\n",
    "plt.legend()\n",
    "# plt.savefig('6-Reconstruction Error vs. Average Real Part.png')\n",
    "# plt.close()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(14, 6))\n",
    "plt.scatter(avg_imag, last_errors, label='Imaginary Part', alpha=0.5)\n",
    "plt.axhline(y=threshold2, color='r', linestyle='--', label='Threshold')\n",
    "plt.title('Reconstruction Error vs. Average Imaginary Part')\n",
    "plt.xlabel('Average Amplitude')\n",
    "plt.ylabel('Reconstruction Error')\n",
    "plt.legend()\n",
    "# plt.savefig('7-Reconstruction Error vs. Average Imaginary Part.png')\n",
    "# plt.close()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5d4235d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define the number of sequences to plot together\n",
    "n = 8  # Change this to desired number of sequences\n",
    "sample_length = sequence_length * n\n",
    "\n",
    "# Select a random starting sequence for plotting\n",
    "sequence_index = np.random.choice(len(X_chunk_test) - n + 1)\n",
    "\n",
    "# Extract and concatenate the original and reconstructed samples\n",
    "original_sample = np.concatenate(X_chunk_test[sequence_index:sequence_index + n])\n",
    "reconstructed_sample = np.concatenate(X_chunk_pred[sequence_index:sequence_index + n])\n",
    "\n",
    "# Plot concatenated sequences\n",
    "plt.figure(figsize=(14, 6))\n",
    "plt.plot(original_sample[:, 0], 'b-', label='Original Real Part')\n",
    "plt.plot(reconstructed_sample[:, 0], 'r--', label='Reconstructed Real Part')\n",
    "plt.plot(original_sample[:, 1], 'g-', label='Original Img Part')\n",
    "plt.plot(reconstructed_sample[:, 1], 'y--', label='Reconstructed Img Part')\n",
    "plt.title(f'Original vs Reconstructed IQ Data for {n} Sequences of Length {sequence_length}')\n",
    "plt.xlabel('Time Steps')\n",
    "plt.ylabel('Amplitude')\n",
    "plt.legend()\n",
    "# plt.savefig('9-Original vs Reconstructed IQ Data for {n} Sequences of Length {sequence_length}.png')\n",
    "# plt.close()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "640153f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b86f49a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define the number of sequences to plot together\n",
    "n = 9  # Change this to desired number of sequences\n",
    "sample_length = sequence_length * n\n",
    "\n",
    "# Select a random starting sequence for plotting\n",
    "sequence_index = np.random.choice(len(X_chunk_test) - n + 1)\n",
    "\n",
    "# Extract and concatenate the original and reconstructed samples\n",
    "original_sample = np.concatenate(X_chunk_test[sequence_index:sequence_index + n])\n",
    "reconstructed_sample = np.concatenate(X_chunk_pred[sequence_index:sequence_index + n])\n",
    "\n",
    "# Plot concatenated sequences\n",
    "plt.figure(figsize=(14, 6))\n",
    "plt.plot(original_sample[:, 0], 'b-', label='Original Real Part')\n",
    "plt.plot(reconstructed_sample[:, 0], 'r--', label='Reconstructed Real Part')\n",
    "# plt.plot(original_sample[:, 1], 'orange', label='Original Real STD')\n",
    "# plt.plot(reconstructed_sample[:, 1], 'orange', label='Reconstructed Real STD', linestyle='--')\n",
    "\n",
    "plt.plot(original_sample[:, 1], 'y-', label='Original Imaginary Part')\n",
    "plt.plot(reconstructed_sample[:, 1], 'g--', label='Reconstructed Imaginary Part')\n",
    "# plt.plot(original_sample[:, 5], 'pink', label='Original Imaginary STD')\n",
    "# plt.plot(reconstructed_sample[:, 5], 'pink', label='Reconstructed Imaginary STD', linestyle='--')\n",
    "plt.title(f'Original vs Reconstructed IQ Data for {n} Sequences of Length {sequence_length}')\n",
    "plt.xlabel('Time Steps')\n",
    "plt.ylabel('Amplitude')\n",
    "plt.legend()\n",
    "# plt.savefig('9-Original vs Reconstructed IQ Data for {n} Sequences of Length {sequence_length}.png')\n",
    "# plt.close()\n",
    "plt.show()\n",
    "\n",
    "# Repeat for n = 9\n",
    "n = 2  # Change this to desired number of sequences\n",
    "sequence_index = np.random.choice(len(X_chunk_test) - n + 1)\n",
    "original_sample = np.concatenate(X_chunk_test[sequence_index:sequence_index + n])\n",
    "reconstructed_sample = np.concatenate(X_chunk_pred[sequence_index:sequence_index + n])\n",
    "\n",
    "plt.figure(figsize=(14, 6))\n",
    "plt.plot(original_sample[:, 0], 'b-', label='Original Real Part')\n",
    "plt.plot(reconstructed_sample[:, 0], 'r--', label='Reconstructed Real Part')\n",
    "plt.plot(original_sample[:, 1], 'g-', label='Original Imaginary Part')\n",
    "plt.plot(reconstructed_sample[:, 1], 'y--', label='Reconstructed Imaginary Part')\n",
    "plt.title(f'Original vs Reconstructed IQ Data for {n} Sequences of Length {sequence_length}')\n",
    "plt.xlabel('Time Steps')\n",
    "plt.ylabel('Amplitude')\n",
    "plt.legend()\n",
    "# plt.savefig('11-Original vs Reconstructed IQ Data for {n} Sequences of Length {sequence_length}.png')\n",
    "# plt.close()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfd97dbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Repeat for n = 9\n",
    "n = 2  # Change this to desired number of sequences\n",
    "sequence_index = np.random.choice(len(X_chunk_test) - n + 1)\n",
    "original_sample = np.concatenate(X_chunk_test[sequence_index:sequence_index + n])\n",
    "reconstructed_sample = np.concatenate(X_chunk_pred[sequence_index:sequence_index + n])\n",
    "\n",
    "plt.figure(figsize=(14, 6))\n",
    "plt.plot(original_sample[:, 0], 'b-', label='Original Real Part')\n",
    "plt.plot(reconstructed_sample[:, 0], 'r--', label='Reconstructed Real Part')\n",
    "\n",
    "plt.title(f'Original vs Reconstructed IQ Data for {n} Sequences of Length {sequence_length}')\n",
    "plt.xlabel('Time Steps')\n",
    "plt.ylabel('Amplitude')\n",
    "plt.legend()\n",
    "# plt.savefig('11-Original vs Reconstructed IQ Data for {n} Sequences of Length {sequence_length}.png')\n",
    "# plt.close()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63145cc9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a6c47fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Repeat for n = 9\n",
    "n = 1  # Change this to desired number of sequences\n",
    "sequence_index = np.random.choice(len(X_chunk_test) - n + 1)\n",
    "original_sample = np.concatenate(X_chunk_test[sequence_index:sequence_index + n])\n",
    "reconstructed_sample = np.concatenate(X_chunk_pred[sequence_index:sequence_index + n])\n",
    "\n",
    "plt.figure(figsize=(14, 6))\n",
    "plt.plot(original_sample[:, 0], 'b-', label='Original Real Part')\n",
    "plt.plot(reconstructed_sample[:, 0], 'r--', label='Reconstructed Real Part')\n",
    "plt.plot(original_sample[:, 1], 'g-', label='Original Imaginary Part')\n",
    "plt.plot(reconstructed_sample[:, 1], 'y--', label='Reconstructed Imaginary Part')\n",
    "plt.title(f'Original vs Reconstructed IQ Data for {n} Sequences of Length {sequence_length}')\n",
    "plt.xlabel('Time Steps')\n",
    "plt.ylabel('Amplitude')\n",
    "plt.legend()\n",
    "# plt.savefig('11-Original vs Reconstructed IQ Data for {n} Sequences of Length {sequence_length}.png')\n",
    "# plt.close()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0025906c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reconstruction error\n",
    "reconstruction_error_real = reconstruction_error[:, 0]\n",
    "reconstruction_error_imag = reconstruction_error[:, 1]\n",
    "\n",
    "# Plot for Real Part\n",
    "plt.figure(figsize=(14, 6))\n",
    "mellow_green = '#89C997' \n",
    "plt.plot(reconstruction_error_real, label='Reconstruction Error', color=mellow_green)\n",
    "plt.axhline(y=threshold2, color='r', linestyle='--', label='Threshold')\n",
    "plt.title('Intrusion Detected by Reconstruction Error',fontsize=16, fontweight='bold')\n",
    "plt.xlabel('Sequence Number (10)', fontsize=16, fontweight='bold')\n",
    "#plt.xlabel('Sequence Number(*1000)', fontsize=16, fontweight='bold')\n",
    "plt.ylabel('Reconstruction Error', fontsize=16, fontweight='bold')\n",
    "for label in (plt.gca().get_xticklabels() + plt.gca().get_yticklabels()):\n",
    "    label.set_fontsize(12)\n",
    "    label.set_fontweight('bold')\n",
    "plt.legend(fontsize=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea42b73d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_index = np.random.choice(len(X_chunk_test))\n",
    "original_sample = X_chunk_test[sample_index]\n",
    "reconstructed_sample = X_chunk_pred[sample_index]\n",
    "\n",
    "plt.figure(figsize=(16, 8))\n",
    "plt.plot(original_sample[:, 0], 'b-', label='Original Real Part')\n",
    "plt.plot(reconstructed_sample[:, 0], 'b--', label='Reconstructed Real Part')\n",
    "# plt.plot(original_sample[:, 1], 'm-', label='Original Real STD')\n",
    "# plt.plot(reconstructed_sample[:, 1], 'm--', label='Reconstructed Real STD')\n",
    "# plt.plot(original_sample[:, 2], 'c-', label='Original Real Skew')\n",
    "# plt.plot(reconstructed_sample[:, 2], 'c--', label='Reconstructed Real Skew')\n",
    "# plt.plot(original_sample[:, 3], 'orange', label='Original Real Kurtosis')\n",
    "# plt.plot(reconstructed_sample[:, 3], 'orange', label='Reconstructed Real Kurtosis', linestyle='--')\n",
    "\n",
    "plt.plot(original_sample[:, 1], 'g-', label='Original Imaginary Part')\n",
    "plt.plot(reconstructed_sample[:, 1], 'g--', label='Reconstructed Imaginary Part')\n",
    "# plt.plot(original_sample[:, 5], 'purple', label='Original Imaginary STD')\n",
    "# plt.plot(reconstructed_sample[:, 5], 'purple', label='Reconstructed Imaginary STD', linestyle='--')\n",
    "# plt.plot(original_sample[:, 6], 'brown', label='Original Imaginary Skew')\n",
    "# plt.plot(reconstructed_sample[:, 6], 'brown', label='Reconstructed Imaginary Skew', linestyle='--')\n",
    "# plt.plot(original_sample[:, 7], 'pink', label='Original Imaginary Kurtosis')\n",
    "# plt.plot(reconstructed_sample[:, 7], 'pink', label='Reconstructed Imaginary Kurtosis', linestyle='--')\n",
    "plt.title('Original vs Reconstructed IQ Data')\n",
    "plt.xlabel('Time Steps')\n",
    "plt.ylabel('Amplitude')\n",
    "\n",
    "# Place the legend outside the plot area\n",
    "plt.legend(loc='upper left', bbox_to_anchor=(1, 1), fontsize='small', title='Legend')\n",
    "\n",
    "# Adjust layout to make room for the legend\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bafc4676",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
