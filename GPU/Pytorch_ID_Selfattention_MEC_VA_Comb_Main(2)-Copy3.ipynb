{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4b3d3f1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-05 14:46:23.887411: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-04-05 14:46:23.888474: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-04-05 14:46:23.914092: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-04-05 14:46:23.914651: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-04-05 14:46:24.344079: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "/usr/lib/python3/dist-packages/requests/__init__.py:89: RequestsDependencyWarning: urllib3 (2.2.1) or chardet (3.0.4) doesn't match a supported version!\n",
      "  warnings.warn(\"urllib3 ({}) or chardet ({}) doesn't match a supported \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Epoch 1/2\n",
      "Step 1/250, Loss: 993.7449340820312\n",
      "Step 101/250, Loss: 799.8992919921875\n",
      "Step 201/250, Loss: 800.3550415039062\n",
      "\n",
      "Epoch 2/2\n",
      "Step 1/250, Loss: 800.0731201171875\n",
      "Step 101/250, Loss: 799.9602661132812\n",
      "Step 201/250, Loss: 800.2033081054688\n",
      "\n",
      "Number of predictions to be performed: 100\n",
      "Prediction DataLoader Details:\n",
      "Batch size: 40\n",
      "Number of batches: 250\n",
      "Dataset used in DataLoader: ComplexNumbersDataset\n",
      "Number of samples in Dataset: 10000\n",
      "Sample 0: tensor([[ 0.6563,  1.5085],\n",
      "        [ 0.4633, -0.1398],\n",
      "        [ 0.4633,  1.0375],\n",
      "        [-2.0460, -1.5526],\n",
      "        [-0.3088, -0.6108],\n",
      "        [ 1.8144,  1.5085],\n",
      "        [ 0.2702,  0.0809],\n",
      "        [ 0.2702, -0.1398],\n",
      "        [-0.5019, -0.6108],\n",
      "        [-1.0809, -1.0817]])\n",
      "Sample 1: tensor([[ 0.0661, -0.0354],\n",
      "        [-0.3745,  1.0962],\n",
      "        [-1.0353, -1.1669],\n",
      "        [ 0.7269, -0.4125],\n",
      "        [-1.4758, -0.4125],\n",
      "        [-0.3745,  1.8270],\n",
      "        [-0.1542, -1.5441],\n",
      "        [ 1.1674,  1.0962],\n",
      "        [ 2.0485, -0.4125],\n",
      "        [-0.5947, -0.0354]])\n",
      "Sample 2: tensor([[ 2.2763e-01, -2.8263e-01],\n",
      "        [-9.4676e-02, -1.4203e+00],\n",
      "        [ 8.7223e-01,  2.2770e+00],\n",
      "        [ 1.1945e+00, -1.1359e+00],\n",
      "        [-4.3712e-01,  8.3722e-01],\n",
      "        [-4.1698e-01, -5.6704e-01],\n",
      "        [-1.3839e+00, -2.8263e-01],\n",
      "        [-1.3839e+00,  1.7775e-03],\n",
      "        [-4.1698e-01,  5.7059e-01],\n",
      "        [ 1.8391e+00,  1.7775e-03]])\n",
      "Sample 3: tensor([[ 0.4050, -0.2311],\n",
      "        [ 1.3692, -0.3909],\n",
      "        [-0.5592,  0.1204],\n",
      "        [-0.5592, -1.0727],\n",
      "        [-1.7163, -1.0727],\n",
      "        [ 0.0193,  2.5064],\n",
      "        [-0.1736, -0.3909],\n",
      "        [-1.1378,  0.8021],\n",
      "        [ 1.5620,  0.2908],\n",
      "        [ 0.7907, -0.5614]])\n",
      "Sample 4: tensor([[ 0.2245, -2.1561],\n",
      "        [ 0.2245,  1.2552],\n",
      "        [ 0.0216, -0.0506],\n",
      "        [-1.1959,  0.7329],\n",
      "        [ 0.8332,  0.2106],\n",
      "        [ 0.8205,  0.4717],\n",
      "        [-2.1977,  1.2552],\n",
      "        [-0.3842, -0.8340],\n",
      "        [ 0.2118, -0.0506],\n",
      "        [ 1.4419, -0.8340]])\n",
      "Sample 5: tensor([[-0.2416, -1.2703],\n",
      "        [-1.1478, -0.5534],\n",
      "        [ 0.0604,  1.9446],\n",
      "        [-1.4499,  0.3428],\n",
      "        [ 0.9666,  0.8805],\n",
      "        [-1.1478, -1.2703],\n",
      "        [ 1.2687, -1.0911],\n",
      "        [ 0.3625,  0.3316],\n",
      "        [-0.2416, -0.0157],\n",
      "        [ 1.5707,  0.7012]])\n",
      "Sample 6: tensor([[ 0.2432, -1.0855],\n",
      "        [-0.5676, -0.3090],\n",
      "        [-0.2973,  0.7264],\n",
      "        [-1.1081,  0.7102],\n",
      "        [ 1.0541,  0.2087],\n",
      "        [-1.9189,  1.5029],\n",
      "        [-0.2973,  0.2087],\n",
      "        [ 1.5946,  0.9852],\n",
      "        [ 0.5135, -1.3444],\n",
      "        [ 0.7838, -1.6032]])\n",
      "Sample 7: tensor([[ 0.5837, -0.4476],\n",
      "        [-1.9651, -0.0395],\n",
      "        [-0.6907, -0.0395],\n",
      "        [-0.1559, -0.0395],\n",
      "        [ 1.3120,  2.1919],\n",
      "        [ 1.4940,  0.5725],\n",
      "        [-0.6907, -1.0596],\n",
      "        [ 0.5837, -1.4676],\n",
      "        [-0.6907, -0.6516],\n",
      "        [ 0.2196,  0.9806]])\n",
      "Sample 8: tensor([[ 1.4510e+00,  5.8583e-01],\n",
      "        [ 1.7296e-08,  1.2352e+00],\n",
      "        [ 1.8137e+00, -4.9654e-01],\n",
      "        [ 1.7296e-08,  1.5288e-01],\n",
      "        [ 7.2548e-01,  7.8877e-01],\n",
      "        [ 1.7296e-08, -1.7818e+00],\n",
      "        [-1.0882e+00, -1.3760e+00],\n",
      "        [-1.0882e+00,  3.6936e-01],\n",
      "        [-7.2548e-01, -7.1301e-01],\n",
      "        [-1.0882e+00,  1.2352e+00]])\n",
      "Sample 9: tensor([[-0.0999, -0.5682],\n",
      "        [-0.0999,  0.7462],\n",
      "        [ 0.8995, -1.6636],\n",
      "        [-0.0999, -1.4445],\n",
      "        [ 0.5664, -0.1301],\n",
      "        [-0.0999,  0.0753],\n",
      "        [ 0.5664,  0.5272],\n",
      "        [ 0.2332, -0.1301],\n",
      "        [-2.7651,  1.8416],\n",
      "        [ 0.8995,  0.7462]])\n",
      "Sample 10: tensor([[-0.1740, -0.0240],\n",
      "        [-0.5022,  1.2412],\n",
      "        [-1.5960,  0.7447],\n",
      "        [ 0.7806, -0.0240],\n",
      "        [ 0.3033,  0.7447],\n",
      "        [-1.6059, -0.5365],\n",
      "        [-0.4922, -0.2803],\n",
      "        [ 1.2480, -2.0740],\n",
      "        [ 1.0988, -1.0490],\n",
      "        [ 0.9397,  1.2572]])\n",
      "Sample 11: tensor([[-0.3768,  1.8091],\n",
      "        [-1.6327, -0.2293],\n",
      "        [ 0.1256, -0.5387],\n",
      "        [-0.6280,  0.0619],\n",
      "        [-0.3768, -0.2293],\n",
      "        [ 0.1256, -1.3941],\n",
      "        [ 1.8839, -1.3941],\n",
      "        [-0.6280,  0.9355],\n",
      "        [-0.1256,  1.2267],\n",
      "        [ 1.6327, -0.2475]])\n",
      "Sample 12: tensor([[ 0.5933,  0.6426],\n",
      "        [ 1.9893,  1.0092],\n",
      "        [-0.8027,  1.0092],\n",
      "        [-0.8027,  0.6426],\n",
      "        [-0.4537, -0.0905],\n",
      "        [-0.4537,  0.2761],\n",
      "        [ 1.2913, -1.7515],\n",
      "        [ 0.5933, -1.1902],\n",
      "        [-0.8027,  0.8259],\n",
      "        [-1.1517, -1.3735]])\n",
      "Sample 13: tensor([[-0.8580, -2.0977],\n",
      "        [-0.6754, -0.2128],\n",
      "        [ 2.2453, -0.8411],\n",
      "        [ 0.2373, -0.2128],\n",
      "        [ 0.0548,  0.0385],\n",
      "        [ 0.6024, -0.7155],\n",
      "        [-0.3103,  0.7924],\n",
      "        [ 0.2373,  0.7846],\n",
      "        [-1.7707,  1.2951],\n",
      "        [ 0.2373,  1.1694]])\n",
      "Sample 14: tensor([[ 0.5540, -0.0440],\n",
      "        [ 0.8804,  0.2556],\n",
      "        [-1.3946, -0.9428],\n",
      "        [-1.7312, -0.3436],\n",
      "        [-0.7519, -0.5028],\n",
      "        [ 0.3907, -1.2330],\n",
      "        [-0.7519,  1.2948],\n",
      "        [ 0.7172, -0.7836],\n",
      "        [ 1.0436,  0.0964],\n",
      "        [ 1.0436,  2.2029]])\n",
      "Sample 15: tensor([[-0.5977, -0.0980],\n",
      "        [ 0.4417,  1.6441],\n",
      "        [ 0.7016,  0.3998],\n",
      "        [-0.8575, -1.0935],\n",
      "        [-0.0780, -0.8446],\n",
      "        [-1.1174, -0.5957],\n",
      "        [-0.8575, -1.0935],\n",
      "        [ 1.2213,  1.8774],\n",
      "        [ 2.0008,  0.1509],\n",
      "        [-0.8575, -0.3469]])\n",
      "Sample 16: tensor([[-0.8075,  0.1488],\n",
      "        [ 1.0196,  1.8093],\n",
      "        [ 1.8026, -0.5893],\n",
      "        [ 0.2202,  0.5178],\n",
      "        [-0.0245, -0.2318],\n",
      "        [-0.0245,  0.7023],\n",
      "        [-0.0245,  0.1488],\n",
      "        [-2.1125, -0.5893],\n",
      "        [ 0.4975,  0.3333],\n",
      "        [-0.5465, -2.2498]])\n",
      "Sample 17: tensor([[ 0.3595, -0.9400],\n",
      "        [-0.6979,  1.4130],\n",
      "        [ 0.3727,  0.8248],\n",
      "        [ 1.1922,  1.2170],\n",
      "        [-1.5438, -0.9400],\n",
      "        [ 1.2054, -1.5282],\n",
      "        [ 0.3595, -0.3517],\n",
      "        [-1.5438,  1.0209],\n",
      "        [-0.6979, -0.7562],\n",
      "        [ 0.9939,  0.0404]])\n",
      "Sample 18: tensor([[ 1.2851,  1.5044],\n",
      "        [ 0.2511, -0.8085],\n",
      "        [-1.3409, -0.8085],\n",
      "        [ 1.0225, -0.9737],\n",
      "        [-0.2905, -1.1389],\n",
      "        [ 1.0225,  1.5044],\n",
      "        [-1.0783, -0.4781],\n",
      "        [-0.5531, -0.4781],\n",
      "        [ 1.0225,  1.0088],\n",
      "        [-1.3409,  0.6681]])\n",
      "Sample 19: tensor([[ 0.0670, -0.0566],\n",
      "        [ 2.0778, -1.3496],\n",
      "        [ 0.0670, -0.9186],\n",
      "        [ 0.3351,  0.9491],\n",
      "        [-1.1394, -0.2002],\n",
      "        [-0.6032,  0.8055],\n",
      "        [ 1.1394, -0.6313],\n",
      "        [-0.4692, -0.3439],\n",
      "        [-1.5416,  2.2332],\n",
      "        [ 0.0670, -0.4876]])\n",
      "Sample 20: tensor([[ 0.1979, -0.0681],\n",
      "        [ 0.3778, -1.9230],\n",
      "        [ 0.0180, -0.5318],\n",
      "        [ 1.2775, -0.0681],\n",
      "        [ 1.4574,  0.1493],\n",
      "        [ 0.7377, -0.3000],\n",
      "        [-0.3419,  1.3230],\n",
      "        [-1.9612, -0.9955],\n",
      "        [-0.8816,  0.8593],\n",
      "        [-0.8816,  1.5549]])\n",
      "Sample 21: tensor([[ 1.0077, -0.2635],\n",
      "        [ 0.4909, -1.0196],\n",
      "        [-0.0258, -0.8306],\n",
      "        [ 0.4748,  0.6817],\n",
      "        [-1.8345,  2.1822],\n",
      "        [ 0.4909, -0.8188],\n",
      "        [ 1.2661,  0.8589],\n",
      "        [-0.0258,  0.6817],\n",
      "        [-0.0258, -0.8306],\n",
      "        [-1.8184, -0.6415]])\n",
      "Sample 22: tensor([[ 1.8574, -0.9076],\n",
      "        [ 1.2127,  0.2886],\n",
      "        [-0.5884, -0.6684],\n",
      "        [-0.7522, -0.9076],\n",
      "        [-0.9159,  0.5278],\n",
      "        [ 1.3764,  1.7091],\n",
      "        [-0.9159,  1.4848],\n",
      "        [-0.4247,  0.5278],\n",
      "        [-0.5884, -0.9076],\n",
      "        [-0.2610, -1.1469]])\n",
      "Sample 23: tensor([[ 1.4223,  1.1831],\n",
      "        [-1.1706, -0.6592],\n",
      "        [-0.4871,  0.1084],\n",
      "        [ 1.5850, -0.3521],\n",
      "        [ 0.2181, -0.2082],\n",
      "        [-1.3659, -0.1986],\n",
      "        [-1.1706, -1.8874],\n",
      "        [ 0.0336,  0.5690],\n",
      "        [ 0.7280,  1.9507],\n",
      "        [ 0.2072, -0.5057]])\n",
      "Sample 24: tensor([[-0.0157, -0.2521],\n",
      "        [ 1.3113, -0.7948],\n",
      "        [-0.3500,  1.5569],\n",
      "        [-1.5098,  1.1838],\n",
      "        [-0.1829,  0.1097],\n",
      "        [ 0.6530,  0.1097],\n",
      "        [ 1.3113, -1.6993],\n",
      "        [ 0.4859,  0.2906],\n",
      "        [ 0.1515,  0.8333],\n",
      "        [-1.8546, -1.3375]])\n",
      "Sample 25: tensor([[-2.5852e-01,  2.0880e-01],\n",
      "        [-1.0132e+00,  1.0388e+00],\n",
      "        [ 7.6914e-01, -1.0362e+00],\n",
      "        [ 7.6914e-01, -4.2667e-01],\n",
      "        [ 2.0537e+00, -1.2437e+00],\n",
      "        [-1.6058e-03,  1.2463e+00],\n",
      "        [-1.5431e+00,  4.1630e-01],\n",
      "        [-1.0293e+00, -1.6587e+00],\n",
      "        [-1.6058e-03,  1.2463e+00],\n",
      "        [ 2.5531e-01,  2.0880e-01]])\n",
      "Sample 26: tensor([[-0.5894,  0.4005],\n",
      "        [-0.4301, -1.0975],\n",
      "        [-0.7487,  0.0676],\n",
      "        [-0.5894,  1.8881],\n",
      "        [ 1.1628,  0.0676],\n",
      "        [ 0.2071, -0.2653],\n",
      "        [-0.1115, -1.4303],\n",
      "        [ 2.2779, -1.2639],\n",
      "        [ 0.2071,  0.7334],\n",
      "        [-1.3858,  0.8998]])\n",
      "Sample 27: tensor([[-0.4297, -1.1678],\n",
      "        [ 1.0027,  0.3182],\n",
      "        [ 0.6446,  0.7391],\n",
      "        [-1.8621,  1.1599],\n",
      "        [-1.5040, -0.5234],\n",
      "        [ 0.6446, -1.1678],\n",
      "        [ 0.2865,  1.5807],\n",
      "        [ 0.2641, -0.3130],\n",
      "        [-0.4297,  0.7391],\n",
      "        [ 1.3831, -1.3650]])\n",
      "Sample 28: tensor([[ 2.0094e+00, -2.3561e+00],\n",
      "        [ 3.4063e-01,  1.9651e-03],\n",
      "        [ 9.9781e-02, -7.8407e-01],\n",
      "        [-1.2937e+00,  1.5917e-01],\n",
      "        [ 8.2578e-02,  1.4935e-01],\n",
      "        [ 1.1836e+00, -1.5524e-01],\n",
      "        [-1.5690e+00,  1.1024e+00],\n",
      "        [-1.9268e-01,  1.9651e-03],\n",
      "        [-4.6794e-01,  3.1638e-01],\n",
      "        [-1.9268e-01,  1.5642e+00]])\n",
      "Sample 29: tensor([[-0.2397,  0.1455],\n",
      "        [-0.0553, -1.1176],\n",
      "        [-1.8993, -1.6589],\n",
      "        [-0.7929, -0.2154],\n",
      "        [ 1.4199, -0.3958],\n",
      "        [-0.9773, -0.2154],\n",
      "        [-0.0553, -0.3958],\n",
      "        [ 0.3135,  1.7695],\n",
      "        [ 1.2355,  0.6755],\n",
      "        [ 1.0511,  1.4086]])\n",
      "Sample 30: tensor([[-1.2423e+00,  1.9065e-01],\n",
      "        [ 2.0704e+00,  1.3636e-08],\n",
      "        [ 5.7972e-01,  2.0971e+00],\n",
      "        [-1.5735e+00,  1.1439e+00],\n",
      "        [ 5.7972e-01, -5.7194e-01],\n",
      "        [ 8.2817e-02, -1.5252e+00],\n",
      "        [ 4.1409e-01, -3.8129e-01],\n",
      "        [-8.2817e-02,  5.6002e-01],\n",
      "        [ 8.2817e-02, -7.6259e-01],\n",
      "        [-9.1099e-01, -7.5067e-01]])\n",
      "Sample 31: tensor([[ 1.1867, -0.8584],\n",
      "        [-0.2967,  0.5353],\n",
      "        [-0.2967,  1.9166],\n",
      "        [-0.5439, -1.2407],\n",
      "        [-1.2856, -0.4514],\n",
      "        [ 1.9284, -0.2541],\n",
      "        [-1.5329, -1.0434],\n",
      "        [ 0.4450, -0.2664],\n",
      "        [ 0.4450,  0.1406],\n",
      "        [-0.0494,  1.5219]])\n",
      "Sample 32: tensor([[-0.9243,  0.4784],\n",
      "        [-0.1094, -1.3968],\n",
      "        [-0.7490,  0.9898],\n",
      "        [ 0.8913, -1.2263],\n",
      "        [ 1.5413, -0.7255],\n",
      "        [ 0.7263, -0.2035],\n",
      "        [ 0.8913, -0.5444],\n",
      "        [ 0.4065,  2.0126],\n",
      "        [-0.9243,  0.6488],\n",
      "        [-1.7496, -0.0330]])\n",
      "Sample 33: tensor([[ 1.1273, -0.2313],\n",
      "        [ 0.4969, -0.6957],\n",
      "        [-1.2459, -1.0054],\n",
      "        [-0.0593, -0.3861],\n",
      "        [ 0.5340,  1.6169],\n",
      "        [-0.0593, -0.3861],\n",
      "        [-0.6526, -1.1602],\n",
      "        [-1.8022, -0.5409],\n",
      "        [-0.0593,  1.4718],\n",
      "        [ 1.7206,  1.3170]])\n",
      "Sample 34: tensor([[ 0.0651,  1.9206],\n",
      "        [-0.9983, -0.1906],\n",
      "        [-1.6496,  0.0024],\n",
      "        [ 1.1286, -0.9627],\n",
      "        [ 1.3413, -1.9279],\n",
      "        [ 0.2778,  0.7745],\n",
      "        [-1.2110, -0.5767],\n",
      "        [-0.5729,  0.5694],\n",
      "        [ 0.7032, -0.1906],\n",
      "        [ 0.9159,  0.5815]])\n",
      "Sample 35: tensor([[-4.1424e-08,  1.7763e+00],\n",
      "        [-8.6874e-01,  8.5594e-02],\n",
      "        [ 1.0859e+00, -8.3481e-02],\n",
      "        [ 1.7375e+00,  9.2040e-01],\n",
      "        [-1.0859e+00,  4.2374e-01],\n",
      "        [-4.1424e-08,  8.5594e-02],\n",
      "        [ 8.6874e-01, -8.3481e-02],\n",
      "        [-2.1719e-01, -1.6052e+00],\n",
      "        [ 2.1719e-01, -1.7742e+00],\n",
      "        [-1.7375e+00,  2.5467e-01]])\n",
      "Sample 36: tensor([[-0.1423,  1.6174],\n",
      "        [ 0.6707, -0.6048],\n",
      "        [ 1.6869, -0.9484],\n",
      "        [-0.9552, -0.5819],\n",
      "        [-0.9552, -0.5819],\n",
      "        [-0.1423, -0.5819],\n",
      "        [-0.3455,  0.1512],\n",
      "        [ 1.4836, -0.9484],\n",
      "        [ 0.2642,  0.4948],\n",
      "        [-1.5649,  1.9839]])\n",
      "Sample 37: tensor([[ 0.0806, -1.0513],\n",
      "        [-1.1282, -0.4060],\n",
      "        [-1.1282,  0.0242],\n",
      "        [-0.1209, -0.1909],\n",
      "        [ 1.8937, -1.0647],\n",
      "        [-0.3223,  0.6695],\n",
      "        [-0.9267,  2.3903],\n",
      "        [ 1.6922,  0.4544],\n",
      "        [ 0.0806, -1.0647],\n",
      "        [-0.1209,  0.2393]])\n",
      "Sample 38: tensor([[ 0.0501,  0.0588],\n",
      "        [ 0.3004, -0.8095],\n",
      "        [ 2.0526,  0.9271],\n",
      "        [-0.2003,  0.9271],\n",
      "        [ 0.0501, -0.2306],\n",
      "        [-0.2003,  0.6377],\n",
      "        [ 1.0513, -1.6778],\n",
      "        [-1.9524, -1.2527],\n",
      "        [-0.7009, -0.2306],\n",
      "        [-0.4506,  1.6507]])\n",
      "Sample 39: tensor([[ 0.2810, -1.0246],\n",
      "        [ 0.2810,  1.6776],\n",
      "        [-0.7107,  0.0450],\n",
      "        [-0.3801,  0.2364],\n",
      "        [-1.5370,  0.0563],\n",
      "        [-0.0496, -1.9253],\n",
      "        [ 2.2642,  1.4974],\n",
      "        [-0.2149, -0.1238],\n",
      "        [-0.8759, -0.3040],\n",
      "        [ 0.9421, -0.1351]])\n",
      "Prediction number: 0\n",
      "Prediction number: 1\n",
      "Prediction number: 2\n",
      "Prediction number: 3\n",
      "Prediction number: 4\n",
      "Prediction number: 5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction number: 6\n",
      "Prediction number: 7\n",
      "Prediction number: 8\n",
      "Prediction number: 9\n",
      "Prediction number: 10\n",
      "Prediction number: 11\n",
      "Prediction number: 12\n",
      "Prediction number: 13\n",
      "Prediction number: 14\n",
      "Prediction number: 15\n",
      "Prediction number: 16\n",
      "Prediction number: 17\n",
      "Prediction number: 18\n",
      "Prediction number: 19\n",
      "Prediction number: 20\n",
      "Prediction number: 21\n",
      "Prediction number: 22\n",
      "Prediction number: 23\n",
      "Prediction number: 24\n",
      "Prediction number: 25\n",
      "Prediction number: 26\n",
      "Prediction number: 27\n",
      "Prediction number: 28\n",
      "Prediction number: 29\n",
      "Prediction number: 30\n",
      "Prediction number: 31\n",
      "Prediction number: 32\n",
      "Prediction number: 33\n",
      "Prediction number: 34\n",
      "Prediction number: 35\n",
      "Prediction number: 36\n",
      "Prediction number: 37\n",
      "Prediction number: 38\n",
      "Prediction number: 39\n",
      "Prediction number: 40\n",
      "Prediction number: 41\n",
      "Prediction number: 42\n",
      "Prediction number: 43\n",
      "Prediction number: 44\n",
      "Prediction number: 45\n",
      "Prediction number: 46\n",
      "Prediction number: 47\n",
      "Prediction number: 48\n",
      "Prediction number: 49\n",
      "Prediction number: 50\n",
      "Prediction number: 51\n",
      "Prediction number: 52\n",
      "Prediction number: 53\n",
      "Prediction number: 54\n",
      "Prediction number: 55\n",
      "Prediction number: 56\n",
      "Prediction number: 57\n",
      "Prediction number: 58\n",
      "Prediction number: 59\n",
      "Prediction number: 60\n",
      "Prediction number: 61\n",
      "Prediction number: 62\n",
      "Prediction number: 63\n",
      "Prediction number: 64\n",
      "Prediction number: 65\n",
      "Prediction number: 66\n",
      "Prediction number: 67\n",
      "Prediction number: 68\n",
      "Prediction number: 69\n",
      "Prediction number: 70\n",
      "Prediction number: 71\n",
      "Prediction number: 72\n",
      "Prediction number: 73\n",
      "Prediction number: 74\n",
      "Prediction number: 75\n",
      "Prediction number: 76\n",
      "Prediction number: 77\n",
      "Prediction number: 78\n",
      "Prediction number: 79\n",
      "Prediction number: 80\n",
      "Prediction number: 81\n",
      "Prediction number: 82\n",
      "Prediction number: 83\n",
      "Prediction number: 84\n",
      "Prediction number: 85\n",
      "Prediction number: 86\n",
      "Prediction number: 87\n",
      "Prediction number: 88\n",
      "Prediction number: 89\n",
      "Prediction number: 90\n",
      "Prediction number: 91\n",
      "Prediction number: 92\n",
      "Prediction number: 93\n",
      "Prediction number: 94\n",
      "Prediction number: 95\n",
      "Prediction number: 96\n",
      "Prediction number: 97\n",
      "Prediction number: 98\n",
      "Prediction number: 99\n",
      "reconstruction_error.shape: (4000, 2)\n",
      "Number of NaNs in reconstruction_error: 0\n",
      "max_error_per_sequence: [0.9988621 1.0000337 1.0002687 ... 1.0014954 1.0013319 1.0016761]\n",
      "max_error_per_sequence.shape: (4000,)\n",
      "threshold1: 1.002755832672119\n",
      "threshold percentile: 1.002046811580658\n",
      "len(is_intrusion_detected): 4000\n",
      "is_intrusion_detected.shape: (4000,)\n",
      "num_total_sequences: 4000\n",
      "num_total_sequences2: 3900\n",
      "        Part  True Count  False Count\n",
      "0       Real          80         3920\n",
      "1  Imaginary           0         4000\n",
      "2    Overall          80         7920\n",
      "Number of jamming sequences detected: 80 out of 8000 sequences\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import LSTM, Dense, RepeatVector, TimeDistributed, Layer, MultiHeadAttention\n",
    "from tensorflow.keras.layers import Bidirectional, Dropout\n",
    "from tensorflow.keras.layers import Masking, Input, Lambda\n",
    "import tensorflow_probability as tfp\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.losses import mse\n",
    "from numpy.fft import fft\n",
    "from scipy.stats import skew, kurtosis \n",
    "import pandas as pd\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import struct\n",
    "import glob\n",
    "from sklearn.metrics import confusion_matrix, roc_auc_score, roc_curve, classification_report, accuracy_score\n",
    "\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import struct\n",
    "from numpy.fft import fft\n",
    "from scipy.stats import skew, kurtosis\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "\n",
    "# Check if CUDA is available and set the device to GPU if it is, otherwise CPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "class ComplexNumbersDataset(Dataset):\n",
    "    def __init__(self, filepath, sequence_length, max_samples=None, for_training=True, process_method='data1'):\n",
    "        self.filepath = filepath\n",
    "        self.sequence_length = sequence_length\n",
    "        self.max_samples = max_samples\n",
    "        self.for_training = for_training\n",
    "        self.process_method = process_method\n",
    "        self.samples = self.load_samples()\n",
    "        self.samples_per_sequence = self.sequence_length\n",
    "        #self.samples_per_sequence = self.sequence_length if for_training else 1\n",
    "\n",
    "    def load_samples(self):\n",
    "        samples = []\n",
    "        with open(self.filepath, 'rb') as binary_file:\n",
    "            while True:\n",
    "                if self.max_samples and len(samples) >= self.max_samples:\n",
    "                    break\n",
    "                binary_data = binary_file.read(8)\n",
    "                if not binary_data:\n",
    "                    break\n",
    "                decoded_data = struct.unpack('ff', binary_data)\n",
    "                if decoded_data[0] == 0 and decoded_data[1] == 0:\n",
    "                    continue\n",
    "                samples.append(f\"{decoded_data[0]}+{decoded_data[1]}j\")\n",
    "        return samples\n",
    "\n",
    "    def process_data1(self, sequence_samples):\n",
    "        #print('sequence_samples:', sequence_samples)\n",
    "        #print('len(sequence_samples):', len(sequence_samples))\n",
    "        real_parts = []\n",
    "        imag_parts = []\n",
    "        for sample in sequence_samples:\n",
    "            # Remove potential unwanted characters (spaces, etc.)\n",
    "            sample = sample.replace(\" \", \"\").replace(\"+-\", \"-\")\n",
    "            try:\n",
    "                c = complex(sample)\n",
    "                real_parts.append(c.real)\n",
    "                imag_parts.append(c.imag)\n",
    "            except ValueError:\n",
    "                print(f\"Failed to convert: {sample}\")  # This will show which string failed\n",
    "                raise\n",
    "        real_parts = np.array(real_parts)\n",
    "        imag_parts = np.array(imag_parts)\n",
    "        # Normalize\n",
    "        real_parts = (real_parts - np.mean(real_parts)) / np.std(real_parts)\n",
    "        imag_parts = (imag_parts - np.mean(imag_parts)) / np.std(imag_parts)\n",
    "\n",
    "        # Combining real and imaginary parts\n",
    "        X = np.stack((real_parts, imag_parts), axis=1)  # Shape: (sequence_length, 2)\n",
    "        return torch.tensor(X, dtype=torch.float32)\n",
    "\n",
    "    def process_data2(self, sequence_samples):\n",
    "        samples_array = np.array([complex(sample) for sample in sequence_samples], dtype=np.complex64)\n",
    "        samples_fft = fft(samples_array)\n",
    "        real_parts = np.real(samples_fft)\n",
    "        imag_parts = np.imag(samples_fft)\n",
    "        \n",
    "        # Normalization\n",
    "        epsilon = 1e-10\n",
    "        real_parts_normalized = (real_parts - np.mean(real_parts)) / (np.std(real_parts) + epsilon)\n",
    "        imag_parts_normalized = (imag_parts - np.mean(imag_parts)) / (np.std(imag_parts) + epsilon)\n",
    "        \n",
    "        # Feature extraction\n",
    "        features = np.column_stack((\n",
    "            np.mean(real_parts_normalized), np.std(real_parts_normalized), skew(real_parts_normalized),\n",
    "            kurtosis(real_parts_normalized), np.mean(imag_parts_normalized), np.std(imag_parts_normalized),\n",
    "            skew(imag_parts_normalized), kurtosis(imag_parts_normalized)\n",
    "        ))\n",
    "        \n",
    "        return torch.tensor(features, dtype=torch.float32).reshape(self.samples_per_sequence, -1)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples) // self.samples_per_sequence\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        start_idx = idx * self.samples_per_sequence\n",
    "        end_idx = start_idx + self.samples_per_sequence\n",
    "        sequence_samples = self.samples[start_idx:end_idx]\n",
    "        \n",
    "        if self.process_method == 'data1':\n",
    "            X = self.process_data1(sequence_samples)\n",
    "        elif self.process_method == 'data2':\n",
    "            X = self.process_data2(sequence_samples)\n",
    "        else:\n",
    "            raise ValueError(\"Invalid process method specified.\")\n",
    "        if self.for_training:\n",
    "            return X, X\n",
    "        else:\n",
    "            return X\n",
    "\n",
    "#------------------------------------------------------------------------------------------------------\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "def mec_kocaoglu_np(p, q):\n",
    "    \"\"\"\n",
    "    Compute the joint distribution matrix with minimal entropy between two given distributions in PyTorch.\n",
    "    \"\"\"\n",
    "    p = p.float() / p.sum()\n",
    "    q = q.float() / q.sum()\n",
    "    J = torch.zeros(q.size(0), p.size(0), dtype=torch.float64)\n",
    "    M = torch.stack([p, q], dim=0)\n",
    "    r = torch.min(torch.max(M, dim=1).values)\n",
    "\n",
    "    while r > 0:\n",
    "        a_i = torch.argmax(M, dim=1)\n",
    "        r_updated = torch.min(torch.max(M, dim=1).values)\n",
    "        update_values = torch.stack([r, r])\n",
    "        for i, index in enumerate(a_i):\n",
    "            M[i, index] -= update_values[i]\n",
    "            J[index, i] += update_values[i]\n",
    "        r = r_updated\n",
    "\n",
    "    return J\n",
    "def apply_mec_to_data(data, num_bins=10, latent_dim=50):\n",
    "    \"\"\"\n",
    "    Apply the MEC transformation to each sample in the data.\n",
    "    \"\"\"\n",
    "    def process_sample(sample):\n",
    "        # Find the min and max values in the sample and convert them to Python scalars\n",
    "        min_val = torch.min(sample).item()\n",
    "        max_val = torch.max(sample).item()\n",
    "        \n",
    "        # Now min_val and max_val are Python numbers, suitable for torch.histc()\n",
    "        sample_histogram = torch.histc(sample, bins=num_bins, min=min_val, max=max_val)\n",
    "        sample_histogram = sample_histogram.float() / sample_histogram.sum()\n",
    "\n",
    "        mec_transformed = mec_kocaoglu_np(sample_histogram, sample_histogram)\n",
    "\n",
    "        # Flatten the 2D to 1D and adjust to match the latent_dim\n",
    "        transformed_sample = mec_transformed.flatten()\n",
    "        if transformed_sample.size(0) > latent_dim:\n",
    "            transformed_sample = transformed_sample[:latent_dim]\n",
    "        else:\n",
    "            padding = torch.zeros(latent_dim - transformed_sample.size(0), dtype=torch.float64)\n",
    "            transformed_sample = torch.cat([transformed_sample, padding], dim=0)\n",
    "\n",
    "        return transformed_sample\n",
    "\n",
    "    # Process each sample in the batch\n",
    "    transformed_batch = torch.stack([process_sample(sample) for sample in data])\n",
    "    transformed_batch = transformed_batch.float() \n",
    "\n",
    "    return transformed_batch\n",
    "\n",
    "# def apply_mec_to_data(data, num_bins=10, latent_dim=50):\n",
    "#     \"\"\"\n",
    "#     Apply the MEC transformation to each sample in the data.\n",
    "#     \"\"\"\n",
    "#     def process_sample(sample):\n",
    "#         min_val, max_val = torch.min(sample), torch.max(sample)\n",
    "#         # Histogram as a way to represent the sample distribution\n",
    "#         sample_histogram = torch.histc(sample, bins=num_bins, min=min_val, max=max_val)\n",
    "#         sample_histogram = sample_histogram.float() / sample_histogram.sum()\n",
    "\n",
    "#         mec_transformed = mec_kocaoglu_np(sample_histogram, sample_histogram)\n",
    "\n",
    "#         # Flatten the 2D to 1D and adjust to match the latent_dim\n",
    "#         transformed_sample = mec_transformed.flatten()\n",
    "#         if transformed_sample.size(0) > latent_dim:\n",
    "#             transformed_sample = transformed_sample[:latent_dim]\n",
    "#         else:\n",
    "#             padding = torch.zeros(latent_dim - transformed_sample.size(0), dtype=torch.float64)\n",
    "#             transformed_sample = torch.cat([transformed_sample, padding], dim=0)\n",
    "\n",
    "#         return transformed_sample\n",
    "\n",
    "#     # Process each sample in the batch\n",
    "#     transformed_batch = torch.stack([process_sample(sample) for sample in data])\n",
    "\n",
    "#     return transformed_batch\n",
    "\n",
    "def process_latent_variables(z):\n",
    "    \"\"\"\n",
    "    Transform latent variables using MEC.\n",
    "    \"\"\"\n",
    "    # Assuming 'z' is a batch of latent variables\n",
    "    z_transformed = apply_mec_to_data(z)\n",
    "    return z_transformed\n",
    "\n",
    "#self Attention LSTM Autoencoder Model\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "class SelfAttentionLayer(nn.Module):\n",
    "    def __init__(self, embed_size, num_heads):\n",
    "        super(SelfAttentionLayer, self).__init__()\n",
    "        self.multihead_attention = nn.MultiheadAttention(embed_dim=embed_size, num_heads=num_heads)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        # PyTorch's MultiheadAttention expects inputs of shape (sequence_length, batch_size, embed_size)\n",
    "        inputs = inputs.permute(1, 0, 2)\n",
    "        attn_output, _ = self.multihead_attention(inputs, inputs, inputs)\n",
    "        return attn_output.permute(1, 0, 2)  # Return to original shape (batch_size, sequence_length, embed_size)\n",
    "\n",
    "# Variational Autoencoder (VAE) Class\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, sequence_length, feature_dim, intermediate_dim, latent_dim, epsilon_std=0.1):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.lstm1 = nn.LSTM(input_size=feature_dim, hidden_size=intermediate_dim, batch_first=True)\n",
    "        self.self_attention = SelfAttentionLayer(embed_size=intermediate_dim, num_heads=2)  # Ensure compatibility\n",
    "        #self.self_attention = SelfAttentionLayer(num_heads=2, key_dim=intermediate_dim)\n",
    "        self.lstm2 = nn.LSTM(input_size=intermediate_dim, hidden_size=50, batch_first=True)\n",
    "        self.z_mean = nn.Linear(in_features=50, out_features=latent_dim)\n",
    "        self.z_log_var = nn.Linear(in_features=50, out_features=latent_dim)\n",
    "        self.epsilon_std = epsilon_std\n",
    "\n",
    "    def _sampling3(self, z_mean):\n",
    "        # Assuming process_latent_variables(z) is adapted for PyTorch\n",
    "        z_mean_transformed = process_latent_variables(z_mean)\n",
    "        eps = torch.randn_like(z_mean_transformed) * self.epsilon_std\n",
    "        return z_mean_transformed\n",
    "    \n",
    "    def _sampling(self, z_mean, z_log_var):\n",
    "        # Standard sampling function, kept for compatibility\n",
    "        std = torch.exp(0.5 * z_log_var)\n",
    "        eps = torch.randn_like(std)\n",
    "        return z_mean + eps * std\n",
    "\n",
    "    def forward(self, x):\n",
    "        x, _ = self.lstm1(x)\n",
    "        x = self.self_attention(x)\n",
    "        x, (h_n, _) = self.lstm2(x)\n",
    "        z_mean = self.z_mean(h_n[-1])\n",
    "        z_log_var = self.z_log_var(h_n[-1])\n",
    "        z = self._sampling3(z_mean)  # Use _sampling3 as required\n",
    "        return z, z_mean, z_log_var\n",
    "    \n",
    "# class Decoder(nn.Module):\n",
    "#     def __init__(self, sequence_length, feature_dim, latent_dim):\n",
    "#         super(Decoder, self).__init__()\n",
    "#         self.repeat_vector = nn.RepeatVector(sequence_length)\n",
    "#         self.lstm1 = nn.LSTM(input_size=latent_dim, hidden_size=50, batch_first=True)\n",
    "#         self.lstm2 = nn.LSTM(input_size=50, hidden_size=100, batch_first=True)\n",
    "#         self.output_layer = nn.Linear(in_features=100, out_features=feature_dim)\n",
    "\n",
    "#     def forward(self, z):\n",
    "#         z = self.repeat_vector(z)\n",
    "#         z, _ = self.lstm1(z)\n",
    "#         z, _ = self.lstm2(z)\n",
    "#         return torch.sigmoid(self.output_layer(z))\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, sequence_length, feature_dim, latent_dim):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.sequence_length = sequence_length\n",
    "        self.lstm1 = nn.LSTM(input_size=latent_dim, hidden_size=50, batch_first=True)\n",
    "        self.lstm2 = nn.LSTM(input_size=50, hidden_size=100, batch_first=True)\n",
    "        self.output_layer = nn.Linear(in_features=100, out_features=feature_dim)\n",
    "\n",
    "    def forward(self, z):\n",
    "        # Repeat 'z' for 'sequence_length' times\n",
    "        # Assuming 'z' shape is [batch_size, latent_dim], we first unsqueeze it to add a sequence length dimension\n",
    "        # Then repeat across this dimension\n",
    "        z = z.unsqueeze(1).repeat(1, self.sequence_length, 1)\n",
    "        \n",
    "        # Now 'z' shape is [batch_size, sequence_length, latent_dim], suitable for LSTM input\n",
    "        z, _ = self.lstm1(z)\n",
    "        z, _ = self.lstm2(z)\n",
    "        return torch.sigmoid(self.output_layer(z))\n",
    "\n",
    "class VAE(nn.Module):\n",
    "    def __init__(self, sequence_length, feature_dim, intermediate_dim, latent_dim, epsilon_std=0.1):\n",
    "        super(VAE, self).__init__()\n",
    "        self.encoder = Encoder(sequence_length, feature_dim, intermediate_dim, latent_dim, epsilon_std)\n",
    "        self.decoder = Decoder(sequence_length, feature_dim, latent_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        z, z_mean, z_log_var = self.encoder(x)\n",
    "        reconstructed_x = self.decoder(z)\n",
    "        return reconstructed_x, z_mean, z_log_var\n",
    "\n",
    "def vae_loss(reconstructed_x, x, z_mean, z_log_var):\n",
    "    reconstruction_loss = F.mse_loss(reconstructed_x, x, reduction='sum')\n",
    "    kl_divergence = -0.5 * torch.sum(1 + z_log_var - z_mean.pow(2) - z_log_var.exp())\n",
    "    return reconstruction_loss + kl_divergence\n",
    "\n",
    "\n",
    "\n",
    "# Instantiate and Compile the VAE\n",
    "import torch\n",
    "from torch import optim\n",
    "\n",
    "# Instantiate the VAE\n",
    "sequence_length = 10\n",
    "feature_dim = 2\n",
    "intermediate_dim = 100\n",
    "latent_dim = 50\n",
    "epsilon_std = 0.1\n",
    "\n",
    "#vae_model = VAE(sequence_length, feature_dim, intermediate_dim, latent_dim, epsilon_std)\n",
    "vae_model = VAE(sequence_length, feature_dim, intermediate_dim, latent_dim, epsilon_std).to(device)\n",
    "\n",
    "optimizer = optim.Adam(vae_model.parameters(), lr=0.005)\n",
    "\n",
    "\n",
    "# Model Training\n",
    "batch_size = 40\n",
    "max_train_samples = 100000\n",
    "train_steps = max_train_samples // (batch_size * sequence_length)\n",
    "max_samples = 100000  # Maximum samples to read (or None to read all)\n",
    "max_test_samples = 100000\n",
    "\n",
    "pure_file_pattern = '/home/mreza/5G accelerator/ID_MEC/data generator/pure_data/pure_iq_samples_*.csv'\n",
    "mixed_file_pattern = '/home/mreza/5G accelerator/ID_MEC/data generator/mixed_data/mixed_iq_samples_*.csv'\n",
    "pure_file_new = '/home/mreza/5G accelerator/ID_MEC/data generator/New Data-Collection/rx_IQ_pure'\n",
    "mixed_file_new = '/home/mreza/5G accelerator/ID_MEC/data generator/New Data-Collection/rx_IQ_MIX'\n",
    "pure_file_old = '/home/mreza/5G accelerator/IQ_samples/data collected/5G_DL_IQ_no_jamming_0924.dat'\n",
    "mixed_file_old = '/home/mreza/5G accelerator/IQ_samples/data collected/5G_DL_IQ_with_periodic_jamming_0928_02.dat'\n",
    "\n",
    "# Creating dataset instances for training and validation/testing\n",
    "train_gen_instance  = ComplexNumbersDataset(filepath=pure_file_new, \n",
    "                                            sequence_length=sequence_length, \n",
    "                                            max_samples=max_train_samples, for_training=True, \n",
    "                                            process_method='data1')\n",
    "\n",
    "combined_gen_instance = ComplexNumbersDataset(filepath=mixed_file_new, \n",
    "                                              sequence_length=sequence_length, \n",
    "                                              max_samples=max_samples, for_training=False, \n",
    "                                              process_method='data1')\n",
    "\n",
    "# Creating DataLoader instances for batching\n",
    "train_gen_instance = DataLoader(train_gen_instance, batch_size=batch_size, shuffle=False, \n",
    "                                drop_last=True)\n",
    "combined_gen_instance = DataLoader(combined_gen_instance, batch_size=batch_size, shuffle=False, \n",
    "                                   drop_last=True)\n",
    "\n",
    "# Custom training loop in PyTorch\n",
    "num_epochs = 2\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
    "    vae_model.train()  # Set the model to training mode\n",
    "    \n",
    "    step = 0  # Initialize step counter if you need it for logging, etc.\n",
    "    for X_chunk, Y_chunk in train_gen_instance:\n",
    "        #X_chunk, Y_chunk = X_chunk.float(), Y_chunk.float()\n",
    "        # Move data to the current device (GPU or CPU)\n",
    "        #print('X_chunk:', X_chunk)\n",
    "        #print('Y_chunk:', Y_chunk)\n",
    "        X_chunk, Y_chunk = X_chunk.to(device), Y_chunk.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        reconstructed_x, z_mean, z_log_var = vae_model(X_chunk)\n",
    "        #loss = vae_loss(reconstructed_x, X_chunk, z_mean, z_log_var)\n",
    "        loss = F.mse_loss(reconstructed_x, X_chunk, reduction='sum')\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if step % 100 == 0:\n",
    "            print(f\"Step {step + 1}/{train_steps}, Loss: {loss}\")\n",
    "        step += 1  # Increment step counter\n",
    "\n",
    "    print()\n",
    "\n",
    "\n",
    "num_predictions = 100  # or any other large number\n",
    "print(f\"Number of predictions to be performed: {num_predictions}\")\n",
    "reconstruction_errors = []\n",
    "all_X_chunk_test = []\n",
    "all_X_chunk_pred = []\n",
    "all_intrusion_flags = []\n",
    "vae_model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "# Assuming 'prediction_loader' is your DataLoader instance\n",
    "# Print details of combined_gen_instance DataLoader\n",
    "print(\"Prediction DataLoader Details:\")\n",
    "print(\"Batch size:\", combined_gen_instance.batch_size)\n",
    "print(\"Number of batches:\", len(combined_gen_instance))\n",
    "print(\"Dataset used in DataLoader:\", type(combined_gen_instance.dataset).__name__)\n",
    "print(\"Number of samples in Dataset:\", len(combined_gen_instance.dataset))\n",
    "\n",
    "# Print the first few samples from the dataset used in the DataLoader\n",
    "for i in range(min(batch_size, len(combined_gen_instance.dataset))):  # Adjust the range as needed\n",
    "    sample = combined_gen_instance.dataset[i]\n",
    "    print(f\"Sample {i}:\", sample)\n",
    "\n",
    "# Get an iterator from your DataLoader\n",
    "combined_gen_iterator = iter(combined_gen_instance)\n",
    "\n",
    "try:\n",
    "    for _ in range(num_predictions):\n",
    "        print(f\"Prediction number: {_}\")\n",
    "        X_chunk_test = next(combined_gen_iterator)  # Getting the batch\n",
    "\n",
    "        # Ensure X_chunk_test is on the correct device\n",
    "        X_chunk_test = X_chunk_test.to(device)\n",
    "\n",
    "        with torch.no_grad():  # Inference without gradient calculation\n",
    "            X_chunk_pred, _, _ = vae_model(X_chunk_test)\n",
    "\n",
    "        chunk_errors = torch.mean((X_chunk_test - X_chunk_pred) ** 2, axis=1)\n",
    "        reconstruction_errors.extend(chunk_errors.cpu().numpy())        \n",
    "        all_X_chunk_test.append(X_chunk_test)\n",
    "        all_X_chunk_pred.append(X_chunk_pred)\n",
    "except StopIteration:\n",
    "    print(\"All samples processed.\")\n",
    "\n",
    "reconstruction_error = np.array(reconstruction_errors)\n",
    "# Further processing...\n",
    "\n",
    "\n",
    "\n",
    "#reconstruction_error = np.array(reconstruction_errors)\n",
    "print('reconstruction_error.shape:', reconstruction_error.shape)\n",
    "print('Number of NaNs in reconstruction_error:', np.isnan(reconstruction_error).sum())\n",
    "max_error_per_sequence = reconstruction_error.max(axis=1) # Max error for each sequence\n",
    "print('max_error_per_sequence:', max_error_per_sequence)\n",
    "\n",
    "print('max_error_per_sequence.shape:', max_error_per_sequence.shape)\n",
    "\n",
    "threshold1 = np.percentile(max_error_per_sequence, 98)\n",
    "print('threshold1:', threshold1)\n",
    "threshold2 = np.percentile(reconstruction_error, 95)\n",
    "print('threshold percentile:', threshold2)\n",
    "\n",
    "is_intrusion_detected = max_error_per_sequence > threshold1  # Boolean array for sequences\n",
    "print('len(is_intrusion_detected):', len(is_intrusion_detected))\n",
    "print('is_intrusion_detected.shape:', is_intrusion_detected.shape)\n",
    "\n",
    "#is_intrusion_detected2 = error_per_sequence > threshold1\n",
    "\n",
    "num_total_sequences = len(max_error_per_sequence)\n",
    "num_total_sequences2 = num_predictions * batch_size - num_predictions\n",
    "print('num_total_sequences:', num_total_sequences)\n",
    "print('num_total_sequences2:', num_total_sequences2)\n",
    "\n",
    "#---------------------------------------finish 111-----------------------------------\n",
    "flat_error_per_sequence = max_error_per_sequence.flatten()\n",
    "#flat_error_per_sequence2 = error_per_sequence.flatten()\n",
    "# Determine if intrusion detected for each sequence\n",
    "for error in flat_error_per_sequence:\n",
    "    all_intrusion_flags.append(error > threshold1)    \n",
    "all_X_chunk_test = np.concatenate(all_X_chunk_test, axis=0)\n",
    "all_X_chunk_pred = np.concatenate(all_X_chunk_pred, axis=0)\n",
    "\n",
    "#save_path = 'C:\\\\Users\\\\Mohammadreza\\\\Desktop\\\\My Class\\\\Proj-DC\\\\My Works\\\\My Papers\\\\intrusion\\\\data generator\\\\intrusion_detected'\n",
    "#plot_with_intrusions8(all_X_chunk_test, all_X_chunk_pred, all_intrusion_flags, sequence_length, save_path)\n",
    "\n",
    "jamming_detected = reconstruction_error > threshold1\n",
    "#train_gen_instance.close()\n",
    "#combined_gen_instance.close()\n",
    "#Table\n",
    "flattened_jamming_detected = jamming_detected.flatten()\n",
    "real_part_detected = jamming_detected[:, 0]\n",
    "imag_part_detected = jamming_detected[:, 1]\n",
    "\n",
    "real_true_count = np.sum(real_part_detected)\n",
    "real_false_count = len(real_part_detected) - real_true_count\n",
    "\n",
    "imag_true_count = np.sum(imag_part_detected)\n",
    "imag_false_count = len(imag_part_detected) - imag_true_count\n",
    "# Overall\n",
    "overall_true_count = np.sum(flattened_jamming_detected)\n",
    "overall_false_count = len(flattened_jamming_detected) - overall_true_count\n",
    "# Table-DataFrame\n",
    "df = pd.DataFrame({\n",
    "    'Part': ['Real', 'Imaginary', 'Overall'],\n",
    "    'True Count': [real_true_count, imag_true_count, overall_true_count],\n",
    "    'False Count': [real_false_count, imag_false_count, overall_false_count]\n",
    "})\n",
    "print(df)\n",
    "num_jamming_detected = np.sum(jamming_detected)\n",
    "print(f\"Number of jamming sequences detected: {num_jamming_detected} out of {len(flattened_jamming_detected)} sequences\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a914d797",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sequence_index' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m n \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m4\u001b[39m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# Ensure n sequences are available\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m n \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmin\u001b[39m(n, \u001b[38;5;28mlen\u001b[39m(X_chunk_test) \u001b[38;5;241m-\u001b[39m \u001b[43msequence_index\u001b[49m)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Convert PyTorch tensors to numpy arrays\u001b[39;00m\n\u001b[1;32m      6\u001b[0m original_sample_np \u001b[38;5;241m=\u001b[39m X_chunk_test[sequence_index:sequence_index \u001b[38;5;241m+\u001b[39m n]\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'sequence_index' is not defined"
     ]
    }
   ],
   "source": [
    "n = 4\n",
    "# Ensure n sequences are available\n",
    "n = min(n, len(X_chunk_test) - sequence_index)\n",
    "\n",
    "# Convert PyTorch tensors to numpy arrays\n",
    "original_sample_np = X_chunk_test[sequence_index:sequence_index + n].detach().cpu().numpy()\n",
    "reconstructed_sample_np = X_chunk_pred[sequence_index:sequence_index + n].detach().cpu().numpy()\n",
    "\n",
    "# Since your data is 3-dimensional (batch, sequence, features), you'll want to concatenate along the sequence axis (axis=1)\n",
    "original_sample_concat = np.concatenate(original_sample_np, axis=0)\n",
    "reconstructed_sample_concat = np.concatenate(reconstructed_sample_np, axis=0)\n",
    "\n",
    "# Plot concatenated sequences\n",
    "plt.figure(figsize=(14, 6))\n",
    "plt.plot(original_sample_concat[:, 0], 'b-', label='Original Real Part')  # Assuming first feature is the real part\n",
    "plt.plot(reconstructed_sample_concat[:, 0], 'r--', label='Reconstructed Real Part')  # Assuming first feature is the real part\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50dd4f2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define the number of sequences to plot together\n",
    "n = 4  # Change this to desired number of sequences\n",
    "\n",
    "# Ensure that we have enough samples for the desired number of sequences\n",
    "sequence_index = np.random.choice(len(X_chunk_test) - n + 1)\n",
    "\n",
    "# Convert PyTorch tensors to numpy arrays and concatenate selected sequences\n",
    "original_sample_concat = np.concatenate(X_chunk_test[sequence_index:sequence_index + n].cpu().numpy(), axis=0)\n",
    "reconstructed_sample_concat = np.concatenate(X_chunk_pred[sequence_index:sequence_index + n].cpu().numpy(), axis=0)\n",
    "\n",
    "# Plot concatenated sequences for n = 4\n",
    "plt.figure(figsize=(14, 6))\n",
    "plt.plot(original_sample_concat[:, 0], 'b-', label='Original Real Part')\n",
    "plt.plot(reconstructed_sample_concat[:, 0], 'r--', label='Reconstructed Real Part')\n",
    "plt.plot(original_sample_concat[:, 1], 'y-', label='Original Imaginary Part')\n",
    "plt.plot(reconstructed_sample_concat[:, 1], 'g--', label='Reconstructed Imaginary Part')\n",
    "plt.title(f'Original vs Reconstructed IQ Data for {n} Sequences of Length {sequence_length}')\n",
    "plt.xlabel('Time Steps')\n",
    "plt.ylabel('Amplitude')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Repeat for n = 2\n",
    "n = 2  # Change this to a different desired number of sequences\n",
    "sequence_index = np.random.choice(len(X_chunk_test) - n + 1)\n",
    "original_sample_concat = np.concatenate(X_chunk_test[sequence_index:sequence_index + n].cpu().numpy(), axis=0)\n",
    "reconstructed_sample_concat = np.concatenate(X_chunk_pred[sequence_index:sequence_index + n].cpu().numpy(), axis=0)\n",
    "\n",
    "# Plot concatenated sequences for n = 2\n",
    "plt.figure(figsize=(14, 6))\n",
    "plt.plot(original_sample_concat[:, 0], 'b-', label='Original Real Part')\n",
    "plt.plot(reconstructed_sample_concat[:, 0], 'r--', label='Reconstructed Real Part')\n",
    "plt.plot(original_sample_concat[:, 1], 'y-', label='Original Imaginary Part')\n",
    "plt.plot(reconstructed_sample_concat[:, 1], 'g--', label='Reconstructed Imaginary Part')\n",
    "plt.title(f'Original vs Reconstructed IQ Data for {n} Sequences of Length {sequence_length}')\n",
    "plt.xlabel('Time Steps')\n",
    "plt.ylabel('Amplitude')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "372d49c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# reconstruction error\n",
    "plt.figure(figsize=(14, 6))\n",
    "plt.plot(reconstruction_error, label='Reconstruction Error')\n",
    "plt.axhline(y=threshold2, color='r', linestyle='--', label='Threshold')\n",
    "plt.title('Reconstruction Error with Threshold')\n",
    "plt.xlabel('Sequence Number')\n",
    "plt.ylabel('Reconstruction Error')\n",
    "plt.legend()\n",
    "# plt.savefig('1-Reconstruction Error with Threshold.png')\n",
    "# plt.close()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4348bb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flatten the reconstruction_error to 1D\n",
    "reconstruction_error_flat = reconstruction_error.flatten()\n",
    "# reconstruction error\n",
    "plt.figure(figsize=(14, 6))\n",
    "plt.plot(reconstruction_error_flat, label='Reconstruction Error')\n",
    "plt.axhline(y=threshold2, color='r', linestyle='--', label='Threshold')\n",
    "plt.title('Reconstruction Error with Threshold')\n",
    "plt.xlabel('Sequence Number')\n",
    "plt.ylabel('Reconstruction Error')\n",
    "plt.legend()\n",
    "# plt.savefig('1-Reconstruction Error with Threshold.png')\n",
    "# plt.close()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab0e1a56",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Histogram of Reconstruction Errors:\n",
    "plt.figure(figsize=(14, 6))\n",
    "plt.hist(reconstruction_error, bins=50, alpha=0.75)\n",
    "plt.axvline(x=threshold2, color='r', linestyle='--', label='Threshold')\n",
    "plt.title('Histogram of Reconstruction Errors')\n",
    "plt.xlabel('Reconstruction Error')\n",
    "plt.ylabel('Frequency')\n",
    "plt.legend()\n",
    "# plt.savefig('4-Histogram of Reconstruction Errors.png')\n",
    "# plt.close()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39a86f70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flatten the reconstruction_error to 1D\n",
    "reconstruction_error_flat = reconstruction_error.flatten()\n",
    "#Histogram of Reconstruction Errors:\n",
    "plt.figure(figsize=(14, 6))\n",
    "plt.hist(reconstruction_error_flat, bins=50, alpha=0.75)\n",
    "plt.axvline(x=threshold2, color='r', linestyle='--', label='Threshold')\n",
    "plt.title('Histogram of Reconstruction Errors')\n",
    "plt.xlabel('Reconstruction Error')\n",
    "plt.ylabel('Frequency')\n",
    "plt.legend()\n",
    "# plt.savefig('4-Histogram of Reconstruction Errors.png')\n",
    "# plt.close()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c4005b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Time Series Plot of IQ Samples:\n",
    "sample_index = np.random.choice(len(X_chunk_test))\n",
    "original_sample = X_chunk_test[sample_index]\n",
    "reconstructed_sample = X_chunk_pred[sample_index]\n",
    "\n",
    "plt.figure(figsize=(14, 6))\n",
    "plt.plot(original_sample[:, 0], 'b-', label='Original Real Part')\n",
    "plt.plot(reconstructed_sample[:, 0], 'r--', label='Reconstructed Real Part')\n",
    "plt.plot(original_sample[:, 1], 'g-', label='Original Imaginary Part')\n",
    "plt.plot(reconstructed_sample[:, 1], 'y--', label='Reconstructed Imaginary Part')\n",
    "plt.title('Original vs Reconstructed IQ Data')\n",
    "plt.xlabel('Time Steps')\n",
    "plt.ylabel('Amplitude')\n",
    "plt.legend()\n",
    "# plt.savefig('5-Original vs Reconstructed IQ Data.png')\n",
    "# plt.close()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dae0391b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert PyTorch tensors to NumPy arrays before calculating means\n",
    "avg_real = np.mean(X_chunk_test.cpu().numpy(), axis=1)[:, 0]\n",
    "avg_imag = np.mean(X_chunk_test.cpu().numpy(), axis=1)[:, 1]\n",
    "\n",
    "# Assuming reconstruction_errors is a NumPy array or a list of errors\n",
    "# If reconstruction_errors is a tensor, ensure to convert it with .cpu().numpy() as well\n",
    "last_errors = np.mean(reconstruction_errors[-len(X_chunk_test):], axis=1)\n",
    "\n",
    "print(\"Shape of avg_real:\", avg_real.shape)\n",
    "print(\"Shape of avg_imag:\", avg_imag.shape)\n",
    "print(\"Shape of last_errors:\", last_errors.shape)\n",
    "\n",
    "plt.figure(figsize=(14, 6))\n",
    "plt.scatter(avg_real, last_errors, label='Real Part', alpha=0.5)\n",
    "plt.axhline(y=threshold2, color='r', linestyle='--', label='Threshold')\n",
    "plt.title('Reconstruction Error vs. Average Real Part')\n",
    "plt.xlabel('Average Amplitude')\n",
    "plt.ylabel('Reconstruction Error')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(14, 6))\n",
    "plt.scatter(avg_imag, last_errors, label='Imaginary Part', alpha=0.5)\n",
    "plt.axhline(y=threshold2, color='r', linestyle='--', label='Threshold')\n",
    "plt.title('Reconstruction Error vs. Average Imaginary Part')\n",
    "plt.xlabel('Average Amplitude')\n",
    "plt.ylabel('Reconstruction Error')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01e87830",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5d4235d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0025906c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reconstruction error\n",
    "reconstruction_error_real = reconstruction_error[:, 0]\n",
    "reconstruction_error_imag = reconstruction_error[:, 1]\n",
    "\n",
    "# Plot for Real Part\n",
    "plt.figure(figsize=(14, 6))\n",
    "mellow_green = '#89C997' \n",
    "plt.plot(reconstruction_error_real, label='Reconstruction Error', color=mellow_green)\n",
    "plt.axhline(y=threshold2, color='r', linestyle='--', label='Threshold')\n",
    "plt.title('Intrusion Detected by Reconstruction Error',fontsize=16, fontweight='bold')\n",
    "plt.xlabel('Sequence Number (×10³)', fontsize=16, fontweight='bold')\n",
    "#plt.xlabel('Sequence Number(*1000)', fontsize=16, fontweight='bold')\n",
    "plt.ylabel('Reconstruction Error', fontsize=16, fontweight='bold')\n",
    "for label in (plt.gca().get_xticklabels() + plt.gca().get_yticklabels()):\n",
    "    label.set_fontsize(12)\n",
    "    label.set_fontweight('bold')\n",
    "plt.legend(fontsize=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea42b73d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_index = np.random.choice(len(X_chunk_test))\n",
    "original_sample = X_chunk_test[sample_index]\n",
    "reconstructed_sample = X_chunk_pred[sample_index]\n",
    "\n",
    "plt.figure(figsize=(16, 8))\n",
    "plt.plot(original_sample[:, 0], 'b-', label='Original Real Part')\n",
    "plt.plot(reconstructed_sample[:, 0], 'b--', label='Reconstructed Real Part')\n",
    "# plt.plot(original_sample[:, 1], 'm-', label='Original Real STD')\n",
    "# plt.plot(reconstructed_sample[:, 1], 'm--', label='Reconstructed Real STD')\n",
    "# plt.plot(original_sample[:, 2], 'c-', label='Original Real Skew')\n",
    "# plt.plot(reconstructed_sample[:, 2], 'c--', label='Reconstructed Real Skew')\n",
    "# plt.plot(original_sample[:, 3], 'orange', label='Original Real Kurtosis')\n",
    "# plt.plot(reconstructed_sample[:, 3], 'orange', label='Reconstructed Real Kurtosis', linestyle='--')\n",
    "\n",
    "plt.plot(original_sample[:, 1], 'g-', label='Original Imaginary Part')\n",
    "plt.plot(reconstructed_sample[:, 1], 'g--', label='Reconstructed Imaginary Part')\n",
    "# plt.plot(original_sample[:, 5], 'purple', label='Original Imaginary STD')\n",
    "# plt.plot(reconstructed_sample[:, 5], 'purple', label='Reconstructed Imaginary STD', linestyle='--')\n",
    "# plt.plot(original_sample[:, 6], 'brown', label='Original Imaginary Skew')\n",
    "# plt.plot(reconstructed_sample[:, 6], 'brown', label='Reconstructed Imaginary Skew', linestyle='--')\n",
    "# plt.plot(original_sample[:, 7], 'pink', label='Original Imaginary Kurtosis')\n",
    "# plt.plot(reconstructed_sample[:, 7], 'pink', label='Reconstructed Imaginary Kurtosis', linestyle='--')\n",
    "plt.title('Original vs Reconstructed IQ Data')\n",
    "plt.xlabel('Time Steps')\n",
    "plt.ylabel('Amplitude')\n",
    "\n",
    "# Place the legend outside the plot area\n",
    "plt.legend(loc='upper left', bbox_to_anchor=(1, 1), fontsize='small', title='Legend')\n",
    "\n",
    "# Adjust layout to make room for the legend\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad266e00",
   "metadata": {},
   "outputs": [],
   "source": [
    "reconstruction_error_real_parts = reconstruction_error[:, 0]\n",
    "reconstruction_error_real_std = reconstruction_error[:, 1]\n",
    "reconstruction_error_real_skew = reconstruction_error[:, 2]\n",
    "reconstruction_error_real_kurtosis = reconstruction_error[:, 3]\n",
    "reconstruction_error_imag_parts = reconstruction_error[:, 4]\n",
    "reconstruction_error_imag_std = reconstruction_error[:, 5]\n",
    "reconstruction_error_imag_skew = reconstruction_error[:, 6]\n",
    "reconstruction_error_imag_kurtosis = reconstruction_error[:, 7]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bafc4676",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_index = np.random.choice(len(X_chunk_test))\n",
    "original_sample = X_chunk_test[sample_index]\n",
    "reconstructed_sample = X_chunk_pred[sample_index]\n",
    "\n",
    "plt.figure(figsize=(16, 8))\n",
    "# plt.plot(original_sample[:, 0], 'b-', label='Original Real Part')\n",
    "# plt.plot(reconstructed_sample[:, 0], 'b--', label='Reconstructed Real Part')\n",
    "# plt.plot(original_sample[:, 1], 'm-', label='Original Real STD')\n",
    "# plt.plot(reconstructed_sample[:, 1], 'm--', label='Reconstructed Real STD')\n",
    "plt.plot(original_sample[:, 2], 'c-', label='Original Real Skew')\n",
    "plt.plot(reconstructed_sample[:, 2], 'c--', label='Reconstructed Real Skew')\n",
    "plt.plot(original_sample[:, 3], 'orange', label='Original Real Kurtosis')\n",
    "plt.plot(reconstructed_sample[:, 3], 'orange', label='Reconstructed Real Kurtosis', linestyle='--')\n",
    "\n",
    "# plt.plot(original_sample[:, 4], 'g-', label='Original Imaginary Part')\n",
    "# plt.plot(reconstructed_sample[:, 4], 'g--', label='Reconstructed Imaginary Part')\n",
    "# plt.plot(original_sample[:, 5], 'purple', label='Original Imaginary STD')\n",
    "# plt.plot(reconstructed_sample[:, 5], 'purple', label='Reconstructed Imaginary STD', linestyle='--')\n",
    "plt.plot(original_sample[:, 6], 'brown', label='Original Imaginary Skew')\n",
    "plt.plot(reconstructed_sample[:, 6], 'brown', label='Reconstructed Imaginary Skew', linestyle='--')\n",
    "plt.plot(original_sample[:, 7], 'pink', label='Original Imaginary Kurtosis')\n",
    "plt.plot(reconstructed_sample[:, 7], 'pink', label='Reconstructed Imaginary Kurtosis', linestyle='--')\n",
    "plt.title('Original vs Reconstructed IQ Data')\n",
    "plt.xlabel('Time Steps')\n",
    "plt.ylabel('Amplitude')\n",
    "\n",
    "# Place the legend outside the plot area\n",
    "plt.legend(loc='upper left', bbox_to_anchor=(1, 1), fontsize='small', title='Legend')\n",
    "\n",
    "# Adjust layout to make room for the legend\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f704988c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define the number of sequences to plot together\n",
    "n = 2  # Change this to desired number of sequences\n",
    "sample_length = sequence_length * n\n",
    "\n",
    "# Select a random starting sequence for plotting\n",
    "sequence_index = np.random.choice(len(X_chunk_test) - n + 1)\n",
    "\n",
    "# Extract and concatenate the original and reconstructed samples\n",
    "original_sample = np.concatenate(X_chunk_test[sequence_index:sequence_index + n])\n",
    "reconstructed_sample = np.concatenate(X_chunk_pred[sequence_index:sequence_index + n])\n",
    "\n",
    "# Plot concatenated sequences\n",
    "plt.figure(figsize=(14, 6))\n",
    "plt.plot(original_sample[:, 0], 'b-', label='Original Real Part')\n",
    "plt.plot(reconstructed_sample[:, 0], 'r--', label='Reconstructed Real Part')\n",
    "plt.plot(original_sample[:, 1], 'orange', label='Original Real STD')\n",
    "plt.plot(reconstructed_sample[:, 1], 'orange', label='Reconstructed Real STD', linestyle='--')\n",
    "\n",
    "plt.plot(original_sample[:, 4], 'y-', label='Original Imaginary Part')\n",
    "plt.plot(reconstructed_sample[:, 4], 'g--', label='Reconstructed Imaginary Part')\n",
    "plt.plot(original_sample[:, 5], 'pink', label='Original Imaginary STD')\n",
    "plt.plot(reconstructed_sample[:, 5], 'pink', label='Reconstructed Imaginary STD', linestyle='--')\n",
    "plt.title(f'Original vs Reconstructed IQ Data for {n} Sequences of Length {sequence_length}')\n",
    "plt.xlabel('Time Steps')\n",
    "plt.ylabel('Amplitude')\n",
    "plt.legend()\n",
    "# plt.savefig('9-Original vs Reconstructed IQ Data for {n} Sequences of Length {sequence_length}.png')\n",
    "# plt.close()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef63d687",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Repeat for n = 9\n",
    "n = 4  # Change this to desired number of sequences\n",
    "sequence_index = np.random.choice(len(X_chunk_test) - n + 1)\n",
    "original_sample = np.concatenate(X_chunk_test[sequence_index:sequence_index + n])\n",
    "reconstructed_sample = np.concatenate(X_chunk_pred[sequence_index:sequence_index + n])\n",
    "\n",
    "plt.figure(figsize=(14, 6))\n",
    "plt.plot(original_sample[:, 2], 'b-', label='Original Real Part Skew')\n",
    "plt.plot(reconstructed_sample[:, 2], 'r--', label='Reconstructed Real Part Skew')\n",
    "plt.plot(original_sample[:, 6], 'g-', label='Original Imaginary Part Skew')\n",
    "plt.plot(reconstructed_sample[:, 6], 'y--', label='Reconstructed Imaginary Part Skew')\n",
    "plt.title(f'Original vs Reconstructed IQ Data for {n} Sequences of Length {sequence_length}')\n",
    "plt.xlabel('Time Steps')\n",
    "plt.ylabel('Amplitude')\n",
    "plt.legend()\n",
    "# plt.savefig('11-Original vs Reconstructed IQ Data for {n} Sequences of Length {sequence_length}.png')\n",
    "# plt.close()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0bf2a7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Repeat for n = 9\n",
    "n = 4  # Change this to desired number of sequences\n",
    "sequence_index = np.random.choice(len(X_chunk_test) - n + 1)\n",
    "original_sample = np.concatenate(X_chunk_test[sequence_index:sequence_index + n])\n",
    "reconstructed_sample = np.concatenate(X_chunk_pred[sequence_index:sequence_index + n])\n",
    "\n",
    "plt.figure(figsize=(14, 6))\n",
    "plt.plot(original_sample[:, 3], 'b-', label='Original Real Part Kurtosis')\n",
    "plt.plot(reconstructed_sample[:, 3], 'r--', label='Reconstructed Real Part Kurtosis')\n",
    "plt.plot(original_sample[:, 7], 'g-', label='Original Imaginary Part Kurtosis')\n",
    "plt.plot(reconstructed_sample[:, 7], 'y--', label='Reconstructed Imaginary Part Kurtosis')\n",
    "plt.title(f'Original vs Reconstructed IQ Data for {n} Sequences of Length {sequence_length}')\n",
    "plt.xlabel('Time Steps')\n",
    "plt.ylabel('Amplitude')\n",
    "plt.legend()\n",
    "# plt.savefig('11-Original vs Reconstructed IQ Data for {n} Sequences of Length {sequence_length}.png')\n",
    "# plt.close()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "671af7c6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
