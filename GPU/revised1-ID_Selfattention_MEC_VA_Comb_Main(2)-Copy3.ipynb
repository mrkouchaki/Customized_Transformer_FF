{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b3d3f1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-19 11:01:20.924240: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-04-19 11:01:20.957786: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 AVX512_FP16 AVX_VNNI AMX_TILE AMX_INT8 AMX_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-04-19 11:01:21.409024: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "2024-04-19 11:01:22.578913: W tensorflow/core/common_runtime/gpu/gpu_device.cc:2251] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File extension detected: \n",
      "File extension detected: \n",
      "x_batch.shape: (40, 10, 2)\n",
      "y_batch.shape: (40, 10, 2)\n",
      "epoch: 0\n",
      "data.shape in apply mec: (40, 25)\n",
      "WARNING:tensorflow:From /home/reza/.local/lib/python3.10/site-packages/tensorflow/python/util/deprecation.py:660: calling map_fn_v2 (from tensorflow.python.ops.map_fn) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use fn_output_signature instead\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-19 11:01:22.808757: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output of MEC transformation shape: (40, 25)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/reza/.local/lib/python3.10/site-packages/keras/src/optimizers/base_optimizer.py:576: UserWarning: Gradients do not exist for variables ['kernel', 'bias'] when minimizing the loss. If using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data.shape in apply mec: (40, 25)\n",
      "Output of MEC transformation shape: (40, 25)\n",
      "Epoch 0, Step 0/25000, Loss: 0.0007119542569853365\n",
      "Epoch 0, Step 100/25000, Loss: 0.00020401019719429314\n",
      "Epoch 0, Step 200/25000, Loss: 5.825431435368955e-05\n",
      "Epoch 0, Step 300/25000, Loss: 0.0019998319912701845\n",
      "Epoch 0, Step 400/25000, Loss: 3.515833668643609e-05\n",
      "Epoch 0, Step 500/25000, Loss: 1.5342040569521487e-05\n",
      "Epoch 0, Step 600/25000, Loss: 0.0003127401287201792\n",
      "Epoch 0, Step 700/25000, Loss: 2.7737789423554204e-05\n",
      "Epoch 0, Step 800/25000, Loss: 4.897564213024452e-05\n",
      "Epoch 0, Step 900/25000, Loss: 0.00030260675703175366\n",
      "Epoch 0, Step 1000/25000, Loss: 3.298062802059576e-05\n",
      "Epoch 0, Step 1100/25000, Loss: 0.0019979572389274836\n",
      "Epoch 0, Step 1200/25000, Loss: 0.00042998764547519386\n",
      "Epoch 0, Step 1300/25000, Loss: 3.477739301160909e-05\n",
      "Epoch 0, Step 1400/25000, Loss: 0.000322130013955757\n",
      "Epoch 0, Step 1500/25000, Loss: 4.19282732764259e-05\n",
      "Epoch 0, Step 1600/25000, Loss: 1.8573338820715435e-05\n",
      "Epoch 0, Step 1700/25000, Loss: 1.0308597666153219e-05\n",
      "Epoch 0, Step 1800/25000, Loss: 2.1269954231684096e-05\n",
      "Epoch 0, Step 1900/25000, Loss: 0.0010564890690147877\n",
      "Epoch 0, Step 2000/25000, Loss: 0.0004220821720082313\n",
      "Epoch 0, Step 2100/25000, Loss: 1.2769480235874653e-05\n",
      "Epoch 0, Step 2200/25000, Loss: 2.038754610111937e-05\n",
      "Epoch 0, Step 2300/25000, Loss: 0.00014357166946865618\n",
      "Epoch 0, Step 2400/25000, Loss: 4.910082134301774e-05\n",
      "Epoch 0, Step 2500/25000, Loss: 3.6874876968795434e-05\n",
      "Epoch 0, Step 2600/25000, Loss: 3.542661215760745e-05\n",
      "Epoch 0, Step 2700/25000, Loss: 0.0013557737693190575\n",
      "Epoch 0, Step 2800/25000, Loss: 1.9243929273216054e-05\n",
      "Epoch 0, Step 2900/25000, Loss: 0.0002060985571006313\n",
      "Epoch 0, Step 3000/25000, Loss: 3.302655022707768e-05\n",
      "Epoch 0, Step 3100/25000, Loss: 4.124731640331447e-05\n",
      "Epoch 0, Step 3200/25000, Loss: 0.00036778635694645345\n",
      "Epoch 0, Step 3300/25000, Loss: 1.2286157470953185e-05\n",
      "Epoch 0, Step 3400/25000, Loss: 0.00042498239781707525\n",
      "Epoch 0, Step 3500/25000, Loss: 0.001586525933817029\n",
      "Epoch 0, Step 3600/25000, Loss: 2.783401941996999e-05\n",
      "Epoch 0, Step 3700/25000, Loss: 1.631193299544975e-05\n",
      "Epoch 0, Step 3800/25000, Loss: 2.8165712137706578e-05\n",
      "Epoch 0, Step 3900/25000, Loss: 2.9490123779396527e-05\n",
      "Epoch 0, Step 4000/25000, Loss: 3.99623422708828e-05\n",
      "Epoch 0, Step 4100/25000, Loss: 0.0001778130535967648\n",
      "Epoch 0, Step 4200/25000, Loss: 3.572471905499697e-05\n",
      "Epoch 0, Step 4300/25000, Loss: 0.001568180974572897\n",
      "Epoch 0, Step 4400/25000, Loss: 1.7046208085957915e-05\n",
      "Epoch 0, Step 4500/25000, Loss: 1.9092725779046305e-05\n",
      "Epoch 0, Step 4600/25000, Loss: 0.00032213458325713873\n",
      "Epoch 0, Step 4700/25000, Loss: 4.5387594582280144e-05\n",
      "Epoch 0, Step 4800/25000, Loss: 2.2132573576527648e-05\n",
      "Epoch 0, Step 4900/25000, Loss: 1.0074745659949258e-05\n",
      "Epoch 0, Step 5000/25000, Loss: 3.070976526942104e-05\n",
      "Epoch 0, Step 5100/25000, Loss: 0.0016633719205856323\n",
      "Epoch 0, Step 5200/25000, Loss: 0.00040703461854718626\n",
      "Epoch 0, Step 5300/25000, Loss: 1.3645577382703777e-05\n",
      "Epoch 0, Step 5400/25000, Loss: 1.7607058907742612e-05\n",
      "Epoch 0, Step 5500/25000, Loss: 0.00010922594083240256\n",
      "Epoch 0, Step 5600/25000, Loss: 3.356712477398105e-05\n",
      "Epoch 0, Step 5700/25000, Loss: 3.5749879316426814e-05\n",
      "Epoch 0, Step 5800/25000, Loss: 0.00041403743671253324\n",
      "Epoch 0, Step 5900/25000, Loss: 0.0014852711465209723\n",
      "Epoch 0, Step 6000/25000, Loss: 1.4230364286049735e-05\n",
      "Epoch 0, Step 6100/25000, Loss: 1.678277476457879e-05\n",
      "Epoch 0, Step 6200/25000, Loss: 2.346767723793164e-05\n",
      "Epoch 0, Step 6300/25000, Loss: 7.587269647046924e-05\n",
      "Epoch 0, Step 6400/25000, Loss: 1.8584843928692862e-05\n",
      "Epoch 0, Step 6500/25000, Loss: 0.00025521102361381054\n",
      "Epoch 0, Step 6600/25000, Loss: 1.679316847003065e-05\n",
      "Epoch 0, Step 6700/25000, Loss: 0.0017553832149133086\n",
      "Epoch 0, Step 6800/25000, Loss: 0.0003812499635387212\n",
      "Epoch 0, Step 6900/25000, Loss: 1.708963100099936e-05\n",
      "Epoch 0, Step 7000/25000, Loss: 0.0003141525376122445\n",
      "Epoch 0, Step 7100/25000, Loss: 4.653786891140044e-05\n",
      "Epoch 0, Step 7200/25000, Loss: 4.665931191993877e-05\n",
      "Epoch 0, Step 7300/25000, Loss: 2.7759913791669533e-05\n",
      "Epoch 0, Step 7400/25000, Loss: 4.421298217494041e-05\n",
      "Epoch 0, Step 7500/25000, Loss: 0.0013373780529946089\n",
      "Epoch 0, Step 7600/25000, Loss: 0.0013703058939427137\n",
      "Epoch 0, Step 7700/25000, Loss: 0.0003759948303923011\n",
      "Epoch 0, Step 7800/25000, Loss: 2.8974776796530932e-05\n",
      "Epoch 0, Step 7900/25000, Loss: 4.5650409447262064e-05\n",
      "Epoch 0, Step 8000/25000, Loss: 0.0003442165907472372\n",
      "Epoch 0, Step 8100/25000, Loss: 1.8696528059081174e-05\n",
      "Epoch 0, Step 8200/25000, Loss: 1.4037325854587834e-05\n",
      "Epoch 0, Step 8300/25000, Loss: 0.001650452264584601\n",
      "Epoch 0, Step 8400/25000, Loss: 3.221315273549408e-05\n",
      "Epoch 0, Step 8500/25000, Loss: 1.530748158984352e-05\n",
      "Epoch 0, Step 8600/25000, Loss: 2.4676217435626313e-05\n",
      "Epoch 0, Step 8700/25000, Loss: 9.006394247990102e-05\n",
      "Epoch 0, Step 8800/25000, Loss: 4.4537307985592633e-05\n",
      "Epoch 0, Step 8900/25000, Loss: 2.8749334887834266e-05\n",
      "Epoch 0, Step 9000/25000, Loss: 0.00036652476410381496\n",
      "Epoch 0, Step 9100/25000, Loss: 0.001945288386195898\n",
      "Epoch 0, Step 9200/25000, Loss: 1.4969602489145473e-05\n",
      "Epoch 0, Step 9300/25000, Loss: 1.6745785615057684e-05\n",
      "Epoch 0, Step 9400/25000, Loss: 2.673806193342898e-05\n",
      "Epoch 0, Step 9500/25000, Loss: 4.1305680497316644e-05\n",
      "Epoch 0, Step 9600/25000, Loss: 1.690144017629791e-05\n",
      "Epoch 0, Step 9700/25000, Loss: 0.000225672876695171\n",
      "Epoch 0, Step 9800/25000, Loss: 1.238300228578737e-05\n",
      "Epoch 0, Step 9900/25000, Loss: 0.001177115016616881\n",
      "Epoch 0, Step 10000/25000, Loss: 0.0004087584966327995\n",
      "Epoch 0, Step 10100/25000, Loss: 1.8425553207634948e-05\n",
      "Epoch 0, Step 10200/25000, Loss: 0.0003166375681757927\n",
      "Epoch 0, Step 10300/25000, Loss: 2.19432040466927e-05\n",
      "Epoch 0, Step 10400/25000, Loss: 3.0485549359582365e-05\n",
      "Epoch 0, Step 10500/25000, Loss: 3.40525402862113e-05\n",
      "Epoch 0, Step 10600/25000, Loss: 2.6126990633201785e-05\n",
      "Epoch 0, Step 10700/25000, Loss: 0.0019692769274115562\n",
      "Epoch 0, Step 10800/25000, Loss: 1.4176875993143767e-05\n",
      "Epoch 0, Step 10900/25000, Loss: 0.00020900036906823516\n",
      "Epoch 0, Step 11000/25000, Loss: 2.1281999579514377e-05\n",
      "Epoch 0, Step 11100/25000, Loss: 0.00014595765969716012\n",
      "Epoch 0, Step 11200/25000, Loss: 1.895832974696532e-05\n",
      "Epoch 0, Step 11300/25000, Loss: 1.5314226402551867e-05\n",
      "Epoch 0, Step 11400/25000, Loss: 0.0004319573054090142\n",
      "Epoch 0, Step 11500/25000, Loss: 0.0014974145451560616\n",
      "Epoch 0, Step 11600/25000, Loss: 2.3188735212897882e-05\n",
      "Epoch 0, Step 11700/25000, Loss: 1.4733086572960019e-05\n",
      "Epoch 0, Step 11800/25000, Loss: 1.7869171642814763e-05\n",
      "Epoch 0, Step 11900/25000, Loss: 2.308765397174284e-05\n",
      "Epoch 0, Step 12000/25000, Loss: 2.1805888536619022e-05\n",
      "Epoch 0, Step 12100/25000, Loss: 0.0002495985827408731\n",
      "Epoch 0, Step 12200/25000, Loss: 3.096757791354321e-05\n",
      "Epoch 0, Step 12300/25000, Loss: 0.001885781530290842\n",
      "Epoch 0, Step 12400/25000, Loss: 2.703725294850301e-05\n",
      "Epoch 0, Step 12500/25000, Loss: 1.5978159353835508e-05\n",
      "Epoch 0, Step 12600/25000, Loss: 0.00029935871134512126\n",
      "Epoch 0, Step 12700/25000, Loss: 3.9295908209169284e-05\n",
      "Epoch 0, Step 12800/25000, Loss: 1.8016258763964288e-05\n",
      "Epoch 0, Step 12900/25000, Loss: 0.00022809643996879458\n",
      "Epoch 0, Step 13000/25000, Loss: 1.52343709487468e-05\n",
      "Epoch 0, Step 13100/25000, Loss: 0.0007745355251245201\n",
      "Epoch 0, Step 13200/25000, Loss: 0.0003171098360326141\n",
      "Epoch 0, Step 13300/25000, Loss: 1.9633211195468903e-05\n",
      "Epoch 0, Step 13400/25000, Loss: 0.00027443087310530245\n",
      "Epoch 0, Step 13500/25000, Loss: 2.030354517046362e-05\n",
      "Epoch 0, Step 13600/25000, Loss: 2.6001927835750394e-05\n",
      "Epoch 0, Step 13700/25000, Loss: 2.0183390006422997e-05\n",
      "Epoch 0, Step 13800/25000, Loss: 3.185059904353693e-05\n",
      "Epoch 0, Step 13900/25000, Loss: 0.0012038968270644546\n",
      "Epoch 0, Step 14000/25000, Loss: 1.4634021681558806e-05\n",
      "Epoch 0, Step 14100/25000, Loss: 0.00011425997945480049\n",
      "Epoch 0, Step 14200/25000, Loss: 1.3809228221361991e-05\n",
      "Epoch 0, Step 14300/25000, Loss: 0.00013638818927574903\n",
      "Epoch 0, Step 14400/25000, Loss: 2.0940720787621103e-05\n",
      "Epoch 0, Step 14500/25000, Loss: 1.5154468201217242e-05\n",
      "Epoch 0, Step 14600/25000, Loss: 2.412829599052202e-05\n",
      "Epoch 0, Step 14700/25000, Loss: 0.0010067499242722988\n",
      "Epoch 0, Step 14800/25000, Loss: 2.377619966864586e-05\n",
      "Epoch 0, Step 14900/25000, Loss: 7.98881592345424e-05\n",
      "Epoch 0, Step 15000/25000, Loss: 4.421012636157684e-05\n",
      "Epoch 0, Step 15100/25000, Loss: 1.821439764171373e-05\n",
      "Epoch 0, Step 15200/25000, Loss: 0.0003465165209490806\n",
      "Epoch 0, Step 15300/25000, Loss: 2.8862910767202266e-05\n",
      "Epoch 0, Step 15400/25000, Loss: 0.0003700179513543844\n",
      "Epoch 0, Step 15500/25000, Loss: 0.001598169095814228\n",
      "Epoch 0, Step 15600/25000, Loss: 1.850549415394198e-05\n",
      "Epoch 0, Step 15700/25000, Loss: 1.0018507055065129e-05\n",
      "Epoch 0, Step 15800/25000, Loss: 1.9655097275972366e-05\n",
      "Epoch 0, Step 15900/25000, Loss: 4.520265429164283e-05\n",
      "Epoch 0, Step 16000/25000, Loss: 0.0003323689161334187\n",
      "Epoch 0, Step 16100/25000, Loss: 2.0998124455218203e-05\n",
      "Epoch 0, Step 16200/25000, Loss: 1.4045047464605886e-05\n",
      "Epoch 0, Step 16300/25000, Loss: 0.001690208213403821\n",
      "Epoch 0, Step 16400/25000, Loss: 2.180256342398934e-05\n",
      "Epoch 0, Step 16500/25000, Loss: 2.7948794013354927e-05\n",
      "Epoch 0, Step 16600/25000, Loss: 3.5695389669854194e-05\n",
      "Epoch 0, Step 16700/25000, Loss: 1.9849885575240478e-05\n",
      "Epoch 0, Step 16800/25000, Loss: 2.1877354811294936e-05\n",
      "Epoch 0, Step 16900/25000, Loss: 0.0002386030973866582\n",
      "Epoch 0, Step 17000/25000, Loss: 2.1661679056705907e-05\n",
      "Epoch 0, Step 17100/25000, Loss: 0.0018819286487996578\n",
      "Epoch 0, Step 17200/25000, Loss: 2.1096340788062662e-05\n",
      "Epoch 0, Step 17300/25000, Loss: 1.7703721823636442e-05\n",
      "Epoch 0, Step 17400/25000, Loss: 2.2730640921508893e-05\n",
      "Epoch 0, Step 17500/25000, Loss: 4.098546560271643e-05\n",
      "Epoch 0, Step 17600/25000, Loss: 0.0002945117012131959\n",
      "Epoch 0, Step 17700/25000, Loss: 1.7183423551614396e-05\n",
      "Epoch 0, Step 17800/25000, Loss: 2.419516931695398e-05\n",
      "Epoch 0, Step 17900/25000, Loss: 0.001403499860316515\n",
      "Epoch 0, Step 18000/25000, Loss: 5.606241393252276e-05\n",
      "Epoch 0, Step 18100/25000, Loss: 2.2789303329773247e-05\n",
      "Epoch 0, Step 18200/25000, Loss: 2.6423726012581028e-05\n",
      "Epoch 0, Step 18300/25000, Loss: 2.1811465558130294e-05\n",
      "Epoch 0, Step 18400/25000, Loss: 0.0002742807555478066\n",
      "Epoch 0, Step 18500/25000, Loss: 2.8841006496804766e-05\n",
      "Epoch 0, Step 18600/25000, Loss: 0.0003753656637854874\n",
      "Epoch 0, Step 18700/25000, Loss: 0.0018417533719912171\n",
      "Epoch 0, Step 18800/25000, Loss: 2.384871731919702e-05\n",
      "Epoch 0, Step 18900/25000, Loss: 2.370992660871707e-05\n",
      "Epoch 0, Step 19000/25000, Loss: 2.288803443661891e-05\n",
      "Epoch 0, Step 19100/25000, Loss: 3.672293314593844e-05\n",
      "Epoch 0, Step 19200/25000, Loss: 0.00037485605571419\n",
      "Epoch 0, Step 19300/25000, Loss: 1.722155684547033e-05\n",
      "Epoch 0, Step 19400/25000, Loss: 0.0004385424545034766\n",
      "Epoch 0, Step 19500/25000, Loss: 0.0010574772022664547\n",
      "Epoch 0, Step 19600/25000, Loss: 2.6501571483095177e-05\n",
      "Epoch 0, Step 19700/25000, Loss: 0.00017682391626294702\n",
      "Epoch 0, Step 19800/25000, Loss: 2.1723097233916633e-05\n",
      "Epoch 0, Step 19900/25000, Loss: 1.7026099158101715e-05\n",
      "Epoch 0, Step 20000/25000, Loss: 0.0002902875712607056\n",
      "Epoch 0, Step 20100/25000, Loss: 3.5322245821589604e-05\n",
      "Epoch 0, Step 20200/25000, Loss: 0.000374316587112844\n",
      "Epoch 0, Step 20300/25000, Loss: 0.0014988129260018468\n",
      "Epoch 0, Step 20400/25000, Loss: 0.0014346516691148281\n",
      "Epoch 0, Step 20500/25000, Loss: 4.4320913730189204e-05\n",
      "Epoch 0, Step 20600/25000, Loss: 2.0526866137515754e-05\n",
      "Epoch 0, Step 20700/25000, Loss: 3.466535417828709e-05\n",
      "Epoch 0, Step 20800/25000, Loss: 0.00032999602262862027\n",
      "Epoch 0, Step 20900/25000, Loss: 1.9384951883694157e-05\n",
      "Epoch 0, Step 21000/25000, Loss: 2.3378041078103706e-05\n",
      "Epoch 0, Step 21100/25000, Loss: 0.0017389124259352684\n",
      "Epoch 0, Step 21200/25000, Loss: 7.100295624695718e-05\n",
      "Epoch 0, Step 21300/25000, Loss: 4.162323602940887e-05\n",
      "Epoch 0, Step 21400/25000, Loss: 0.0003068521909881383\n",
      "Epoch 0, Step 21500/25000, Loss: 2.782105002552271e-05\n",
      "Epoch 0, Step 21600/25000, Loss: 2.5253541025449522e-05\n",
      "Epoch 0, Step 21700/25000, Loss: 2.478676469763741e-05\n",
      "Epoch 0, Step 21800/25000, Loss: 3.5375818697502837e-05\n",
      "Epoch 0, Step 21900/25000, Loss: 0.0013074815506115556\n",
      "Epoch 0, Step 22000/25000, Loss: 0.0003751487238332629\n",
      "Epoch 0, Step 22100/25000, Loss: 3.8317601138260216e-05\n",
      "Epoch 0, Step 22200/25000, Loss: 0.0002935016236733645\n",
      "Epoch 0, Step 22300/25000, Loss: 5.7370009017176926e-05\n",
      "Epoch 0, Step 22400/25000, Loss: 3.2199470297200605e-05\n",
      "Epoch 0, Step 22500/25000, Loss: 1.7481268514529802e-05\n",
      "Epoch 0, Step 22600/25000, Loss: 2.391218367847614e-05\n",
      "Epoch 0, Step 22700/25000, Loss: 0.0018727014539763331\n",
      "Epoch 0, Step 22800/25000, Loss: 0.00045411489554680884\n",
      "Epoch 0, Step 22900/25000, Loss: 2.127847619703971e-05\n",
      "Epoch 0, Step 23000/25000, Loss: 2.9310576792340726e-05\n",
      "Epoch 0, Step 23100/25000, Loss: 0.00011980464478256181\n",
      "Epoch 0, Step 23200/25000, Loss: 3.510855822241865e-05\n",
      "Epoch 0, Step 23300/25000, Loss: 2.4592032787040807e-05\n",
      "Epoch 0, Step 23400/25000, Loss: 2.9946926588308997e-05\n",
      "Epoch 0, Step 23500/25000, Loss: 0.0018279093783348799\n",
      "Epoch 0, Step 23600/25000, Loss: 1.600802715984173e-05\n",
      "Epoch 0, Step 23700/25000, Loss: 0.00012757739750668406\n",
      "Epoch 0, Step 23800/25000, Loss: 1.9224569768994115e-05\n",
      "Epoch 0, Step 23900/25000, Loss: 3.3464883017586544e-05\n",
      "Epoch 0, Step 24000/25000, Loss: 0.0004265546740498394\n",
      "Epoch 0, Step 24100/25000, Loss: 1.9210472601116635e-05\n",
      "Epoch 0, Step 24200/25000, Loss: 0.0004248968616593629\n",
      "Epoch 0, Step 24300/25000, Loss: 0.0012142170453444123\n",
      "Epoch 0, Step 24400/25000, Loss: 2.592680903035216e-05\n",
      "Epoch 0, Step 24500/25000, Loss: 1.4708726666867733e-05\n",
      "Epoch 0, Step 24600/25000, Loss: 2.7607669835560955e-05\n",
      "Epoch 0, Step 24700/25000, Loss: 2.581723492767196e-05\n",
      "Epoch 0, Step 24800/25000, Loss: 0.0003784419677685946\n",
      "Epoch 0, Step 24900/25000, Loss: 3.6859801184618846e-05\n",
      "epoch: 1\n",
      "Epoch 1, Step 0/25000, Loss: 2.134954593202565e-05\n",
      "Epoch 1, Step 100/25000, Loss: 0.000590818643104285\n",
      "Epoch 1, Step 200/25000, Loss: 2.1859097614651546e-05\n",
      "Epoch 1, Step 300/25000, Loss: 1.8719985746429302e-05\n",
      "Epoch 1, Step 400/25000, Loss: 0.00023161472927313298\n",
      "Epoch 1, Step 500/25000, Loss: 3.715750062838197e-05\n",
      "Epoch 1, Step 600/25000, Loss: 3.2052237656898797e-05\n",
      "Epoch 1, Step 700/25000, Loss: 1.8838129108189605e-05\n",
      "Epoch 1, Step 800/25000, Loss: 3.1342653528554365e-05\n",
      "Epoch 1, Step 900/25000, Loss: 0.0002777903573587537\n",
      "Epoch 1, Step 1000/25000, Loss: 2.4308743377332576e-05\n",
      "Epoch 1, Step 1100/25000, Loss: 0.00015177790191955864\n",
      "Epoch 1, Step 1200/25000, Loss: 2.7172278350917622e-05\n",
      "Epoch 1, Step 1300/25000, Loss: 0.00025289913173764944\n",
      "Epoch 1, Step 1400/25000, Loss: 2.5042441848199815e-05\n",
      "Epoch 1, Step 1500/25000, Loss: 2.412373032711912e-05\n",
      "Epoch 1, Step 1600/25000, Loss: 8.389895083382726e-05\n",
      "Epoch 1, Step 1700/25000, Loss: 0.0005981369176879525\n",
      "Epoch 1, Step 1800/25000, Loss: 2.4579498131060973e-05\n",
      "Epoch 1, Step 1900/25000, Loss: 0.00018295693735126406\n",
      "Epoch 1, Step 2000/25000, Loss: 2.601811138447374e-05\n",
      "Epoch 1, Step 2100/25000, Loss: 2.4477787519572303e-05\n",
      "Epoch 1, Step 2200/25000, Loss: 0.00031492524431087077\n",
      "Epoch 1, Step 2300/25000, Loss: 2.3532256818725727e-05\n",
      "Epoch 1, Step 2400/25000, Loss: 3.118969107163139e-05\n",
      "Epoch 1, Step 2500/25000, Loss: 0.0005651518586091697\n",
      "Epoch 1, Step 2600/25000, Loss: 1.3274967386678327e-05\n",
      "Epoch 1, Step 2700/25000, Loss: 9.292112372349948e-05\n",
      "Epoch 1, Step 2800/25000, Loss: 0.00026825990062206984\n",
      "Epoch 1, Step 2900/25000, Loss: 3.0608800443587825e-05\n",
      "Epoch 1, Step 3000/25000, Loss: 1.9404906197451055e-05\n",
      "Epoch 1, Step 3100/25000, Loss: 1.80437091330532e-05\n",
      "Epoch 1, Step 3200/25000, Loss: 2.672020491445437e-05\n",
      "Epoch 1, Step 3300/25000, Loss: 0.00012251008593011647\n",
      "Epoch 1, Step 3400/25000, Loss: 2.2839776647742838e-05\n",
      "Epoch 1, Step 3500/25000, Loss: 0.00020437740022316575\n",
      "Epoch 1, Step 3600/25000, Loss: 2.5126490072580054e-05\n",
      "Epoch 1, Step 3700/25000, Loss: 0.00026739350869320333\n",
      "Epoch 1, Step 3800/25000, Loss: 4.544711555354297e-05\n",
      "Epoch 1, Step 3900/25000, Loss: 2.024918831011746e-05\n",
      "Epoch 1, Step 4000/25000, Loss: 9.248152491636574e-05\n",
      "Epoch 1, Step 4100/25000, Loss: 0.00040063649066723883\n",
      "Epoch 1, Step 4200/25000, Loss: 2.171777123294305e-05\n",
      "Epoch 1, Step 4300/25000, Loss: 2.180826777475886e-05\n",
      "Epoch 1, Step 4400/25000, Loss: 0.0003080948954448104\n",
      "Epoch 1, Step 4500/25000, Loss: 3.641691000666469e-05\n",
      "Epoch 1, Step 4600/25000, Loss: 2.4584640414104797e-05\n",
      "Epoch 1, Step 4700/25000, Loss: 0.00012753844202961773\n",
      "Epoch 1, Step 4800/25000, Loss: 2.055223740171641e-05\n",
      "Epoch 1, Step 4900/25000, Loss: 0.00012180225894553587\n",
      "Epoch 1, Step 5000/25000, Loss: 0.0002797029446810484\n",
      "Epoch 1, Step 5100/25000, Loss: 1.798887569748331e-05\n",
      "Epoch 1, Step 5200/25000, Loss: 2.0413066522451118e-05\n",
      "Epoch 1, Step 5300/25000, Loss: 0.00018341898976359516\n",
      "Epoch 1, Step 5400/25000, Loss: 4.176800212007947e-05\n",
      "Epoch 1, Step 5500/25000, Loss: 3.5577264497987926e-05\n",
      "Epoch 1, Step 5600/25000, Loss: 8.738352335058153e-05\n",
      "Epoch 1, Step 5700/25000, Loss: 0.000973415095359087\n",
      "Epoch 1, Step 5800/25000, Loss: 2.7615682483883575e-05\n",
      "Epoch 1, Step 5900/25000, Loss: 2.2683194401906803e-05\n",
      "Epoch 1, Step 6000/25000, Loss: 2.723402030824218e-05\n",
      "Epoch 1, Step 6100/25000, Loss: 3.986187221016735e-05\n",
      "Epoch 1, Step 6200/25000, Loss: 1.849136060627643e-05\n",
      "Epoch 1, Step 6300/25000, Loss: 0.0002566998591646552\n",
      "Epoch 1, Step 6400/25000, Loss: 2.095453965011984e-05\n",
      "Epoch 1, Step 6500/25000, Loss: 0.0003698969376273453\n",
      "Epoch 1, Step 6600/25000, Loss: 2.665649117261637e-05\n",
      "Epoch 1, Step 6700/25000, Loss: 2.29315828619292e-05\n",
      "Epoch 1, Step 6800/25000, Loss: 0.00024914302048273385\n",
      "Epoch 1, Step 6900/25000, Loss: 3.712802572408691e-05\n",
      "Epoch 1, Step 7000/25000, Loss: 3.47356908605434e-05\n",
      "Epoch 1, Step 7100/25000, Loss: 2.443429548293352e-05\n",
      "Epoch 1, Step 7200/25000, Loss: 3.3837124647106975e-05\n",
      "Epoch 1, Step 7300/25000, Loss: 0.0003608620900195092\n",
      "Epoch 1, Step 7400/25000, Loss: 0.0002832397585734725\n",
      "Epoch 1, Step 7500/25000, Loss: 3.1266848964150995e-05\n",
      "Epoch 1, Step 7600/25000, Loss: 2.0920650058542378e-05\n",
      "Epoch 1, Step 7700/25000, Loss: 0.0002012147888308391\n",
      "Epoch 1, Step 7800/25000, Loss: 1.8708245988818817e-05\n",
      "Epoch 1, Step 7900/25000, Loss: 1.696230174275115e-05\n",
      "Epoch 1, Step 8000/25000, Loss: 9.622581274015829e-05\n",
      "Epoch 1, Step 8100/25000, Loss: 0.0002292749413754791\n",
      "Epoch 1, Step 8200/25000, Loss: 0.0017563658766448498\n",
      "Epoch 1, Step 8300/25000, Loss: 0.00016556688933633268\n",
      "Epoch 1, Step 8400/25000, Loss: 1.8906363038695417e-05\n",
      "Epoch 1, Step 8500/25000, Loss: 2.0057264919159934e-05\n",
      "Epoch 1, Step 8600/25000, Loss: 0.00022302991419564933\n",
      "Epoch 1, Step 8700/25000, Loss: 2.5975657990784384e-05\n",
      "Epoch 1, Step 8800/25000, Loss: 2.4371123799937777e-05\n",
      "Epoch 1, Step 8900/25000, Loss: 0.000928589841350913\n",
      "Epoch 1, Step 9000/25000, Loss: 4.808527955901809e-05\n",
      "Epoch 1, Step 9100/25000, Loss: 6.460358417825773e-05\n",
      "Epoch 1, Step 9200/25000, Loss: 2.9114582503098063e-05\n",
      "Epoch 1, Step 9300/25000, Loss: 0.0002134423266397789\n",
      "Epoch 1, Step 9400/25000, Loss: 2.4014558221097104e-05\n",
      "Epoch 1, Step 9500/25000, Loss: 2.2742757209925912e-05\n",
      "Epoch 1, Step 9600/25000, Loss: 2.5171326342388056e-05\n",
      "Epoch 1, Step 9700/25000, Loss: 0.0005303525249473751\n",
      "Epoch 1, Step 9800/25000, Loss: 5.005545972380787e-05\n",
      "Epoch 1, Step 9900/25000, Loss: 0.00014453048061113805\n",
      "Epoch 1, Step 10000/25000, Loss: 3.396653482923284e-05\n",
      "Epoch 1, Step 10100/25000, Loss: 2.4649240003782324e-05\n",
      "Epoch 1, Step 10200/25000, Loss: 0.00020042687538079917\n",
      "Epoch 1, Step 10300/25000, Loss: 3.249322617193684e-05\n",
      "Epoch 1, Step 10400/25000, Loss: 2.8820424631703645e-05\n",
      "Epoch 1, Step 10500/25000, Loss: 0.00042640831088647246\n",
      "Epoch 1, Step 10600/25000, Loss: 3.227406705264002e-05\n",
      "Epoch 1, Step 10700/25000, Loss: 3.533803464961238e-05\n",
      "Epoch 1, Step 10800/25000, Loss: 3.5728699003811926e-05\n",
      "Epoch 1, Step 10900/25000, Loss: 4.151926259510219e-05\n",
      "Epoch 1, Step 11000/25000, Loss: 2.13026341953082e-05\n",
      "Epoch 1, Step 11100/25000, Loss: 0.00014693978300783783\n",
      "Epoch 1, Step 11200/25000, Loss: 2.4714570827200077e-05\n",
      "Epoch 1, Step 11300/25000, Loss: 0.00038587822928093374\n",
      "Epoch 1, Step 11400/25000, Loss: 3.6069694033358246e-05\n",
      "Epoch 1, Step 11500/25000, Loss: 1.78010650415672e-05\n",
      "Epoch 1, Step 11600/25000, Loss: 0.00024098802532535046\n",
      "Epoch 1, Step 11700/25000, Loss: 3.1333634979091585e-05\n",
      "Epoch 1, Step 11800/25000, Loss: 2.9459322831826285e-05\n",
      "Epoch 1, Step 11900/25000, Loss: 0.00016465428052470088\n",
      "Epoch 1, Step 12000/25000, Loss: 2.599822801130358e-05\n",
      "Epoch 1, Step 12100/25000, Loss: 0.0003189182607457042\n",
      "Epoch 1, Step 12200/25000, Loss: 0.0002486633602529764\n",
      "Epoch 1, Step 12300/25000, Loss: 3.918794755009003e-05\n",
      "Epoch 1, Step 12400/25000, Loss: 0.00025345789617858827\n",
      "Epoch 1, Step 12500/25000, Loss: 4.483965312829241e-05\n",
      "Epoch 1, Step 12600/25000, Loss: 2.358803067181725e-05\n",
      "Epoch 1, Step 12700/25000, Loss: 1.983210677281022e-05\n",
      "Epoch 1, Step 12800/25000, Loss: 2.5786277547013015e-05\n",
      "Epoch 1, Step 12900/25000, Loss: 0.00015050213551148772\n",
      "Epoch 1, Step 13000/25000, Loss: 0.000237196683883667\n",
      "Epoch 1, Step 13100/25000, Loss: 1.7362062862957828e-05\n",
      "Epoch 1, Step 13200/25000, Loss: 0.00023798765323590487\n",
      "Epoch 1, Step 13300/25000, Loss: 3.0022776627447456e-05\n",
      "Epoch 1, Step 13400/25000, Loss: 2.165801197406836e-05\n",
      "Epoch 1, Step 13500/25000, Loss: 2.7474816306494176e-05\n",
      "Epoch 1, Step 13600/25000, Loss: 3.417069456190802e-05\n",
      "Epoch 1, Step 13700/25000, Loss: 0.00030389547464437783\n",
      "Epoch 1, Step 13800/25000, Loss: 0.00017238342843484133\n",
      "Epoch 1, Step 13900/25000, Loss: 2.98347877105698e-05\n",
      "Epoch 1, Step 14000/25000, Loss: 0.00024256088363472372\n",
      "Epoch 1, Step 14100/25000, Loss: 3.942208786611445e-05\n",
      "Epoch 1, Step 14200/25000, Loss: 1.900165807455778e-05\n",
      "Epoch 1, Step 14300/25000, Loss: 1.81865780177759e-05\n",
      "Epoch 1, Step 14400/25000, Loss: 3.223128442186862e-05\n",
      "Epoch 1, Step 14500/25000, Loss: 0.0001391376426909119\n",
      "Epoch 1, Step 14600/25000, Loss: 0.00022782151063438505\n",
      "Epoch 1, Step 14700/25000, Loss: 1.7693813788355328e-05\n",
      "Epoch 1, Step 14800/25000, Loss: 0.00024379439128097147\n",
      "Epoch 1, Step 14900/25000, Loss: 3.385929448995739e-05\n",
      "Epoch 1, Step 15000/25000, Loss: 3.5787041269941255e-05\n",
      "Epoch 1, Step 15100/25000, Loss: 2.5427949367440306e-05\n",
      "Epoch 1, Step 15200/25000, Loss: 4.022793291369453e-05\n",
      "Epoch 1, Step 15300/25000, Loss: 0.0006656157202087343\n",
      "Epoch 1, Step 15400/25000, Loss: 0.0001295466354349628\n",
      "Epoch 1, Step 15500/25000, Loss: 6.0139143897686154e-05\n",
      "Epoch 1, Step 15600/25000, Loss: 0.00025323376758024096\n",
      "Epoch 1, Step 15700/25000, Loss: 5.38432395842392e-05\n",
      "Epoch 1, Step 15800/25000, Loss: 2.2431435354519635e-05\n",
      "Epoch 1, Step 15900/25000, Loss: 0.00028453688719309866\n",
      "Epoch 1, Step 16000/25000, Loss: 2.6165040253545158e-05\n",
      "Epoch 1, Step 16100/25000, Loss: 0.00034668162697926164\n",
      "Epoch 1, Step 16200/25000, Loss: 5.91680764046032e-05\n",
      "Epoch 1, Step 16300/25000, Loss: 1.643402902118396e-05\n",
      "Epoch 1, Step 16400/25000, Loss: 0.00024283284437842667\n",
      "Epoch 1, Step 16500/25000, Loss: 3.463507164269686e-05\n",
      "Epoch 1, Step 16600/25000, Loss: 2.7068779672845267e-05\n",
      "Epoch 1, Step 16700/25000, Loss: 0.0002689858665689826\n",
      "Epoch 1, Step 16800/25000, Loss: 2.5488123355899006e-05\n",
      "Epoch 1, Step 16900/25000, Loss: 0.0004501278745010495\n",
      "Epoch 1, Step 17000/25000, Loss: 4.2109968489967287e-05\n",
      "Epoch 1, Step 17100/25000, Loss: 3.101086258538999e-05\n",
      "Epoch 1, Step 17200/25000, Loss: 0.00025598524371162057\n",
      "Epoch 1, Step 17300/25000, Loss: 4.8875892389332876e-05\n",
      "Epoch 1, Step 17400/25000, Loss: 2.104508712363895e-05\n",
      "Epoch 1, Step 17500/25000, Loss: 0.0002966538886539638\n",
      "Epoch 1, Step 17600/25000, Loss: 2.4649118131492287e-05\n",
      "Epoch 1, Step 17700/25000, Loss: 0.0006457719719037414\n",
      "Epoch 1, Step 17800/25000, Loss: 0.0001317789574386552\n",
      "Epoch 1, Step 17900/25000, Loss: 1.6614005289738998e-05\n",
      "Epoch 1, Step 18000/25000, Loss: 0.000222888367716223\n",
      "Epoch 1, Step 18100/25000, Loss: 3.3621119655435905e-05\n",
      "Epoch 1, Step 18200/25000, Loss: 2.993522139149718e-05\n",
      "Epoch 1, Step 18300/25000, Loss: 0.00023625392350368202\n",
      "Epoch 1, Step 18400/25000, Loss: 2.9209657441242598e-05\n",
      "Epoch 1, Step 18500/25000, Loss: 0.0005914951325394213\n",
      "Epoch 1, Step 18600/25000, Loss: 3.8114041672088206e-05\n",
      "Epoch 1, Step 18700/25000, Loss: 3.89741180697456e-05\n",
      "Epoch 1, Step 18800/25000, Loss: 2.4215169105445966e-05\n",
      "Epoch 1, Step 18900/25000, Loss: 4.025311136501841e-05\n",
      "Epoch 1, Step 19000/25000, Loss: 2.4602890334790573e-05\n",
      "Epoch 1, Step 19100/25000, Loss: 0.00021873552759643644\n",
      "Epoch 1, Step 19200/25000, Loss: 2.8014997951686382e-05\n",
      "Epoch 1, Step 19300/25000, Loss: 0.00045672047417610884\n",
      "Epoch 1, Step 19400/25000, Loss: 5.273949500406161e-05\n",
      "Epoch 1, Step 19500/25000, Loss: 1.7224661860382184e-05\n",
      "Epoch 1, Step 19600/25000, Loss: 0.0002311360149178654\n",
      "Epoch 1, Step 19700/25000, Loss: 4.374342461233027e-05\n",
      "Epoch 1, Step 19800/25000, Loss: 2.951705209852662e-05\n",
      "Epoch 1, Step 19900/25000, Loss: 0.0005106150638312101\n",
      "Epoch 1, Step 20000/25000, Loss: 2.710692206164822e-05\n",
      "Epoch 1, Step 20100/25000, Loss: 0.0003807916946243495\n",
      "Epoch 1, Step 20200/25000, Loss: 3.3319152862532064e-05\n",
      "Epoch 1, Step 20300/25000, Loss: 2.8495900551206432e-05\n",
      "Epoch 1, Step 20400/25000, Loss: 0.00026544497814029455\n",
      "Epoch 1, Step 20500/25000, Loss: 5.900210817344487e-05\n",
      "Epoch 1, Step 20600/25000, Loss: 2.547584881540388e-05\n",
      "Epoch 1, Step 20700/25000, Loss: 0.0002803051902446896\n",
      "Epoch 1, Step 20800/25000, Loss: 2.886905349441804e-05\n",
      "Epoch 1, Step 20900/25000, Loss: 0.00026517847436480224\n",
      "Epoch 1, Step 21000/25000, Loss: 0.001745916437357664\n",
      "Epoch 1, Step 21100/25000, Loss: 0.00017851432494353503\n",
      "Epoch 1, Step 21200/25000, Loss: 0.00021386411390267313\n",
      "Epoch 1, Step 21300/25000, Loss: 3.982406633440405e-05\n",
      "Epoch 1, Step 21400/25000, Loss: 2.972330003103707e-05\n",
      "Epoch 1, Step 21500/25000, Loss: 2.4994544219225645e-05\n",
      "Epoch 1, Step 21600/25000, Loss: 3.581944838515483e-05\n",
      "Epoch 1, Step 21700/25000, Loss: 0.000298032391583547\n",
      "Epoch 1, Step 21800/25000, Loss: 3.1877614674158394e-05\n",
      "Epoch 1, Step 21900/25000, Loss: 4.850298137171194e-05\n",
      "Epoch 1, Step 22000/25000, Loss: 3.171147909597494e-05\n",
      "Epoch 1, Step 22100/25000, Loss: 4.047411493957043e-05\n",
      "Epoch 1, Step 22200/25000, Loss: 0.00024137528089340776\n",
      "Epoch 1, Step 22300/25000, Loss: 1.9875273210345767e-05\n",
      "Epoch 1, Step 22400/25000, Loss: 2.5878558517433703e-05\n",
      "Epoch 1, Step 22500/25000, Loss: 0.0004697609692811966\n",
      "Epoch 1, Step 22600/25000, Loss: 4.352262112661265e-05\n",
      "Epoch 1, Step 22700/25000, Loss: 1.990001146623399e-05\n",
      "Epoch 1, Step 22800/25000, Loss: 0.00024057150585576892\n",
      "Epoch 1, Step 22900/25000, Loss: 3.070788807235658e-05\n",
      "Epoch 1, Step 23000/25000, Loss: 2.4438375476165675e-05\n",
      "Epoch 1, Step 23100/25000, Loss: 0.00024307015701197088\n",
      "Epoch 1, Step 23200/25000, Loss: 2.0460891391849145e-05\n",
      "Epoch 1, Step 23300/25000, Loss: 0.0002070902264676988\n",
      "Epoch 1, Step 23400/25000, Loss: 0.00019014342979062349\n",
      "Epoch 1, Step 23500/25000, Loss: 3.653183739515953e-05\n",
      "Epoch 1, Step 23600/25000, Loss: 0.0002767519326880574\n",
      "Epoch 1, Step 23700/25000, Loss: 4.585550050251186e-05\n",
      "Epoch 1, Step 23800/25000, Loss: 2.7300280635245144e-05\n",
      "Epoch 1, Step 23900/25000, Loss: 1.8492633898858912e-05\n",
      "Epoch 1, Step 24000/25000, Loss: 3.200910214218311e-05\n",
      "Epoch 1, Step 24100/25000, Loss: 0.00013674602087121457\n",
      "Epoch 1, Step 24200/25000, Loss: 1.4530159205605742e-05\n",
      "Epoch 1, Step 24300/25000, Loss: 0.00016164459520950913\n",
      "Epoch 1, Step 24400/25000, Loss: 2.7614380087470636e-05\n",
      "Epoch 1, Step 24500/25000, Loss: 2.1306559574441053e-05\n",
      "Epoch 1, Step 24600/25000, Loss: 0.00021429445769172162\n",
      "Epoch 1, Step 24700/25000, Loss: 2.8294605726841837e-05\n",
      "Epoch 1, Step 24800/25000, Loss: 0.00015672256995458156\n",
      "Epoch 1, Step 24900/25000, Loss: 0.0006147152744233608\n",
      "epoch: 2\n",
      "Epoch 2, Step 0/25000, Loss: 2.4590328393969685e-05\n",
      "Epoch 2, Step 100/25000, Loss: 2.998876698256936e-05\n",
      "Epoch 2, Step 200/25000, Loss: 3.410552380955778e-05\n",
      "Epoch 2, Step 300/25000, Loss: 3.805503001785837e-05\n",
      "Epoch 2, Step 400/25000, Loss: 4.2524767195573077e-05\n",
      "Epoch 2, Step 500/25000, Loss: 0.0001205536536872387\n",
      "Epoch 2, Step 600/25000, Loss: 2.7804439014289528e-05\n",
      "Epoch 2, Step 700/25000, Loss: 0.0004957748460583389\n",
      "Epoch 2, Step 800/25000, Loss: 0.00020438485080376267\n",
      "Epoch 2, Step 900/25000, Loss: 3.604513767641038e-05\n",
      "Epoch 2, Step 1000/25000, Loss: 0.00019930330745410174\n",
      "Epoch 2, Step 1100/25000, Loss: 2.7062771550845355e-05\n",
      "Epoch 2, Step 1200/25000, Loss: 2.7055637474404648e-05\n",
      "Epoch 2, Step 1300/25000, Loss: 2.4659120754222386e-05\n",
      "Epoch 2, Step 1400/25000, Loss: 3.2729189115343615e-05\n",
      "Epoch 2, Step 1500/25000, Loss: 0.00018569148960523307\n",
      "Epoch 2, Step 1600/25000, Loss: 2.1218178517301567e-05\n",
      "Epoch 2, Step 1700/25000, Loss: 0.00019098534539807588\n",
      "Epoch 2, Step 1800/25000, Loss: 2.7238475013291463e-05\n",
      "Epoch 2, Step 1900/25000, Loss: 0.00016271859931293875\n",
      "Epoch 2, Step 2000/25000, Loss: 5.571930523728952e-05\n",
      "Epoch 2, Step 2100/25000, Loss: 1.8950771845993586e-05\n",
      "Epoch 2, Step 2200/25000, Loss: 0.00028362986631691456\n",
      "Epoch 2, Step 2300/25000, Loss: 0.00026528979651629925\n",
      "Epoch 2, Step 2400/25000, Loss: 3.416905747144483e-05\n",
      "Epoch 2, Step 2500/25000, Loss: 1.951726881088689e-05\n",
      "Epoch 2, Step 2600/25000, Loss: 2.5975867174565792e-05\n",
      "Epoch 2, Step 2700/25000, Loss: 2.229023266409058e-05\n",
      "Epoch 2, Step 2800/25000, Loss: 2.3837028493289836e-05\n",
      "Epoch 2, Step 2900/25000, Loss: 9.355002839583904e-05\n",
      "Epoch 2, Step 3000/25000, Loss: 3.165998714393936e-05\n",
      "Epoch 2, Step 3100/25000, Loss: 0.00015735358465462923\n",
      "Epoch 2, Step 3200/25000, Loss: 0.00024419609690085053\n",
      "Epoch 2, Step 3300/25000, Loss: 5.2297731599537656e-05\n",
      "Epoch 2, Step 3400/25000, Loss: 5.8019038988277316e-05\n",
      "Epoch 2, Step 3500/25000, Loss: 0.00010711682989494875\n",
      "Epoch 2, Step 3600/25000, Loss: 4.6274206397356465e-05\n",
      "Epoch 2, Step 3700/25000, Loss: 1.774477459548507e-05\n",
      "Epoch 2, Step 3800/25000, Loss: 3.1830040825298056e-05\n",
      "Epoch 2, Step 3900/25000, Loss: 0.0002344677341170609\n",
      "Epoch 2, Step 4000/25000, Loss: 1.0857498637051322e-05\n",
      "Epoch 2, Step 4100/25000, Loss: 0.00011379963689250872\n",
      "Epoch 2, Step 4200/25000, Loss: 3.335967267048545e-05\n",
      "Epoch 2, Step 4300/25000, Loss: 2.958048389700707e-05\n",
      "Epoch 2, Step 4400/25000, Loss: 0.00028260311228223145\n",
      "Epoch 2, Step 4500/25000, Loss: 2.9907072530477308e-05\n",
      "Epoch 2, Step 4600/25000, Loss: 2.9923163310741074e-05\n",
      "Epoch 2, Step 4700/25000, Loss: 0.00044508624705486\n",
      "Epoch 2, Step 4800/25000, Loss: 2.025142930506263e-05\n",
      "Epoch 2, Step 4900/25000, Loss: 1.6801302990643308e-05\n",
      "Epoch 2, Step 5000/25000, Loss: 0.00020557247626129538\n",
      "Epoch 2, Step 5100/25000, Loss: 5.3215117077343166e-05\n",
      "Epoch 2, Step 5200/25000, Loss: 3.548469612724148e-05\n",
      "Epoch 2, Step 5300/25000, Loss: 1.8732531316345558e-05\n",
      "Epoch 2, Step 5400/25000, Loss: 3.831633512163535e-05\n",
      "Epoch 2, Step 5500/25000, Loss: 0.00018130124954041094\n",
      "Epoch 2, Step 5600/25000, Loss: 2.5376799385412596e-05\n",
      "Epoch 2, Step 5700/25000, Loss: 0.0002526185126043856\n",
      "Epoch 2, Step 5800/25000, Loss: 2.5379362341482192e-05\n",
      "Epoch 2, Step 5900/25000, Loss: 3.1106545065995306e-05\n",
      "Epoch 2, Step 6000/25000, Loss: 0.00014782437938265502\n",
      "Epoch 2, Step 6100/25000, Loss: 2.8947108148713596e-05\n",
      "Epoch 2, Step 6200/25000, Loss: 2.2995600374997593e-05\n",
      "Epoch 2, Step 6300/25000, Loss: 0.00047578648081980646\n",
      "Epoch 2, Step 6400/25000, Loss: 2.7410727852839045e-05\n",
      "Epoch 2, Step 6500/25000, Loss: 2.1478352209669538e-05\n",
      "Epoch 2, Step 6600/25000, Loss: 3.824137820629403e-05\n",
      "Epoch 2, Step 6700/25000, Loss: 2.99505700240843e-05\n",
      "Epoch 2, Step 6800/25000, Loss: 3.330111940158531e-05\n",
      "Epoch 2, Step 6900/25000, Loss: 0.00020490691531449556\n",
      "Epoch 2, Step 7000/25000, Loss: 2.9341128538362682e-05\n",
      "Epoch 2, Step 7100/25000, Loss: 0.0002852057805284858\n",
      "Epoch 2, Step 7200/25000, Loss: 0.00020755536388605833\n",
      "Epoch 2, Step 7300/25000, Loss: 2.7720307116396725e-05\n",
      "Epoch 2, Step 7400/25000, Loss: 0.0002002062974497676\n",
      "Epoch 2, Step 7500/25000, Loss: 2.823321847245097e-05\n",
      "Epoch 2, Step 7600/25000, Loss: 2.72880479315063e-05\n",
      "Epoch 2, Step 7700/25000, Loss: 0.0001003812431008555\n",
      "Epoch 2, Step 7800/25000, Loss: 2.536676765885204e-05\n",
      "Epoch 2, Step 7900/25000, Loss: 0.00016219109238591045\n",
      "Epoch 2, Step 8000/25000, Loss: 0.00028794509125873446\n",
      "Epoch 2, Step 8100/25000, Loss: 2.8653688787017018e-05\n",
      "Epoch 2, Step 8200/25000, Loss: 2.372572271269746e-05\n",
      "Epoch 2, Step 8300/25000, Loss: 0.00020803209918085486\n",
      "Epoch 2, Step 8400/25000, Loss: 5.046963633503765e-05\n",
      "Epoch 2, Step 8500/25000, Loss: 1.963573959073983e-05\n",
      "Epoch 2, Step 8600/25000, Loss: 3.250816007493995e-05\n",
      "Epoch 2, Step 8700/25000, Loss: 0.00018007095786742866\n",
      "Epoch 2, Step 8800/25000, Loss: 0.0019393813563510776\n",
      "Epoch 2, Step 8900/25000, Loss: 0.0001246146421181038\n",
      "Epoch 2, Step 9000/25000, Loss: 2.6044990590889938e-05\n",
      "Epoch 2, Step 9100/25000, Loss: 0.00019806131604127586\n",
      "Epoch 2, Step 9200/25000, Loss: 2.771812978608068e-05\n",
      "Epoch 2, Step 9300/25000, Loss: 3.0411392799578607e-05\n",
      "Epoch 2, Step 9400/25000, Loss: 0.0003258186625316739\n",
      "Epoch 2, Step 9500/25000, Loss: 0.00035662282607518137\n",
      "Epoch 2, Step 9600/25000, Loss: 1.5524408809142187e-05\n",
      "Epoch 2, Step 9700/25000, Loss: 2.9001448638155125e-05\n",
      "Epoch 2, Step 9800/25000, Loss: 7.052651926642284e-05\n",
      "Epoch 2, Step 9900/25000, Loss: 3.566504892660305e-05\n",
      "Epoch 2, Step 10000/25000, Loss: 3.3694173907861114e-05\n",
      "Epoch 2, Step 10100/25000, Loss: 0.0001606588630238548\n",
      "Epoch 2, Step 10200/25000, Loss: 3.086127253482118e-05\n",
      "Epoch 2, Step 10300/25000, Loss: 0.00018606035155244172\n",
      "Epoch 2, Step 10400/25000, Loss: 0.00023781528580002487\n",
      "Epoch 2, Step 10500/25000, Loss: 3.936611028620973e-05\n",
      "Epoch 2, Step 10600/25000, Loss: 0.00020803831284865737\n",
      "Epoch 2, Step 10700/25000, Loss: 4.441493729245849e-05\n",
      "Epoch 2, Step 10800/25000, Loss: 2.5974293748731725e-05\n",
      "Epoch 2, Step 10900/25000, Loss: 2.343157393625006e-05\n",
      "Epoch 2, Step 11000/25000, Loss: 4.235024607623927e-05\n",
      "Epoch 2, Step 11100/25000, Loss: 0.0002956331882160157\n",
      "Epoch 2, Step 11200/25000, Loss: 0.00017951044719666243\n",
      "Epoch 2, Step 11300/25000, Loss: 2.441619108140003e-05\n",
      "Epoch 2, Step 11400/25000, Loss: 0.00018766701396089047\n",
      "Epoch 2, Step 11500/25000, Loss: 4.8099307605298236e-05\n",
      "Epoch 2, Step 11600/25000, Loss: 3.74047776858788e-05\n",
      "Epoch 2, Step 11700/25000, Loss: 2.1622987333103083e-05\n",
      "Epoch 2, Step 11800/25000, Loss: 3.822434155154042e-05\n",
      "Epoch 2, Step 11900/25000, Loss: 0.0001821846526581794\n",
      "Epoch 2, Step 12000/25000, Loss: 0.0002161426964448765\n",
      "Epoch 2, Step 12100/25000, Loss: 3.542686317814514e-05\n",
      "Epoch 2, Step 12200/25000, Loss: 0.00021837140957359225\n",
      "Epoch 2, Step 12300/25000, Loss: 4.858521788264625e-05\n",
      "Epoch 2, Step 12400/25000, Loss: 2.3328289898927324e-05\n",
      "Epoch 2, Step 12500/25000, Loss: 0.0002708228712435812\n",
      "Epoch 2, Step 12600/25000, Loss: 3.577405368559994e-05\n",
      "Epoch 2, Step 12700/25000, Loss: 0.00012461836740840226\n",
      "Epoch 2, Step 12800/25000, Loss: 0.00022671531769447029\n",
      "Epoch 2, Step 12900/25000, Loss: 2.4085458790068515e-05\n",
      "Epoch 2, Step 13000/25000, Loss: 0.00019114158931188285\n",
      "Epoch 2, Step 13100/25000, Loss: 5.7340927014593035e-05\n",
      "Epoch 2, Step 13200/25000, Loss: 2.6964904463966377e-05\n",
      "Epoch 2, Step 13300/25000, Loss: 0.0002650416281539947\n",
      "Epoch 2, Step 13400/25000, Loss: 2.437367402308155e-05\n",
      "Epoch 2, Step 13500/25000, Loss: 0.0005009464803151786\n",
      "Epoch 2, Step 13600/25000, Loss: 3.848724372801371e-05\n",
      "Epoch 2, Step 13700/25000, Loss: 2.592622331576422e-05\n",
      "Epoch 2, Step 13800/25000, Loss: 3.0968294595368207e-05\n",
      "Epoch 2, Step 13900/25000, Loss: 2.9503382393158972e-05\n",
      "Epoch 2, Step 14000/25000, Loss: 2.2837370124761946e-05\n",
      "Epoch 2, Step 14100/25000, Loss: 0.00014368613483384252\n",
      "Epoch 2, Step 14200/25000, Loss: 3.648692654678598e-05\n",
      "Epoch 2, Step 14300/25000, Loss: 0.0003618955670390278\n",
      "Epoch 2, Step 14400/25000, Loss: 5.910807885811664e-05\n",
      "Epoch 2, Step 14500/25000, Loss: 2.1749972802354023e-05\n",
      "Epoch 2, Step 14600/25000, Loss: 3.29536160279531e-05\n",
      "Epoch 2, Step 14700/25000, Loss: 4.6454249968519434e-05\n",
      "Epoch 2, Step 14800/25000, Loss: 0.00020898778166156262\n",
      "Epoch 2, Step 14900/25000, Loss: 4.272468504495919e-05\n",
      "Epoch 2, Step 15000/25000, Loss: 2.7293232051306404e-05\n",
      "Epoch 2, Step 15100/25000, Loss: 0.000514101586304605\n",
      "Epoch 2, Step 15200/25000, Loss: 5.368765414459631e-05\n",
      "Epoch 2, Step 15300/25000, Loss: 4.056615580338985e-05\n",
      "Epoch 2, Step 15400/25000, Loss: 3.847630796371959e-05\n",
      "Epoch 2, Step 15500/25000, Loss: 3.645686592790298e-05\n",
      "Epoch 2, Step 15600/25000, Loss: 0.000240104622207582\n",
      "Epoch 2, Step 15700/25000, Loss: 2.5853731131064706e-05\n",
      "Epoch 2, Step 15800/25000, Loss: 2.9085587812005542e-05\n",
      "Epoch 2, Step 15900/25000, Loss: 0.0004521460796240717\n",
      "Epoch 2, Step 16000/25000, Loss: 4.51081468781922e-05\n",
      "Epoch 2, Step 16100/25000, Loss: 2.7469219276099466e-05\n",
      "Epoch 2, Step 16200/25000, Loss: 6.813962681917474e-05\n",
      "Epoch 2, Step 16300/25000, Loss: 5.425032213679515e-05\n",
      "Epoch 2, Step 16400/25000, Loss: 3.6004184948978946e-05\n",
      "Epoch 2, Step 16500/25000, Loss: 8.499439718434587e-05\n",
      "Epoch 2, Step 16600/25000, Loss: 3.531072070472874e-05\n",
      "Epoch 2, Step 16700/25000, Loss: 0.000529348268173635\n",
      "Epoch 2, Step 16800/25000, Loss: 5.913211134611629e-05\n",
      "Epoch 2, Step 16900/25000, Loss: 0.00023030031297821552\n",
      "Epoch 2, Step 17000/25000, Loss: 3.9706526877125725e-05\n",
      "Epoch 2, Step 17100/25000, Loss: 3.1893636332824826e-05\n",
      "Epoch 2, Step 17200/25000, Loss: 0.0002273219870403409\n",
      "Epoch 2, Step 17300/25000, Loss: 2.9996243029017933e-05\n",
      "Epoch 2, Step 17400/25000, Loss: 3.353539068484679e-05\n",
      "Epoch 2, Step 17500/25000, Loss: 0.0002531578647904098\n",
      "Epoch 2, Step 17600/25000, Loss: 5.823593164677732e-05\n",
      "Epoch 2, Step 17700/25000, Loss: 1.9021937987417914e-05\n",
      "Epoch 2, Step 17800/25000, Loss: 4.276281106285751e-05\n",
      "Epoch 2, Step 17900/25000, Loss: 3.74246301362291e-05\n",
      "Epoch 2, Step 18000/25000, Loss: 2.110047898895573e-05\n",
      "Epoch 2, Step 18100/25000, Loss: 0.0001658877299632877\n",
      "Epoch 2, Step 18200/25000, Loss: 2.6226914997096173e-05\n",
      "Epoch 2, Step 18300/25000, Loss: 0.0005266882362775505\n",
      "Epoch 2, Step 18400/25000, Loss: 5.772297299699858e-05\n",
      "Epoch 2, Step 18500/25000, Loss: 3.025137266376987e-05\n",
      "Epoch 2, Step 18600/25000, Loss: 0.0001941509690368548\n",
      "Epoch 2, Step 18700/25000, Loss: 6.858581764390692e-05\n",
      "Epoch 2, Step 18800/25000, Loss: 2.405481609457638e-05\n",
      "Epoch 2, Step 18900/25000, Loss: 0.00018395950610283762\n",
      "Epoch 2, Step 19000/25000, Loss: 3.711529279826209e-05\n",
      "Epoch 2, Step 19100/25000, Loss: 0.0001729604264255613\n",
      "Epoch 2, Step 19200/25000, Loss: 0.00019175240595359355\n",
      "Epoch 2, Step 19300/25000, Loss: 2.1302159439073876e-05\n",
      "Epoch 2, Step 19400/25000, Loss: 0.00020711195247713476\n",
      "Epoch 2, Step 19500/25000, Loss: 3.7113473808858544e-05\n",
      "Epoch 2, Step 19600/25000, Loss: 3.691907477332279e-05\n",
      "Epoch 2, Step 19700/25000, Loss: 2.1704390746890567e-05\n",
      "Epoch 2, Step 19800/25000, Loss: 3.172942888340913e-05\n",
      "Epoch 2, Step 19900/25000, Loss: 0.00021752509928774089\n",
      "Epoch 2, Step 20000/25000, Loss: 0.00019729671475943178\n",
      "Epoch 2, Step 20100/25000, Loss: 3.682515671243891e-05\n",
      "Epoch 2, Step 20200/25000, Loss: 2.47019106609514e-05\n",
      "Epoch 2, Step 20300/25000, Loss: 0.0001560434466227889\n",
      "Epoch 2, Step 20400/25000, Loss: 2.8409514925442636e-05\n",
      "Epoch 2, Step 20500/25000, Loss: 1.5662300938856788e-05\n",
      "Epoch 2, Step 20600/25000, Loss: 3.774590732064098e-05\n",
      "Epoch 2, Step 20700/25000, Loss: 0.00019971522851847112\n",
      "Epoch 2, Step 20800/25000, Loss: 3.378796100150794e-05\n",
      "Epoch 2, Step 20900/25000, Loss: 5.844064071425237e-05\n",
      "Epoch 2, Step 21000/25000, Loss: 3.89538035960868e-05\n",
      "Epoch 2, Step 21100/25000, Loss: 0.00016066931129898876\n",
      "Epoch 2, Step 21200/25000, Loss: 4.434178481460549e-05\n",
      "Epoch 2, Step 21300/25000, Loss: 2.228884659416508e-05\n",
      "Epoch 2, Step 21400/25000, Loss: 0.0002544126764405519\n",
      "Epoch 2, Step 21500/25000, Loss: 0.0003048082871828228\n",
      "Epoch 2, Step 21600/25000, Loss: 0.0019843769259750843\n",
      "Epoch 2, Step 21700/25000, Loss: 0.0001540436496725306\n",
      "Epoch 2, Step 21800/25000, Loss: 2.3363907530438155e-05\n",
      "Epoch 2, Step 21900/25000, Loss: 3.8615376979578286e-05\n",
      "Epoch 2, Step 22000/25000, Loss: 0.00013352355745155364\n",
      "Epoch 2, Step 22100/25000, Loss: 2.0221377781126648e-05\n",
      "Epoch 2, Step 22200/25000, Loss: 3.418820051592775e-05\n",
      "Epoch 2, Step 22300/25000, Loss: 0.00039984151953831315\n",
      "Epoch 2, Step 22400/25000, Loss: 8.431273454334587e-05\n",
      "Epoch 2, Step 22500/25000, Loss: 4.4282645831117406e-05\n",
      "Epoch 2, Step 22600/25000, Loss: 4.740549411508255e-05\n",
      "Epoch 2, Step 22700/25000, Loss: 0.00013913685688748956\n",
      "Epoch 2, Step 22800/25000, Loss: 5.211649840930477e-05\n",
      "Epoch 2, Step 22900/25000, Loss: 2.2821370293968357e-05\n",
      "Epoch 2, Step 23000/25000, Loss: 0.0002698772877920419\n",
      "Epoch 2, Step 23100/25000, Loss: 0.0003113061538897455\n",
      "Epoch 2, Step 23200/25000, Loss: 2.5317136532976292e-05\n",
      "Epoch 2, Step 23300/25000, Loss: 2.49532768066274e-05\n",
      "Epoch 2, Step 23400/25000, Loss: 3.130690674879588e-05\n",
      "Epoch 2, Step 23500/25000, Loss: 3.1501218472840264e-05\n",
      "Epoch 2, Step 23600/25000, Loss: 2.3758813767926767e-05\n",
      "Epoch 2, Step 23700/25000, Loss: 0.00020589691121131182\n",
      "Epoch 2, Step 23800/25000, Loss: 5.311016138875857e-05\n",
      "Epoch 2, Step 23900/25000, Loss: 0.0006041548913344741\n",
      "Epoch 2, Step 24000/25000, Loss: 7.304130849661306e-05\n",
      "Epoch 2, Step 24100/25000, Loss: 2.5240149625460617e-05\n",
      "Epoch 2, Step 24200/25000, Loss: 0.00020685610070358962\n",
      "Epoch 2, Step 24300/25000, Loss: 3.254420153098181e-05\n",
      "Epoch 2, Step 24400/25000, Loss: 2.9834749511792324e-05\n",
      "Epoch 2, Step 24500/25000, Loss: 1.9875085854437202e-05\n",
      "Epoch 2, Step 24600/25000, Loss: 3.758784805540927e-05\n",
      "Epoch 2, Step 24700/25000, Loss: 0.00030210622935555875\n",
      "Epoch 2, Step 24800/25000, Loss: 2.6736386644188315e-05\n",
      "Epoch 2, Step 24900/25000, Loss: 0.00016975517792161554\n",
      "epoch: 3\n",
      "Epoch 3, Step 0/25000, Loss: 3.906693382305093e-05\n",
      "Epoch 3, Step 100/25000, Loss: 9.351367771159858e-05\n",
      "Epoch 3, Step 200/25000, Loss: 5.9392918046796694e-05\n",
      "Epoch 3, Step 300/25000, Loss: 2.3482269170926884e-05\n",
      "Epoch 3, Step 400/25000, Loss: 0.00028245753492228687\n",
      "Epoch 3, Step 500/25000, Loss: 0.00024907916667871177\n",
      "Epoch 3, Step 600/25000, Loss: 3.321121403132565e-05\n",
      "Epoch 3, Step 700/25000, Loss: 2.4072403903119266e-05\n",
      "Epoch 3, Step 800/25000, Loss: 3.417717744014226e-05\n",
      "Epoch 3, Step 900/25000, Loss: 3.4877284633694217e-05\n",
      "Epoch 3, Step 1000/25000, Loss: 2.8845324777648784e-05\n",
      "Epoch 3, Step 1100/25000, Loss: 0.0002123735030181706\n",
      "Epoch 3, Step 1200/25000, Loss: 3.3167867513839155e-05\n",
      "Epoch 3, Step 1300/25000, Loss: 0.00020894003682769835\n",
      "Epoch 3, Step 1400/25000, Loss: 0.00036626256769523025\n",
      "Epoch 3, Step 1500/25000, Loss: 3.1550178391626105e-05\n",
      "Epoch 3, Step 1600/25000, Loss: 2.7441277779871598e-05\n",
      "Epoch 3, Step 1700/25000, Loss: 0.00021452030341606587\n",
      "Epoch 3, Step 1800/25000, Loss: 6.810177001170814e-05\n",
      "Epoch 3, Step 1900/25000, Loss: 2.2783129679737613e-05\n",
      "Epoch 3, Step 2000/25000, Loss: 0.0003168320981785655\n",
      "Epoch 3, Step 2100/25000, Loss: 0.00025654741330072284\n",
      "Epoch 3, Step 2200/25000, Loss: 4.584959606290795e-05\n",
      "Epoch 3, Step 2300/25000, Loss: 2.856514220184181e-05\n",
      "Epoch 3, Step 2400/25000, Loss: 2.9992335839779116e-05\n",
      "Epoch 3, Step 2500/25000, Loss: 3.295957139926031e-05\n",
      "Epoch 3, Step 2600/25000, Loss: 0.00035819041659124196\n",
      "Epoch 3, Step 2700/25000, Loss: 3.212295996490866e-05\n",
      "Epoch 3, Step 2800/25000, Loss: 2.6949621314997785e-05\n",
      "Epoch 3, Step 2900/25000, Loss: 0.0006132457638159394\n",
      "Epoch 3, Step 3000/25000, Loss: 4.005136725027114e-05\n",
      "Epoch 3, Step 3100/25000, Loss: 2.030956238741055e-05\n",
      "Epoch 3, Step 3200/25000, Loss: 0.00036789095611311495\n",
      "Epoch 3, Step 3300/25000, Loss: 4.938004713039845e-05\n",
      "Epoch 3, Step 3400/25000, Loss: 4.026361784781329e-05\n",
      "Epoch 3, Step 3500/25000, Loss: 2.4037237380980514e-05\n",
      "Epoch 3, Step 3600/25000, Loss: 3.145074151689187e-05\n",
      "Epoch 3, Step 3700/25000, Loss: 0.0001870238920673728\n",
      "Epoch 3, Step 3800/25000, Loss: 4.9106172809842974e-05\n",
      "Epoch 3, Step 3900/25000, Loss: 0.0001361891918350011\n",
      "Epoch 3, Step 4000/25000, Loss: 5.209229129832238e-05\n",
      "Epoch 3, Step 4100/25000, Loss: 0.0002272991114296019\n",
      "Epoch 3, Step 4200/25000, Loss: 2.618717735458631e-05\n",
      "Epoch 3, Step 4300/25000, Loss: 2.853698970284313e-05\n",
      "Epoch 3, Step 4400/25000, Loss: 0.00022301603166852146\n",
      "Epoch 3, Step 4500/25000, Loss: 0.000149871819303371\n",
      "Epoch 3, Step 4600/25000, Loss: 1.4335181731439661e-05\n",
      "Epoch 3, Step 4700/25000, Loss: 2.294773730682209e-05\n",
      "Epoch 3, Step 4800/25000, Loss: 3.352025669300929e-05\n",
      "Epoch 3, Step 4900/25000, Loss: 3.6388999433256686e-05\n",
      "Epoch 3, Step 5000/25000, Loss: 0.0003946978540625423\n",
      "Epoch 3, Step 5100/25000, Loss: 2.9372489734669216e-05\n",
      "Epoch 3, Step 5200/25000, Loss: 2.922496605606284e-05\n",
      "Epoch 3, Step 5300/25000, Loss: 0.0006371728959493339\n",
      "Epoch 3, Step 5400/25000, Loss: 6.466476042987779e-05\n",
      "Epoch 3, Step 5500/25000, Loss: 4.1410978155909106e-05\n",
      "Epoch 3, Step 5600/25000, Loss: 3.5089502489427105e-05\n",
      "Epoch 3, Step 5700/25000, Loss: 4.21197728428524e-05\n",
      "Epoch 3, Step 5800/25000, Loss: 2.2591973902308382e-05\n",
      "Epoch 3, Step 5900/25000, Loss: 0.0001385577634209767\n",
      "Epoch 3, Step 6000/25000, Loss: 4.3611602450255305e-05\n",
      "Epoch 3, Step 6100/25000, Loss: 0.0001768174988683313\n",
      "Epoch 3, Step 6200/25000, Loss: 0.00043547202949412167\n",
      "Epoch 3, Step 6300/25000, Loss: 2.3055221390677616e-05\n",
      "Epoch 3, Step 6400/25000, Loss: 0.00031267607118934393\n",
      "Epoch 3, Step 6500/25000, Loss: 6.719634984619915e-05\n",
      "Epoch 3, Step 6600/25000, Loss: 4.008898395113647e-05\n",
      "Epoch 3, Step 6700/25000, Loss: 0.00020936742657795548\n",
      "Epoch 3, Step 6800/25000, Loss: 3.3899945265147835e-05\n",
      "Epoch 3, Step 6900/25000, Loss: 0.00019411284301895648\n",
      "Epoch 3, Step 7000/25000, Loss: 0.00036014147917740047\n",
      "Epoch 3, Step 7100/25000, Loss: 4.6456014388240874e-05\n",
      "Epoch 3, Step 7200/25000, Loss: 0.0002751370193436742\n",
      "Epoch 3, Step 7300/25000, Loss: 5.576749026658945e-05\n",
      "Epoch 3, Step 7400/25000, Loss: 2.223145929747261e-05\n",
      "Epoch 3, Step 7500/25000, Loss: 2.208326077379752e-05\n",
      "Epoch 3, Step 7600/25000, Loss: 3.708486838149838e-05\n",
      "Epoch 3, Step 7700/25000, Loss: 0.00014078397362027317\n",
      "Epoch 3, Step 7800/25000, Loss: 0.0003034950641449541\n",
      "Epoch 3, Step 7900/25000, Loss: 0.0002543930313549936\n",
      "Epoch 3, Step 8000/25000, Loss: 0.0002498345565982163\n",
      "Epoch 3, Step 8100/25000, Loss: 5.119744673720561e-05\n",
      "Epoch 3, Step 8200/25000, Loss: 4.0363385778618976e-05\n",
      "Epoch 3, Step 8300/25000, Loss: 2.4159695385606028e-05\n",
      "Epoch 3, Step 8400/25000, Loss: 4.371413524495438e-05\n",
      "Epoch 3, Step 8500/25000, Loss: 0.0003827692708000541\n",
      "Epoch 3, Step 8600/25000, Loss: 0.0004401528858579695\n",
      "Epoch 3, Step 8700/25000, Loss: 4.152348265051842e-05\n",
      "Epoch 3, Step 8800/25000, Loss: 2.7091873562312685e-05\n",
      "Epoch 3, Step 8900/25000, Loss: 0.0001246598840225488\n",
      "Epoch 3, Step 9000/25000, Loss: 2.9733710107393563e-05\n",
      "Epoch 3, Step 9100/25000, Loss: 2.8508207833510824e-05\n",
      "Epoch 3, Step 9200/25000, Loss: 3.260663652326912e-05\n",
      "Epoch 3, Step 9300/25000, Loss: 0.0001320922892773524\n",
      "Epoch 3, Step 9400/25000, Loss: 0.0017685428028926253\n",
      "Epoch 3, Step 9500/25000, Loss: 0.00018131120305042714\n",
      "Epoch 3, Step 9600/25000, Loss: 3.954392741434276e-05\n",
      "Epoch 3, Step 9700/25000, Loss: 0.00025493919383734465\n",
      "Epoch 3, Step 9800/25000, Loss: 4.7278004785766825e-05\n",
      "Epoch 3, Step 9900/25000, Loss: 2.791652877931483e-05\n",
      "Epoch 3, Step 10000/25000, Loss: 3.5760622267844155e-05\n",
      "Epoch 3, Step 10100/25000, Loss: 0.00022901655756868422\n",
      "Epoch 3, Step 10200/25000, Loss: 3.744539208128117e-05\n",
      "Epoch 3, Step 10300/25000, Loss: 0.0003259666555095464\n",
      "Epoch 3, Step 10400/25000, Loss: 4.9133130232803524e-05\n",
      "Epoch 3, Step 10500/25000, Loss: 0.00010770681547001004\n",
      "Epoch 3, Step 10600/25000, Loss: 2.9896227715653367e-05\n",
      "Epoch 3, Step 10700/25000, Loss: 3.2009673304855824e-05\n",
      "Epoch 3, Step 10800/25000, Loss: 4.7512512537650764e-05\n",
      "Epoch 3, Step 10900/25000, Loss: 0.00015525970957241952\n",
      "Epoch 3, Step 11000/25000, Loss: 2.436759132251609e-05\n",
      "Epoch 3, Step 11100/25000, Loss: 8.771717693889514e-05\n",
      "Epoch 3, Step 11200/25000, Loss: 3.045992889383342e-05\n",
      "Epoch 3, Step 11300/25000, Loss: 3.447381459409371e-05\n",
      "Epoch 3, Step 11400/25000, Loss: 0.0004281720903236419\n",
      "Epoch 3, Step 11500/25000, Loss: 3.97745861846488e-05\n",
      "Epoch 3, Step 11600/25000, Loss: 0.00030021602287888527\n",
      "Epoch 3, Step 11700/25000, Loss: 0.00042881054105237126\n",
      "Epoch 3, Step 11800/25000, Loss: 5.895276626688428e-05\n",
      "Epoch 3, Step 11900/25000, Loss: 0.00015541620086878538\n",
      "Epoch 3, Step 12000/25000, Loss: 2.952341856143903e-05\n",
      "Epoch 3, Step 12100/25000, Loss: 0.00022720228298567235\n",
      "Epoch 3, Step 12200/25000, Loss: 2.9492995963664725e-05\n",
      "Epoch 3, Step 12300/25000, Loss: 2.531393511162605e-05\n",
      "Epoch 3, Step 12400/25000, Loss: 4.07105399062857e-05\n",
      "Epoch 3, Step 12500/25000, Loss: 0.00013316770491655916\n",
      "Epoch 3, Step 12600/25000, Loss: 2.4354470951948315e-05\n",
      "Epoch 3, Step 12700/25000, Loss: 0.00021503899188246578\n",
      "Epoch 3, Step 12800/25000, Loss: 3.445390029810369e-05\n",
      "Epoch 3, Step 12900/25000, Loss: 0.00027226138627156615\n",
      "Epoch 3, Step 13000/25000, Loss: 6.975373980822042e-05\n",
      "Epoch 3, Step 13100/25000, Loss: 2.6629239073372446e-05\n",
      "Epoch 3, Step 13200/25000, Loss: 4.3879594159079716e-05\n",
      "Epoch 3, Step 13300/25000, Loss: 0.00025218952214345336\n",
      "Epoch 3, Step 13400/25000, Loss: 2.2221987819648348e-05\n",
      "Epoch 3, Step 13500/25000, Loss: 0.00014117737009655684\n",
      "Epoch 3, Step 13600/25000, Loss: 2.104432314808946e-05\n",
      "Epoch 3, Step 13700/25000, Loss: 3.452947930782102e-05\n",
      "Epoch 3, Step 13800/25000, Loss: 0.0003164412919431925\n",
      "Epoch 3, Step 13900/25000, Loss: 2.756974208750762e-05\n",
      "Epoch 3, Step 14000/25000, Loss: 0.0003122077032458037\n",
      "Epoch 3, Step 14100/25000, Loss: 0.0002439466625219211\n",
      "Epoch 3, Step 14200/25000, Loss: 2.3710468667559326e-05\n",
      "Epoch 3, Step 14300/25000, Loss: 1.5038771380204707e-05\n",
      "Epoch 3, Step 14400/25000, Loss: 3.857303818222135e-05\n",
      "Epoch 3, Step 14500/25000, Loss: 3.223528983653523e-05\n",
      "Epoch 3, Step 14600/25000, Loss: 0.00031459861202165484\n",
      "Epoch 3, Step 14700/25000, Loss: 3.522096449160017e-05\n",
      "Epoch 3, Step 14800/25000, Loss: 0.0002534194500185549\n",
      "Epoch 3, Step 14900/25000, Loss: 0.00030984467593953013\n",
      "Epoch 3, Step 15000/25000, Loss: 5.946516102994792e-05\n",
      "Epoch 3, Step 15100/25000, Loss: 3.469301373115741e-05\n",
      "Epoch 3, Step 15200/25000, Loss: 2.687577398319263e-05\n",
      "Epoch 3, Step 15300/25000, Loss: 4.643892316380516e-05\n",
      "Epoch 3, Step 15400/25000, Loss: 1.9806222553597763e-05\n",
      "Epoch 3, Step 15500/25000, Loss: 0.00017396119073964655\n",
      "Epoch 3, Step 15600/25000, Loss: 3.98893425881397e-05\n",
      "Epoch 3, Step 15700/25000, Loss: 0.0005258062737993896\n",
      "Epoch 3, Step 15800/25000, Loss: 6.399832636816427e-05\n",
      "Epoch 3, Step 15900/25000, Loss: 2.4440458219032735e-05\n",
      "Epoch 3, Step 16000/25000, Loss: 0.00019695468654390424\n",
      "Epoch 3, Step 16100/25000, Loss: 7.791513780830428e-05\n",
      "Epoch 3, Step 16200/25000, Loss: 3.8466845580842346e-05\n",
      "Epoch 3, Step 16300/25000, Loss: 3.4460674214642495e-05\n",
      "Epoch 3, Step 16400/25000, Loss: 3.3201267797267064e-05\n",
      "Epoch 3, Step 16500/25000, Loss: 0.0001381852343911305\n",
      "Epoch 3, Step 16600/25000, Loss: 0.0003518583544064313\n",
      "Epoch 3, Step 16700/25000, Loss: 8.371933654416353e-05\n",
      "Epoch 3, Step 16800/25000, Loss: 5.389354191720486e-05\n",
      "Epoch 3, Step 16900/25000, Loss: 0.00019760645227506757\n",
      "Epoch 3, Step 17000/25000, Loss: 3.503090556478128e-05\n",
      "Epoch 3, Step 17100/25000, Loss: 2.8884758648928255e-05\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import LSTM, Dense, RepeatVector, TimeDistributed, Layer, MultiHeadAttention\n",
    "from tensorflow.keras.layers import Bidirectional, Dropout\n",
    "from tensorflow.keras.layers import Masking, Input, Lambda\n",
    "import tensorflow_probability as tfp\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.losses import mse\n",
    "from numpy.fft import fft\n",
    "from scipy.stats import skew, kurtosis \n",
    "import pandas as pd\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import struct\n",
    "import glob\n",
    "from sklearn.metrics import confusion_matrix, roc_auc_score, roc_curve, classification_report, accuracy_score\n",
    "from tensorflow.keras.losses import MeanSquaredError\n",
    "\n",
    "# if tf.test.gpu_device_name():\n",
    "#     print('Default GPU Device: {}'.format(tf.test.gpu_device_name()))\n",
    "# else:\n",
    "#     print(\"Please install GPU version of TF\")\n",
    "class DataGenerator:        \n",
    "    def __init__(self, filepath, batch_size, sequence_length, max_samples=None, for_training=True):\n",
    "        self.filepath = filepath\n",
    "        self.batch_size = batch_size\n",
    "        self.sequence_length = sequence_length\n",
    "        self.max_samples = max_samples\n",
    "        self.for_training = for_training\n",
    "        self.samples = []\n",
    "        self.binary_file = open(self.filepath, 'rb')  # Initialize the binary_file here\n",
    "        self.reset()\n",
    "    def reset(self):\n",
    "        self.total_samples_processed = 0\n",
    "        _, self.file_extension = os.path.splitext(self.filepath)\n",
    "        print(f\"File extension detected: {self.file_extension}\")  # Add this line\n",
    "    def __iter__(self):\n",
    "        self.binary_file.seek(0)  # reset file pointer\n",
    "        self.samples = []\n",
    "        return self    \n",
    "    def close(self):\n",
    "        if not self.binary_file.closed:\n",
    "            self.binary_file.close()\n",
    "    def process_data1(self, samples):\n",
    "        real_parts = []\n",
    "        imag_parts = []\n",
    "        for sample in samples:\n",
    "            try:\n",
    "                cnum = complex(sample.replace('j', 'j'))\n",
    "                real_parts.append(np.real(cnum))\n",
    "                imag_parts.append(np.imag(cnum))\n",
    "            except ValueError:\n",
    "                continue\n",
    "\n",
    "        real_parts = (real_parts - np.mean(real_parts)) / np.std(real_parts)\n",
    "        imag_parts = (imag_parts - np.mean(imag_parts)) / np.std(imag_parts)\n",
    "\n",
    "        X = [list(zip(real_parts[i:i+self.sequence_length], imag_parts[i:i+self.sequence_length])) for i in range(len(real_parts) - self.sequence_length)]\n",
    "        return np.array(X)\n",
    "    def process_data2(self, samples):\n",
    "        # Convert samples list to a NumPy array and check the total number of samples\n",
    "        samples_array = np.array(samples, dtype=np.complex64)\n",
    "        total_samples = samples_array.size\n",
    "\n",
    "        # Ensure that the total number of samples matches self.batch_size * self.sequence_length\n",
    "        if total_samples != self.batch_size * self.sequence_length:\n",
    "            # Handle this scenario: you might want to raise an error or handle it in some way\n",
    "            raise ValueError(\"Total number of samples does not match batch_size * sequence_length\")\n",
    "\n",
    "        # Check for invalid values in samples_array before processing\n",
    "        if np.isnan(samples_array).any() or np.isinf(samples_array).any():\n",
    "            print(f\"Invalid values found in samples_array: {samples_array}\")\n",
    "        # Reshape the samples array\n",
    "        samples_array = samples_array.reshape(self.batch_size, self.sequence_length)\n",
    "        #print('samples_array.shape:', samples_array.shape)\n",
    "\n",
    "        # Apply FFT to convert time-domain signals into frequency domain\n",
    "        samples_fft = fft(samples_array)\n",
    "        # Extract real and imaginary parts\n",
    "        real_parts = np.real(samples_fft)\n",
    "        imag_parts = np.imag(samples_fft)     \n",
    "        # Normalize the real and imaginary parts\n",
    "        epsilon = 1e-10\n",
    "        real_parts_mean = np.mean(real_parts, axis=1, keepdims=True)\n",
    "        real_parts_std = np.std(real_parts, axis=1, keepdims=True)\n",
    "        real_parts_std[real_parts_std == 0] = epsilon  # Avoid division by zero\n",
    "        real_parts = (real_parts - real_parts_mean) / real_parts_std\n",
    "\n",
    "        imag_parts_mean = np.mean(imag_parts, axis=1, keepdims=True)\n",
    "        imag_parts_std = np.std(imag_parts, axis=1, keepdims=True)\n",
    "        imag_parts_std[imag_parts_std == 0] = epsilon  # Avoid division by zero\n",
    "        imag_parts = (imag_parts - imag_parts_mean) / imag_parts_std\n",
    "\n",
    "        # Extract statistical features from the real and imaginary parts\n",
    "        features = np.column_stack((\n",
    "            np.mean(real_parts, axis=1),\n",
    "            np.std(real_parts, axis=1),\n",
    "            skew(real_parts, axis=1),\n",
    "            kurtosis(real_parts, axis=1),\n",
    "            np.mean(imag_parts, axis=1),\n",
    "            np.std(imag_parts, axis=1),\n",
    "            skew(imag_parts, axis=1),\n",
    "            kurtosis(imag_parts, axis=1)\n",
    "        ))\n",
    "\n",
    "        # Reshape features to match the input shape of the model\n",
    "        X = features.reshape(-1, self.sequence_length, features.shape[1])\n",
    "        return X\n",
    "\n",
    "    def __next__(self):\n",
    "        samples = []\n",
    "        while len(samples) < self.batch_size:\n",
    "            binary_data = self.binary_file.read(8 * self.sequence_length)\n",
    "            if not binary_data:\n",
    "                break  # End of file or incomplete data\n",
    "            # Assume each data point consists of two floats (real and imaginary parts)\n",
    "            data_array = np.frombuffer(binary_data, dtype=np.float32).reshape(self.sequence_length, 2)\n",
    "            samples.append(data_array)\n",
    "    \n",
    "        if not samples:\n",
    "            raise StopIteration\n",
    "    \n",
    "        X_chunk = np.array(samples)  # This should now be of shape (batch_size, sequence_length, feature_dim)\n",
    "        if self.for_training:\n",
    "            return X_chunk, X_chunk  # Return input as both features and labels for autoencoding\n",
    "        return X_chunk\n",
    "\n",
    "\n",
    "    # def __next__(self):\n",
    "    #     chunksize = self.batch_size * self.sequence_length\n",
    "    #     global totalMagnitude  # Access the global variable\n",
    "    #     global totalnumberofsamples  # Access the global variable        \n",
    "    #     #if self.file_extension == '.dat':        \n",
    "    #     samples = []\n",
    "    #     while True:\n",
    "    #         binary_data = self.binary_file.read(8)\n",
    "    #         if not binary_data:\n",
    "    #             break  # End of file\n",
    "    #         decoded_data = struct.unpack('ff', binary_data)            \n",
    "    #         # Skip samples that are exactly zero (0 + 0j)\n",
    "    #         if decoded_data[0] == 0 and decoded_data[1] == 0:\n",
    "    #             continue\n",
    "    #         # Convert the binary data to a complex number string\n",
    "    #         decoded_line = f\"{decoded_data[0]}+{decoded_data[1]}j\\n\" if decoded_data[1] >= 0 else f\"{decoded_data[0]}{decoded_data[1]}j\\n\"\n",
    "    #         samples.append(decoded_line)\n",
    "    #         # Check if we have enough samples for a batch\n",
    "    #         if len(samples) == chunksize:\n",
    "    #             X_chunk = self.process_data1(samples)\n",
    "    #             #X_chunk = self.process_data2(samples)\n",
    "    #             print('X_chunk.shape:', X_chunk.shape)\n",
    "    #             if self.for_training:\n",
    "    #                 return X_chunk, X_chunk\n",
    "    #             else:\n",
    "    #                 return X_chunk\n",
    "    #             # Clear samples for the next batch (optional, depends on your logic)\n",
    "    #             samples = []\n",
    "#------------------------------------------------------------------------------------------------------\n",
    "# Minimum Entropy Coupling (MEC) Functions\n",
    "def mec_kocaoglu_np(p, q):\n",
    "    \"\"\"\n",
    "    Compute the joint distribution matrix with minimal entropy between two given distributions.\n",
    "    \"\"\"\n",
    "    p = tf.cast(p, tf.float64) / tf.reduce_sum(p)\n",
    "    q = tf.cast(q, tf.float64) / tf.reduce_sum(q)\n",
    "    J = tf.zeros((tf.size(q), tf.size(p)), dtype=tf.float64)\n",
    "    M = tf.stack([p, q], axis=0)\n",
    "    r = tf.reduce_min(tf.reduce_max(M, axis=1))\n",
    "    #print('Input shapes to mec_kocaoglu_np:', p.shape, q.shape)\n",
    "    def body(r, M, J):\n",
    "        a_i = tf.argmax(M, axis=1)\n",
    "        r_updated = tf.reduce_min(tf.reduce_max(M, axis=1))\n",
    "        update_values = tf.stack([r, r])\n",
    "        # ensure tensors have same datatype\n",
    "        a_i = tf.cast(a_i, dtype=tf.int32)\n",
    "        indices_range = tf.range(tf.size(a_i), dtype=tf.int32)\n",
    "        #prepare indices for scatter update\n",
    "        indices = tf.stack([indices_range, a_i], axis=1)\n",
    "        #now update\n",
    "        M_updates = tf.scatter_nd(indices, -update_values, tf.shape(M))\n",
    "        M = M + M_updates\n",
    "        J_updates = tf.scatter_nd(indices, [r, r], tf.shape(J))\n",
    "        J = J + J_updates\n",
    "        return r_updated, M, J\n",
    "\n",
    "    def condition(r, M, J):\n",
    "        return r > 0\n",
    "    r, M, J = tf.while_loop(condition, body, loop_vars=[r, M, J])\n",
    "    return J\n",
    "\n",
    "def apply_mec_to_data(data, num_bins=4, latent_dim=25):\n",
    "    print('data.shape in apply mec:', data.shape)\n",
    "    \"\"\"\n",
    "    Apply the MEC transformation to each sample in the data using tf.map_fn.\n",
    "    \"\"\"\n",
    "    def process_sample(sample):\n",
    "        min_val = tf.reduce_min(sample)\n",
    "        max_val = tf.reduce_max(sample)\n",
    "\n",
    "        # Adjust max_val to ensure a non-zero range using tf.cond\n",
    "        max_val = tf.cond(min_val == max_val,\n",
    "                          lambda: min_val + 1e-10,  # if true (min_val equals max_val)\n",
    "                          lambda: max_val)          # if false\n",
    "        \n",
    "        sample_distribution = tf.histogram_fixed_width(sample, [min_val, max_val], nbins=num_bins)\n",
    "        sample_distribution = tf.cast(sample_distribution, tf.float64)\n",
    "        sum_distribution = tf.cast(tf.reduce_sum(sample_distribution), tf.float64)\n",
    "        sample_distribution /= sum_distribution\n",
    "\n",
    "        mec_transformed = mec_kocaoglu_np(sample_distribution, sample_distribution)\n",
    "\n",
    "        # Flatten the 2D to 1D\n",
    "        if len(mec_transformed.shape) > 1:\n",
    "            transformed_sample = tf.reshape(mec_transformed, [-1])\n",
    "\n",
    "        # slice/pad to match the latent_dim\n",
    "        if transformed_sample.shape[0] > latent_dim:\n",
    "            transformed_sample = transformed_sample[:latent_dim]\n",
    "        elif transformed_sample.shape[0] < latent_dim:\n",
    "            padding = tf.zeros(latent_dim - transformed_sample.shape[0], dtype=tf.float64)\n",
    "            transformed_sample = tf.concat([transformed_sample, padding], axis=0)\n",
    "\n",
    "        return tf.reshape(transformed_sample, (latent_dim,))\n",
    "    # apply function to each sample in the batch\n",
    "    transformed_batch = tf.map_fn(process_sample, data, dtype=tf.float64, parallel_iterations=10)\n",
    "    return transformed_batch\n",
    "\n",
    "def process_latent_variables(z):\n",
    "    z_transformed = apply_mec_to_data(z)\n",
    "    print('Output of MEC transformation shape:', z_transformed.shape)\n",
    "    return z_transformed\n",
    "class SelfAttentionLayer(Layer):\n",
    "    def __init__(self, num_heads, key_dim):\n",
    "        super(SelfAttentionLayer, self).__init__()\n",
    "        self.multi_head_attention = MultiHeadAttention(num_heads=num_heads, key_dim=key_dim)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return self.multi_head_attention(inputs, inputs, inputs)\n",
    "# Variational Autoencoder (VAE) Class\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense, LSTM, Lambda, TimeDistributed\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "class VAE(Model):\n",
    "    def __init__(self, sequence_length, feature_dim, original_dim, intermediate_dim, latent_dim,\n",
    "                 epsilon_std=0.1, dropout_rate=0.2):\n",
    "        super(VAE, self).__init__()  # Initialize the superclass to integrate fully with Keras\n",
    "        self.sequence_length = sequence_length\n",
    "        self.feature_dim = feature_dim\n",
    "        self.original_dim = original_dim\n",
    "        self.intermediate_dim = intermediate_dim\n",
    "        self.latent_dim = latent_dim\n",
    "        self.epsilon_std = epsilon_std\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self._build()\n",
    "\n",
    "    def _build(self):\n",
    "        # Encoder\n",
    "        self.inputs = Input(shape=(self.sequence_length, self.feature_dim), name='encoder_input')\n",
    "        x = LSTM(100, activation='tanh', return_sequences=True)(self.inputs)\n",
    "        self_attention = SelfAttentionLayer(num_heads=2, key_dim=100)\n",
    "        x = self_attention(x)\n",
    "        x = LSTM(50, activation='tanh', return_sequences=False)(x)\n",
    "        self.z_mean = Dense(self.latent_dim, name='z_mean')(x)\n",
    "        self.z_log_var = Dense(self.latent_dim, name='z_log_var')(x)\n",
    "        z = Lambda(self._sampling3, output_shape=(self.latent_dim,))([self.z_mean, self.z_mean])\n",
    "        self.encoder = Model(self.inputs, [self.z_mean, self.z_log_var, z])\n",
    "\n",
    "        # Decoder\n",
    "        latent_inputs = Input(shape=(self.latent_dim,), name='z_sampling')\n",
    "        \n",
    "        x = RepeatVector(self.sequence_length)(latent_inputs)\n",
    "        x = LSTM(50, activation='tanh', return_sequences=True)(x)\n",
    "        x = LSTM(100, activation='tanh', return_sequences=True)(x)\n",
    "        final_activation = 'sigmoid'\n",
    "        \n",
    "        outputs = TimeDistributed(Dense(self.feature_dim))(x)\n",
    "        self.decoder = Model(latent_inputs, outputs, name='decoder')\n",
    "\n",
    "        # VAE Model\n",
    "        self.vae_outputs = self.decoder(z)\n",
    "        self.vae = Model(self.inputs, self.vae_outputs, name='vae_mlp')\n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "        z_mean, z_log_var, z = self.encoder(inputs)\n",
    "        return self.decoder(z)\n",
    "\n",
    "    # def compile(self, learning_rate=0.001, optimizer='adam'):\n",
    "    #     if optimizer == 'adam':\n",
    "    #         optimizer = Adam(learning_rate=learning_rate)\n",
    "    #     super().compile(optimizer=optimizer, loss=self.vae_loss)\n",
    "    def compile(self, learning_rate=0.001, optimizer='adam'):\n",
    "        if optimizer == 'adam':\n",
    "            optimizer = Adam(learning_rate=learning_rate)\n",
    "        mse_loss = MeanSquaredError()  # Instantiate the MSE loss function\n",
    "        super().compile(optimizer=optimizer, loss=mse_loss) \n",
    "\n",
    "    def _sampling(self, args):\n",
    "        z_mean, z_log_var = args\n",
    "        batch = K.shape(z_mean)[0]\n",
    "        dim = K.int_shape(z_mean)[1]\n",
    "        epsilon = K.random_normal(shape=(batch, dim), mean=0., stddev=self.epsilon_std)\n",
    "        output = z_mean + K.exp(0.5 * z_log_var) * epsilon\n",
    "        return output\n",
    "    def _sampling3(self, args):\n",
    "        z_mean, _ = args\n",
    "        z_mean_transformed = process_latent_variables(z_mean)\n",
    "        epsilon_std = 0.1\n",
    "        epsilon = K.random_normal(shape=K.shape(z_mean_transformed), mean=0., stddev=epsilon_std)\n",
    "        epsilon = tf.cast(epsilon, 'float64')        \n",
    "        #a = z_mean_transformed + epsilon\n",
    "        return z_mean_transformed\n",
    "\n",
    "    def vae_loss(self, x, x_decoded_mean, z_mean, z_log_var):\n",
    "        mse = tf.reduce_mean(tf.square(x - x_decoded_mean), axis=(1, 2))\n",
    "        xent_loss = mse\n",
    "        #kl_loss = -0.5 * K.sum(1 + z_log_var - K.square(z_mean) - K.exp(z_log_var), axis=-1)\n",
    "        #return K.mean(xent_loss + kl_loss)\n",
    "        return mse\n",
    "\n",
    "\n",
    "\n",
    "# Instantiate and Compile the VAE\n",
    "sequence_length = 10\n",
    "feature_dim = 2\n",
    "original_dim = 50\n",
    "intermediate_dim = 50\n",
    "latent_dim = 25\n",
    "\n",
    "\n",
    "vae_model = VAE(sequence_length, feature_dim, original_dim, intermediate_dim, latent_dim)\n",
    "vae_model.compile(learning_rate=0.005)\n",
    "\n",
    "\n",
    "# vae_model.compile(optimizer=Adam(learning_rate=0.005), loss='mse')\n",
    "\n",
    "# # Access the optimizer from the model for use in a custom training loop\n",
    "# optimizer = vae_model.optimizer\n",
    "\n",
    "\n",
    "# Model Training\n",
    "batch_size = 40\n",
    "max_train_samples = 10000000\n",
    "train_steps = max_train_samples // (batch_size * sequence_length)\n",
    "max_samples = 10000000  # Maximum samples to read (or None to read all)\n",
    "max_test_samples = 10000000\n",
    "\n",
    "# pure_file_pattern = '/home/mreza/5G accelerator/ID_MEC/data generator/pure_data/pure_iq_samples_*.csv'\n",
    "# mixed_file_pattern = '/home/mreza/5G accelerator/ID_MEC/data generator/mixed_data/mixed_iq_samples_*.csv'\n",
    "# pure_file_new = '/home/mreza/5G accelerator/ID_MEC/data generator/New Data-Collection/rx_IQ_pure'\n",
    "# mixed_file_new = '/home/mreza/5G accelerator/ID_MEC/data generator/New Data-Collection/rx_IQ_MIX'\n",
    "# pure_file_old = '/home/mreza/5G accelerator/IQ_samples/data collected/5G_DL_IQ_no_jamming_0924.dat'\n",
    "# mixed_file_old = '/home/mreza/5G accelerator/IQ_samples/data collected/5G_DL_IQ_with_periodic_jamming_0928_02.dat'\n",
    "\n",
    "pure_file_pattern = '/home/reza/My_models/Data/data generator/pure_data/pure_iq_samples_*.csv'\n",
    "mixed_file_pattern = '/home/reza/My_models/Data/data generator/mixed_data/mixed_iq_samples_*.csv'\n",
    "pure_file_new = '/home/reza/My_models/Data/data generator/New Data-Collection/rx_IQ_pure'\n",
    "mixed_file_new = '/home/reza/My_models/Data/data generator/New Data-Collection/rx_IQ_MIX'\n",
    "pure_file_old = '/home/reza/My_models/Data/data generator/first data/5G_DL_IQ_no_jamming_0924.dat'\n",
    "mixed_file_old = '/home/reza/My_models/Data/data generator/first data/5G_DL_IQ_with_periodic_jamming_0928_02.dat'\n",
    "\n",
    "# Data Generator Instances\n",
    "train_gen_instance = DataGenerator(pure_file_new,batch_size=batch_size, sequence_length=sequence_length, \n",
    "                                   max_samples=max_train_samples, for_training=True)\n",
    "combined_gen_instance = DataGenerator(mixed_file_new,batch_size=batch_size, sequence_length=sequence_length, \n",
    "                                      for_training=False)\n",
    "\n",
    "def generator():\n",
    "    produced_batches = 0\n",
    "    max_batches = max_train_samples // batch_size  # calculate how many batches you expect\n",
    "    while produced_batches < max_batches:\n",
    "        try:\n",
    "            yield next(train_gen_instance)\n",
    "            produced_batches += 1\n",
    "        except StopIteration:\n",
    "            train_gen_instance.reset()  # Reset generator if needed\n",
    "            continue\n",
    "\n",
    "dataset = tf.data.Dataset.from_generator(\n",
    "    generator,\n",
    "    output_signature=(\n",
    "        tf.TensorSpec(shape=(batch_size, sequence_length, feature_dim), dtype=tf.float32),\n",
    "        tf.TensorSpec(shape=(batch_size, sequence_length, feature_dim), dtype=tf.float32)\n",
    "    )\n",
    ")\n",
    "\n",
    "dataset = dataset.prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "for x_batch, y_batch in dataset.take(1):\n",
    "    print('x_batch.shape:', x_batch.shape)  # Expected: (batch_size, sequence_length, feature_dim)\n",
    "    print('y_batch.shape:', y_batch.shape)  # Expected: (batch_size, sequence_length, feature_dim)\n",
    "\n",
    "# Use this dataset in your training loop\n",
    "\n",
    "num_epochs = 12  # You can adjust the number of epochs as needed\n",
    "steps_per_epoch = train_steps  # Assuming one epoch processes all the data\n",
    "\n",
    "optimizer = Adam(learning_rate=0.005)\n",
    "def compute_loss(y_true, y_pred):\n",
    "    return tf.reduce_mean(tf.square(y_true - y_pred))\n",
    "\n",
    "@tf.function\n",
    "def train_step(model, x, y, optimizer):\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions = model(x, training=True)\n",
    "        loss = compute_loss(y, predictions)\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "    return loss\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print('epoch:', epoch)\n",
    "    step = 0  # Initialize step counter\n",
    "    total_steps = train_steps  # Assuming 'train_steps' is calculated as data_size / batch_size\n",
    "    for x_batch, y_batch in dataset:\n",
    "        loss = train_step(vae_model, x_batch, y_batch, optimizer)\n",
    "        if step % 100 == 0:  # Check if the step is a multiple of 100\n",
    "            print(f\"Epoch {epoch}, Step {step}/{total_steps}, Loss: {loss.numpy()}\")\n",
    "        step += 1  # Increment step counter\n",
    "        if step >= train_steps:\n",
    "            break\n",
    "        \n",
    "\n",
    "num_predictions = 500  # or any other large number\n",
    "print(f\"Number of predictions to be performed: {num_predictions}\")\n",
    "\n",
    "\n",
    "reconstruction_errors = []\n",
    "all_X_chunk_test = []\n",
    "all_X_chunk_pred = []\n",
    "all_intrusion_flags = []\n",
    "try:\n",
    "    for _ in range(num_predictions):\n",
    "        print('prediction number:', _)\n",
    "        X_chunk_test = next(combined_gen_instance)\n",
    "        #X_chunk_pred = model.predict(X_chunk_test)\n",
    "        X_chunk_pred = vae_model.vae.predict(X_chunk_test)\n",
    "        chunk_errors = np.mean(np.square(X_chunk_test - X_chunk_pred), axis=1)\n",
    "        reconstruction_errors.extend(chunk_errors)        \n",
    "        all_X_chunk_test.append(X_chunk_test)\n",
    "        all_X_chunk_pred.append(X_chunk_pred)\n",
    "except StopIteration:\n",
    "    print(\"All samples processed.\")\n",
    "\n",
    "\n",
    "reconstruction_error = np.array(reconstruction_errors)\n",
    "print('reconstruction_error.shape:', reconstruction_error.shape)\n",
    "print('Number of NaNs in reconstruction_error:', np.isnan(reconstruction_error).sum())\n",
    "max_error_per_sequence = reconstruction_error.max(axis=1) # Max error for each sequence\n",
    "print('max_error_per_sequence:', max_error_per_sequence)\n",
    "\n",
    "print('max_error_per_sequence.shape:', max_error_per_sequence.shape)\n",
    "\n",
    "threshold1 = np.percentile(max_error_per_sequence, 98)\n",
    "print('threshold1:', threshold1)\n",
    "threshold2 = np.percentile(reconstruction_error, 95)\n",
    "print('threshold percentile:', threshold2)\n",
    "\n",
    "is_intrusion_detected = max_error_per_sequence > threshold1  # Boolean array for sequences\n",
    "print('len(is_intrusion_detected):', len(is_intrusion_detected))\n",
    "print('is_intrusion_detected.shape:', is_intrusion_detected.shape)\n",
    "\n",
    "#is_intrusion_detected2 = error_per_sequence > threshold1\n",
    "\n",
    "num_total_sequences = len(max_error_per_sequence)\n",
    "num_total_sequences2 = num_predictions * batch_size - num_predictions\n",
    "print('num_total_sequences:', num_total_sequences)\n",
    "print('num_total_sequences2:', num_total_sequences2)\n",
    "\n",
    "#---------------------------------------finish 111-----------------------------------\n",
    "flat_error_per_sequence = max_error_per_sequence.flatten()\n",
    "#flat_error_per_sequence2 = error_per_sequence.flatten()\n",
    "# Determine if intrusion detected for each sequence\n",
    "for error in flat_error_per_sequence:\n",
    "    all_intrusion_flags.append(error > threshold1)    \n",
    "all_X_chunk_test = np.concatenate(all_X_chunk_test, axis=0)\n",
    "all_X_chunk_pred = np.concatenate(all_X_chunk_pred, axis=0)\n",
    "\n",
    "#save_path = 'C:\\\\Users\\\\Mohammadreza\\\\Desktop\\\\My Class\\\\Proj-DC\\\\My Works\\\\My Papers\\\\intrusion\\\\data generator\\\\intrusion_detected'\n",
    "#plot_with_intrusions8(all_X_chunk_test, all_X_chunk_pred, all_intrusion_flags, sequence_length, save_path)\n",
    "\n",
    "jamming_detected = reconstruction_error > threshold1\n",
    "train_gen_instance.close()\n",
    "combined_gen_instance.close()\n",
    "#Table\n",
    "flattened_jamming_detected = jamming_detected.flatten()\n",
    "real_part_detected = jamming_detected[:, 0]\n",
    "imag_part_detected = jamming_detected[:, 1]\n",
    "\n",
    "real_true_count = np.sum(real_part_detected)\n",
    "real_false_count = len(real_part_detected) - real_true_count\n",
    "\n",
    "imag_true_count = np.sum(imag_part_detected)\n",
    "imag_false_count = len(imag_part_detected) - imag_true_count\n",
    "# Overall\n",
    "overall_true_count = np.sum(flattened_jamming_detected)\n",
    "overall_false_count = len(flattened_jamming_detected) - overall_true_count\n",
    "# Table-DataFrame\n",
    "df = pd.DataFrame({\n",
    "    'Part': ['Real', 'Imaginary', 'Overall'],\n",
    "    'True Count': [real_true_count, imag_true_count, overall_true_count],\n",
    "    'False Count': [real_false_count, imag_false_count, overall_false_count]\n",
    "})\n",
    "print(df)\n",
    "num_jamming_detected = np.sum(jamming_detected)\n",
    "print(f\"Number of jamming sequences detected: {num_jamming_detected} out of {len(flattened_jamming_detected)} sequences\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50dd4f2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define the number of sequences to plot together\n",
    "n = 4  # Change this to desired number of sequences\n",
    "\n",
    "sample_length = sequence_length * n\n",
    "\n",
    "# Select a random starting sequence for plotting\n",
    "sequence_index = np.random.choice(len(X_chunk_test) - n + 1)\n",
    "\n",
    "# Extract and concatenate the original and reconstructed samples\n",
    "original_sample = np.concatenate(X_chunk_test[sequence_index:sequence_index + n])\n",
    "reconstructed_sample = np.concatenate(X_chunk_pred[sequence_index:sequence_index + n])\n",
    "\n",
    "# Plot concatenated sequences\n",
    "plt.figure(figsize=(14, 6)) # Decorate the training step with tf.function\n",
    "plt.plot(original_sample[:, 0], 'b-', label='Original Real Part')\n",
    "plt.plot(reconstructed_sample[:, 0], 'r--', label='Reconstructed Real Part')\n",
    "# plt.plot(original_sample[:, 1], 'orange', label='Original Real STD')\n",
    "# plt.plot(reconstructed_sample[:, 1], 'orange', label='Reconstructed Real STD', linestyle='--')\n",
    "\n",
    "plt.plot(original_sample[:, 1], 'y-', label='Original Imaginary Part')\n",
    "plt.plot(reconstructed_sample[:, 1], 'g--', label='Reconstructed Imaginary Part')\n",
    "# plt.plot(original_sample[:, 5], 'pink', label='Original Imaginary STD')\n",
    "# plt.plot(reconstructed_sample[:, 5], 'pink', label='Reconstructed Imaginary STD', linestyle='--')\n",
    "plt.title(f'Original vs Reconstructed IQ Data for {n} Sequences of Length {sequence_length}')\n",
    "plt.xlabel('Time Steps')\n",
    "plt.ylabel('Amplitude')\n",
    "plt.legend()\n",
    "# plt.savefig('9-Original vs Reconstructed IQ Data for {n} Sequences of Length {sequence_length}.png')\n",
    "# plt.close()if step >= train_steps:\n",
    "plt.show()\n",
    "\n",
    "# Repeat for n = 9\n",
    "n = 2  # Change this to desired number of sequences\n",
    "sequence_index = np.random.choice(len(X_chunk_test) - n + 1)\n",
    "original_sample = np.concatenate(X_chunk_test[sequence_index:sequence_index + n])\n",
    "reconstructed_sample = np.concatenate(X_chunk_pred[sequence_index:sequence_index + n])\n",
    "\n",
    "plt.figure(figsize=(14, 6))\n",
    "plt.plot(original_sample[:, 0], 'b-', label='Original Real Part')\n",
    "plt.plot(reconstructed_sample[:, 0], 'r--', label='Reconstructed Real Part')\n",
    "plt.plot(original_sample[:, 1], 'g-', label='Original Imaginary Part')\n",
    "plt.plot(reconstructed_sample[:, 1], 'y--', label='Reconstructed Imaginary Part')\n",
    "plt.title(f'Original vs Reconstructed IQ Data for {n} Sequences of Length {sequence_length}')\n",
    "plt.xlabel('Time Steps')\n",
    "plt.ylabel('Amplitude')\n",
    "plt.legend()\n",
    "# plt.savefig('11-Original vs Reconstructed IQ Data for {n} Sequences of Length {sequence_length}.png')\n",
    "# plt.close()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70ef53e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import gaussian_kde\n",
    "import seaborn as sns\n",
    "\n",
    "# Ensure compatibility for TensorFlow 2.x\n",
    "tf.compat.v1.enable_eager_execution()\n",
    "\n",
    "# Dummy data assuming z_mean and z_log_var have been obtained\n",
    "z_mean = tf.random.normal([100, 50])  # 100 samples, 50 dimensions\n",
    "z_log_var = tf.random.normal([100, 50])\n",
    "\n",
    "# Simplified version of your model's sampling functions for visualization purposes\n",
    "def sample_vae(z_mean, z_log_var, epsilon_std=0.1):\n",
    "    batch = tf.shape(z_mean)[0]\n",
    "    dim = tf.shape(z_mean)[1]\n",
    "    epsilon = tf.random.normal(shape=(batch, dim), mean=0., stddev=epsilon_std)\n",
    "    return z_mean + tf.exp(0.5 * z_log_var) * epsilon\n",
    "\n",
    "def sample_mec(z_mean, epsilon_std=0.1):\n",
    "    # Assuming process_latent_variables mimics the actual MEC transformation process\n",
    "    z_transformed = process_latent_variables(z_mean)  # Your actual MEC function\n",
    "    # Ensure epsilon has the same dtype as z_transformed\n",
    "    epsilon = tf.random.normal(shape=tf.shape(z_transformed), mean=0., stddev=epsilon_std, dtype=z_transformed.dtype)\n",
    "    return z_transformed + epsilon\n",
    "\n",
    "# Function to plot distributions\n",
    "def plot_distribution(tensor, label, color):\n",
    "    print('tensor:', tensor)\n",
    "    if isinstance(tensor, tf.Tensor):\n",
    "        tensor = tensor.numpy()  # Convert TensorFlow tensor to numpy array\n",
    "    tensor = tensor.flatten()  # Flatten the tensor/array\n",
    "    \n",
    "    plt.hist(tensor, bins=30, density=True, alpha=0.6, label=label, color=color)\n",
    "    \n",
    "    kde = gaussian_kde(tensor)\n",
    "    kde_x = np.linspace(tensor.min(), tensor.max(), 500)\n",
    "    kde_y = kde.evaluate(kde_x)\n",
    "    plt.plot(kde_x, kde_y, color=color, alpha=0.7)\n",
    "\n",
    "\n",
    "# Generate samples\n",
    "z_sampled_vae = sample_vae(z_mean, z_log_var)\n",
    "z_sampled_mec = sample_mec(z_mean)\n",
    "\n",
    "# Visualization\n",
    "plt.figure(figsize=(14, 6))\n",
    "plot_distribution(z_mean.numpy().flatten(), \"z_mean distribution\", \"blue\")\n",
    "plot_distribution(z_sampled_vae.numpy().flatten(), \"Sampled VAE z distribution\", \"red\")\n",
    "plot_distribution(z_sampled_mec.numpy().flatten(), \"Sampled MEC z distribution\", \"green\")\n",
    "\n",
    "plt.legend()\n",
    "plt.title('Latent Variable Distributions Before and After Sampling')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfa42b4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def extract_first_two_dimensions(latent_vars):\n",
    "    return latent_vars[:, :2].numpy()  # Extract first two dimensions and convert to numpy\n",
    "\n",
    "# Prepare the latent variables\n",
    "z_mean_2d = extract_first_two_dimensions(z_mean)\n",
    "z_sampled_vae_2d = extract_first_two_dimensions(z_sampled_vae)\n",
    "z_sampled_mec_2d = extract_first_two_dimensions(z_sampled_mec)\n",
    "\n",
    "def scatter_plots(latent_variables, labels, titles):\n",
    "    # Set up a figure with three subplots\n",
    "    fig, axs = plt.subplots(1, 3, figsize=(24, 10))\n",
    "    colors = ['red', 'orange', 'limegreen']\n",
    "\n",
    "    # First plot: z_mean vs. Sampled VAE\n",
    "    axs[0].scatter(latent_variables[:, 0, 0], latent_variables[:, 0, 1], color='red', s=50, label='z_mean')\n",
    "    axs[0].scatter(latent_variables[:, 1, 0], latent_variables[:, 1, 1], color='orange', s=50, label='Sampled VAE')\n",
    "    axs[0].set_title(titles[0])\n",
    "    axs[0].legend()\n",
    "    axs[0].grid(True)\n",
    "\n",
    "    # Second plot: z_mean vs. Sampled MEC\n",
    "    axs[1].scatter(latent_variables[:, 0, 0], latent_variables[:, 0, 1], color='red', s=50, label='z_mean')\n",
    "    axs[1].scatter(latent_variables[:, 2, 0], latent_variables[:, 2, 1], color='limegreen', s=50, label='Sampled MEC')\n",
    "    axs[1].set_title(titles[1])\n",
    "    axs[1].legend()\n",
    "    axs[1].grid(True)\n",
    "\n",
    "    # Third plot: Combined Latent Space\n",
    "    axs[2].scatter(latent_variables[:, 0, 0], latent_variables[:, 0, 1], color='red', s=50, label='z_mean')\n",
    "    axs[2].scatter(latent_variables[:, 1, 0], latent_variables[:, 1, 1], color='orange', s=50, label='Sampled VAE')\n",
    "    axs[2].scatter(latent_variables[:, 2, 0], latent_variables[:, 2, 1], color='limegreen', s=50, label='Sampled MEC')\n",
    "    axs[2].set_title(titles[2])\n",
    "    axs[2].legend()\n",
    "    axs[2].grid(True)\n",
    "\n",
    "    # Set axes labels\n",
    "    for ax in axs:\n",
    "        ax.set_xlabel('Latent Dimension 1')\n",
    "        ax.set_ylabel('Latent Dimension 2')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Prepare the latent variables for plotting\n",
    "latent_variables = np.stack([z_mean_2d, z_sampled_vae_2d, z_sampled_mec_2d], axis=1)\n",
    "labels = ['z_mean', 'Sampled VAE', 'Sampled MEC']\n",
    "titles = ['z_mean vs. Sampled VAE', 'z_mean vs. Sampled MEC', 'Combined Latent Space']\n",
    "\n",
    "# Call the plotting function\n",
    "scatter_plots(latent_variables, labels, titles)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99fa3329",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Convert TensorFlow tensors to numpy arrays\n",
    "z_mean_np = z_mean.numpy()\n",
    "z_sampled_vae_np = z_sampled_vae.numpy()\n",
    "z_sampled_mec_np = z_sampled_mec.numpy()\n",
    "\n",
    "# Only use the first two dimensions for the 2D scatter plot\n",
    "z_mean_np = z_mean_np[:, :2]\n",
    "z_sampled_vae_np = z_sampled_vae_np[:, :2]\n",
    "z_sampled_mec_np = z_sampled_mec_np[:, :2]\n",
    "\n",
    "# Create a DataFrame\n",
    "df = pd.DataFrame({\n",
    "    'Latent Dimension 1': np.concatenate([z_mean_np[:, 0], z_sampled_vae_np[:, 0], z_sampled_mec_np[:, 0]]),\n",
    "    'Latent Dimension 2': np.concatenate([z_mean_np[:, 1], z_sampled_vae_np[:, 1], z_sampled_mec_np[:, 1]]),\n",
    "    'Type': ['z_mean'] * len(z_mean_np) + ['Sampled VAE'] * len(z_sampled_vae_np) + ['Sampled MEC'] * len(z_sampled_mec_np)\n",
    "})\n",
    "\n",
    "def enhanced_scatter_plot(dataframe, hue, figsize=(12, 9), marker_sizes_dict=None,\n",
    "                          context='talk', style='whitegrid', palette='colorblind',\n",
    "                          title='Enhanced Latent Space 2D Distribution with Seaborn',\n",
    "                          extend_axis_factor=2):\n",
    "    # Set the aesthetics\n",
    "    sns.set(style=style, context=context, palette=palette)\n",
    "\n",
    "    plt.figure(figsize=figsize)\n",
    "    default_marker_size = 150\n",
    "\n",
    "    if marker_sizes_dict:\n",
    "        for category, msize in marker_sizes_dict.items():\n",
    "            subset = dataframe[dataframe[hue] == category]\n",
    "            sns.scatterplot(data=subset, x='Latent Dimension 1', y='Latent Dimension 2', \n",
    "                            hue=hue, s=msize, alpha=0.8, legend=False)\n",
    "    else:\n",
    "        sns.scatterplot(data=dataframe, x='Latent Dimension 1', y='Latent Dimension 2', \n",
    "                        hue=hue, s=default_marker_size, alpha=0.8)\n",
    "\n",
    "    x_min, x_max = plt.xlim()\n",
    "    y_min, y_max = plt.ylim()\n",
    "    plt.xlim(x_min * extend_axis_factor, x_max * extend_axis_factor)\n",
    "    plt.ylim(y_min * extend_axis_factor, y_max * extend_axis_factor)\n",
    "\n",
    "    plt.legend(title=hue, title_fontsize='13', fontsize='12', loc='upper left', bbox_to_anchor=(1, 1))\n",
    "    plt.xlabel('Latent Dimension 1', fontsize=15)\n",
    "    plt.ylabel('Latent Dimension 2', fontsize=15)\n",
    "    plt.title(title, fontsize=18)\n",
    "    sns.despine(trim=True)\n",
    "    plt.show()\n",
    "    \n",
    "def enhanced_scatter_plot(dataframe, hue, figsize=(12, 9), marker_sizes_dict=None,\n",
    "                          context='talk', style='whitegrid', palette=None,\n",
    "                          title='Enhanced Latent Space 2D Distribution with Seaborn',\n",
    "                          extend_axis_factor=2):\n",
    "    # Set the aesthetics\n",
    "    sns.set(style=style, context=context)\n",
    "\n",
    "    plt.figure(figsize=figsize)\n",
    "\n",
    "    # Define the color palette manually if not provided\n",
    "    if palette is None:\n",
    "        palette = {\n",
    "            'z_mean': 'red',\n",
    "            'Sampled VAE': 'orange',\n",
    "            'Sampled MEC': 'limegreen'\n",
    "        }\n",
    "    \n",
    "    # Define default marker size if not provided\n",
    "    if marker_sizes_dict is None:\n",
    "        marker_sizes_dict = {\n",
    "            'z_mean': 100,        # Default marker size for z_mean\n",
    "            'Sampled VAE': 100,   # Default marker size for Sampled VAE\n",
    "            'Sampled MEC': 150    # Default marker size for Sampled MEC\n",
    "        }\n",
    "\n",
    "    # Plot each subset with its specified marker size\n",
    "    for category, msize in marker_sizes_dict.items():\n",
    "        subset = dataframe[dataframe[hue] == category]\n",
    "        sns.scatterplot(data=subset, x='Latent Dimension 1', y='Latent Dimension 2',\n",
    "                        hue=hue, palette=palette, s=msize, alpha=0.8, legend=False)\n",
    "    \n",
    "    # Plot the legend separately\n",
    "    for category in marker_sizes_dict.keys():\n",
    "        plt.scatter([], [], color=palette[category], s=marker_sizes_dict[category],\n",
    "                    label=category, alpha=0.8)\n",
    "    \n",
    "    x_min, x_max = plt.xlim()\n",
    "    y_min, y_max = plt.ylim()\n",
    "    plt.xlim(x_min * extend_axis_factor, x_max * extend_axis_factor)\n",
    "    plt.ylim(y_min * extend_axis_factor, y_max * extend_axis_factor)\n",
    "\n",
    "    plt.legend(title=hue, title_fontsize='13', fontsize='12', loc='upper left', bbox_to_anchor=(1, 1))\n",
    "    plt.xlabel('Latent Dimension 1', fontsize=15)\n",
    "    plt.ylabel('Latent Dimension 2', fontsize=15)\n",
    "    plt.title(title, fontsize=18)\n",
    "    sns.despine(trim=True)\n",
    "    plt.show()\n",
    "\n",
    "# Call the function with custom settings\n",
    "enhanced_scatter_plot(df, hue='Type', extend_axis_factor=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "372d49c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# reconstruction error\n",
    "plt.figure(figsize=(14, 6))\n",
    "plt.plot(reconstruction_error, label='Reconstruction Error')\n",
    "plt.axhline(y=threshold2, color='r', linestyle='--', label='Threshold')\n",
    "plt.title('Reconstruction Error with Threshold')\n",
    "plt.xlabel('Sequence Number')\n",
    "plt.ylabel('Reconstruction Error')\n",
    "plt.legend()\n",
    "# plt.savefig('1-Reconstruction Error with Threshold.png')\n",
    "# plt.close()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4348bb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flatten the reconstruction_error to 1D\n",
    "reconstruction_error_flat = reconstruction_error.flatten()\n",
    "# reconstruction error\n",
    "plt.figure(figsize=(14, 6))\n",
    "plt.plot(reconstruction_error_flat, label='Reconstruction Error')\n",
    "plt.axhline(y=threshold2, color='r', linestyle='--', label='Threshold')\n",
    "plt.title('Reconstruction Error with Threshold')\n",
    "plt.xlabel('Sequence Number')\n",
    "plt.ylabel('Reconstruction Error')\n",
    "plt.legend()\n",
    "# plt.savefig('1-Reconstruction Error with Threshold.png')\n",
    "# plt.close()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab0e1a56",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Histogram of Reconstruction Errors:\n",
    "plt.figure(figsize=(14, 6))\n",
    "plt.hist(reconstruction_error, bins=50, alpha=0.75)\n",
    "plt.axvline(x=threshold2, color='r', linestyle='--', label='Threshold')\n",
    "plt.title('Histogram of Reconstruction Errors')\n",
    "plt.xlabel('Reconstruction Error')\n",
    "plt.ylabel('Frequency')\n",
    "plt.legend()\n",
    "# plt.savefig('4-Histogram of Reconstruction Errors.png')\n",
    "# plt.close()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39a86f70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flatten the reconstruction_error to 1D\n",
    "reconstruction_error_flat = reconstruction_error.flatten()\n",
    "#Histogram of Reconstruction Errors:\n",
    "plt.figure(figsize=(14, 6))\n",
    "plt.hist(reconstruction_error_flat, bins=50, alpha=0.75)\n",
    "plt.axvline(x=threshold2, color='r', linestyle='--', label='Threshold')\n",
    "plt.title('Histogram of Reconstruction Errors')\n",
    "plt.xlabel('Reconstruction Error')\n",
    "plt.ylabel('Frequency')\n",
    "plt.legend()\n",
    "# plt.savefig('4-Histogram of Reconstruction Errors.png')\n",
    "# plt.close()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c4005b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Time Series Plot of IQ Samples:\n",
    "sample_index = np.random.choice(len(X_chunk_test))\n",
    "original_sample = X_chunk_test[sample_index]\n",
    "reconstructed_sample = X_chunk_pred[sample_index]\n",
    "\n",
    "plt.figure(figsize=(14, 6))\n",
    "plt.plot(original_sample[:, 0], 'b-', label='Original Real Part')\n",
    "plt.plot(reconstructed_sample[:, 0], 'r--', label='Reconstructed Real Part')\n",
    "plt.plot(original_sample[:, 1], 'g-', label='Original Imaginary Part')\n",
    "plt.plot(reconstructed_sample[:, 1], 'y--', label='Reconstructed Imaginary Part')\n",
    "plt.title('Original vs Reconstructed IQ Data')\n",
    "plt.xlabel('Time Steps')\n",
    "plt.ylabel('Amplitude')\n",
    "plt.legend()\n",
    "# plt.savefig('5-Original vs Reconstructed IQ Data.png')\n",
    "# plt.close()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01e87830",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scatter Plot of Reconstruction Errors vs. Real and Imaginary Parts:\n",
    "avg_real = np.mean(X_chunk_test, axis=1)[:, 0]\n",
    "avg_imag = np.mean(X_chunk_test, axis=1)[:, 1]\n",
    "\n",
    "last_errors = np.mean(reconstruction_errors[-len(X_chunk_test):], axis=1)\n",
    "\n",
    "print(\"Shape of avg_real:\", avg_real.shape)\n",
    "print(\"Shape of avg_imag:\", avg_imag.shape)\n",
    "print(\"Shape of last_errors:\", len(last_errors))\n",
    "\n",
    "\n",
    "plt.figure(figsize=(14, 6))\n",
    "plt.scatter(avg_real, last_errors, label='Real Part', alpha=0.5)\n",
    "plt.axhline(y=threshold2, color='r', linestyle='--', label='Threshold')\n",
    "plt.title('Reconstruction Error vs. Average Real Part')\n",
    "plt.xlabel('Average Amplitude')\n",
    "plt.ylabel('Reconstruction Error')\n",
    "plt.legend()\n",
    "# plt.savefig('6-Reconstruction Error vs. Average Real Part.png')\n",
    "# plt.close()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(14, 6))\n",
    "plt.scatter(avg_imag, last_errors, label='Imaginary Part', alpha=0.5)\n",
    "plt.axhline(y=threshold2, color='r', linestyle='--', label='Threshold')\n",
    "plt.title('Reconstruction Error vs. Average Imaginary Part')\n",
    "plt.xlabel('Average Amplitude')\n",
    "plt.ylabel('Reconstruction Error')\n",
    "plt.legend()\n",
    "# plt.savefig('7-Reconstruction Error vs. Average Imaginary Part.png')\n",
    "# plt.close()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5d4235d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define the number of sequences to plot together\n",
    "n = 4  # Change this to desired number of sequences\n",
    "sample_length = sequence_length * n\n",
    "\n",
    "# Select a random starting sequence for plotting\n",
    "sequence_index = np.random.choice(len(X_chunk_test) - n + 1)\n",
    "\n",
    "# Extract and concatenate the original and reconstructed samples\n",
    "original_sample = np.concatenate(X_chunk_test[sequence_index:sequence_index + n])\n",
    "reconstructed_sample = np.concatenate(X_chunk_pred[sequence_index:sequence_index + n])\n",
    "\n",
    "# Plot concatenated sequences\n",
    "plt.figure(figsize=(14, 6))\n",
    "plt.plot(original_sample[:, 0], 'b-', label='Original Real Part')\n",
    "plt.plot(reconstructed_sample[:, 0], 'r--', label='Reconstructed Real Part')\n",
    "plt.plot(original_sample[:, 1], 'g-', label='Original Img Part')\n",
    "plt.plot(reconstructed_sample[:, 1], 'y--', label='Reconstructed Img Part')\n",
    "plt.title(f'Original vs Reconstructed IQ Data for {n} Sequences of Length {sequence_length}')\n",
    "plt.xlabel('Time Steps')\n",
    "plt.ylabel('Amplitude')\n",
    "plt.legend()\n",
    "# plt.savefig('9-Original vs Reconstructed IQ Data for {n} Sequences of Length {sequence_length}.png')\n",
    "# plt.close()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "640153f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b86f49a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define the number of sequences to plot together\n",
    "n = 4  # Change this to desired number of sequences\n",
    "sample_length = sequence_length * n\n",
    "\n",
    "# Select a random starting sequence for plotting\n",
    "sequence_index = np.random.choice(len(X_chunk_test) - n + 1)\n",
    "\n",
    "# Extract and concatenate the original and reconstructed samples\n",
    "original_sample = np.concatenate(X_chunk_test[sequence_index:sequence_index + n])\n",
    "reconstructed_sample = np.concatenate(X_chunk_pred[sequence_index:sequence_index + n])\n",
    "\n",
    "# Plot concatenated sequences\n",
    "plt.figure(figsize=(14, 6))\n",
    "plt.plot(original_sample[:, 0], 'b-', label='Original Real Part')\n",
    "plt.plot(reconstructed_sample[:, 0], 'r--', label='Reconstructed Real Part')\n",
    "# plt.plot(original_sample[:, 1], 'orange', label='Original Real STD')\n",
    "# plt.plot(reconstructed_sample[:, 1], 'orange', label='Reconstructed Real STD', linestyle='--')\n",
    "\n",
    "plt.plot(original_sample[:, 1], 'y-', label='Original Imaginary Part')\n",
    "plt.plot(reconstructed_sample[:, 1], 'g--', label='Reconstructed Imaginary Part')\n",
    "# plt.plot(original_sample[:, 5], 'pink', label='Original Imaginary STD')\n",
    "# plt.plot(reconstructed_sample[:, 5], 'pink', label='Reconstructed Imaginary STD', linestyle='--')\n",
    "plt.title(f'Original vs Reconstructed IQ Data for {n} Sequences of Length {sequence_length}')\n",
    "plt.xlabel('Time Steps')\n",
    "plt.ylabel('Amplitude')\n",
    "plt.legend()\n",
    "# plt.savefig('9-Original vs Reconstructed IQ Data for {n} Sequences of Length {sequence_length}.png')\n",
    "# plt.close()\n",
    "plt.show()\n",
    "\n",
    "# Repeat for n = 9\n",
    "n = 2  # Change this to desired number of sequences\n",
    "sequence_index = np.random.choice(len(X_chunk_test) - n + 1)\n",
    "original_sample = np.concatenate(X_chunk_test[sequence_index:sequence_index + n])\n",
    "reconstructed_sample = np.concatenate(X_chunk_pred[sequence_index:sequence_index + n])\n",
    "\n",
    "plt.figure(figsize=(14, 6))\n",
    "plt.plot(original_sample[:, 0], 'b-', label='Original Real Part')\n",
    "plt.plot(reconstructed_sample[:, 0], 'r--', label='Reconstructed Real Part')\n",
    "plt.plot(original_sample[:, 1], 'g-', label='Original Imaginary Part')\n",
    "plt.plot(reconstructed_sample[:, 1], 'y--', label='Reconstructed Imaginary Part')\n",
    "plt.title(f'Original vs Reconstructed IQ Data for {n} Sequences of Length {sequence_length}')\n",
    "plt.xlabel('Time Steps')\n",
    "plt.ylabel('Amplitude')\n",
    "plt.legend()\n",
    "# plt.savefig('11-Original vs Reconstructed IQ Data for {n} Sequences of Length {sequence_length}.png')\n",
    "# plt.close()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfd97dbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Repeat for n = 9\n",
    "n = 2  # Change this to desired number of sequences\n",
    "sequence_index = np.random.choice(len(X_chunk_test) - n + 1)\n",
    "original_sample = np.concatenate(X_chunk_test[sequence_index:sequence_index + n])\n",
    "reconstructed_sample = np.concatenate(X_chunk_pred[sequence_index:sequence_index + n])\n",
    "\n",
    "plt.figure(figsize=(14, 6))\n",
    "plt.plot(original_sample[:, 0], 'b-', label='Original Real Part')\n",
    "plt.plot(reconstructed_sample[:, 0], 'r--', label='Reconstructed Real Part')\n",
    "\n",
    "plt.title(f'Original vs Reconstructed IQ Data for {n} Sequences of Length {sequence_length}')\n",
    "plt.xlabel('Time Steps')\n",
    "plt.ylabel('Amplitude')\n",
    "plt.legend()\n",
    "# plt.savefig('11-Original vs Reconstructed IQ Data for {n} Sequences of Length {sequence_length}.png')\n",
    "# plt.close()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63145cc9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a6c47fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Repeat for n = 9\n",
    "n = 1  # Change this to desired number of sequences\n",
    "sequence_index = np.random.choice(len(X_chunk_test) - n + 1)\n",
    "original_sample = np.concatenate(X_chunk_test[sequence_index:sequence_index + n])\n",
    "reconstructed_sample = np.concatenate(X_chunk_pred[sequence_index:sequence_index + n])\n",
    "\n",
    "plt.figure(figsize=(14, 6))\n",
    "plt.plot(original_sample[:, 0], 'b-', label='Original Real Part')\n",
    "plt.plot(reconstructed_sample[:, 0], 'r--', label='Reconstructed Real Part')\n",
    "plt.plot(original_sample[:, 1], 'g-', label='Original Imaginary Part')\n",
    "plt.plot(reconstructed_sample[:, 1], 'y--', label='Reconstructed Imaginary Part')\n",
    "plt.title(f'Original vs Reconstructed IQ Data for {n} Sequences of Length {sequence_length}')\n",
    "plt.xlabel('Time Steps')\n",
    "plt.ylabel('Amplitude')\n",
    "plt.legend()\n",
    "# plt.savefig('11-Original vs Reconstructed IQ Data for {n} Sequences of Length {sequence_length}.png')\n",
    "# plt.close()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0025906c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reconstruction error\n",
    "reconstruction_error_real = reconstruction_error[:, 0]\n",
    "reconstruction_error_imag = reconstruction_error[:, 1]\n",
    "\n",
    "# Plot for Real Part\n",
    "plt.figure(figsize=(14, 6))\n",
    "mellow_green = '#89C997' \n",
    "plt.plot(reconstruction_error_real, label='Reconstruction Error', color=mellow_green)\n",
    "plt.axhline(y=threshold2, color='r', linestyle='--', label='Threshold')\n",
    "plt.title('Intrusion Detected by Reconstruction Error',fontsize=16, fontweight='bold')\n",
    "plt.xlabel('Sequence Number (×10³)', fontsize=16, fontweight='bold')\n",
    "#plt.xlabel('Sequence Number(*1000)', fontsize=16, fontweight='bold')\n",
    "plt.ylabel('Reconstruction Error', fontsize=16, fontweight='bold')\n",
    "for label in (plt.gca().get_xticklabels() + plt.gca().get_yticklabels()):\n",
    "    label.set_fontsize(12)\n",
    "    label.set_fontweight('bold')\n",
    "plt.legend(fontsize=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea42b73d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_index = np.random.choice(len(X_chunk_test))\n",
    "original_sample = X_chunk_test[sample_index]\n",
    "reconstructed_sample = X_chunk_pred[sample_index]\n",
    "\n",
    "plt.figure(figsize=(16, 8))\n",
    "plt.plot(original_sample[:, 0], 'b-', label='Original Real Part')\n",
    "plt.plot(reconstructed_sample[:, 0], 'b--', label='Reconstructed Real Part')\n",
    "# plt.plot(original_sample[:, 1], 'm-', label='Original Real STD')\n",
    "# plt.plot(reconstructed_sample[:, 1], 'm--', label='Reconstructed Real STD')\n",
    "# plt.plot(original_sample[:, 2], 'c-', label='Original Real Skew')\n",
    "# plt.plot(reconstructed_sample[:, 2], 'c--', label='Reconstructed Real Skew')\n",
    "# plt.plot(original_sample[:, 3], 'orange', label='Original Real Kurtosis')\n",
    "# plt.plot(reconstructed_sample[:, 3], 'orange', label='Reconstructed Real Kurtosis', linestyle='--')\n",
    "\n",
    "plt.plot(original_sample[:, 1], 'g-', label='Original Imaginary Part')\n",
    "plt.plot(reconstructed_sample[:, 1], 'g--', label='Reconstructed Imaginary Part')\n",
    "# plt.plot(original_sample[:, 5], 'purple', label='Original Imaginary STD')\n",
    "# plt.plot(reconstructed_sample[:, 5], 'purple', label='Reconstructed Imaginary STD', linestyle='--')\n",
    "# plt.plot(original_sample[:, 6], 'brown', label='Original Imaginary Skew')\n",
    "# plt.plot(reconstructed_sample[:, 6], 'brown', label='Reconstructed Imaginary Skew', linestyle='--')\n",
    "# plt.plot(original_sample[:, 7], 'pink', label='Original Imaginary Kurtosis')\n",
    "# plt.plot(reconstructed_sample[:, 7], 'pink', label='Reconstructed Imaginary Kurtosis', linestyle='--')\n",
    "plt.title('Original vs Reconstructed IQ Data')\n",
    "plt.xlabel('Time Steps')\n",
    "plt.ylabel('Amplitude')\n",
    "\n",
    "# Place the legend outside the plot area\n",
    "plt.legend(loc='upper left', bbox_to_anchor=(1, 1), fontsize='small', title='Legend')\n",
    "\n",
    "# Adjust layout to make room for the legend\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad266e00",
   "metadata": {},
   "outputs": [],
   "source": [
    "reconstruction_error_real_parts = reconstruction_error[:, 0]\n",
    "reconstruction_error_real_std = reconstruction_error[:, 1]\n",
    "reconstruction_error_real_skew = reconstruction_error[:, 2]\n",
    "reconstruction_error_real_kurtosis = reconstruction_error[:, 3]\n",
    "reconstruction_error_imag_parts = reconstruction_error[:, 4]\n",
    "reconstruction_error_imag_std = reconstruction_error[:, 5]\n",
    "reconstruction_error_imag_skew = reconstruction_error[:, 6]\n",
    "reconstruction_error_imag_kurtosis = reconstruction_error[:, 7]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bafc4676",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_index = np.random.choice(len(X_chunk_test))\n",
    "original_sample = X_chunk_test[sample_index]\n",
    "reconstructed_sample = X_chunk_pred[sample_index]\n",
    "\n",
    "plt.figure(figsize=(16, 8))\n",
    "# plt.plot(original_sample[:, 0], 'b-', label='Original Real Part')\n",
    "# plt.plot(reconstructed_sample[:, 0], 'b--', label='Reconstructed Real Part')\n",
    "# plt.plot(original_sample[:, 1], 'm-', label='Original Real STD')\n",
    "# plt.plot(reconstructed_sample[:, 1], 'm--', label='Reconstructed Real STD')\n",
    "plt.plot(original_sample[:, 2], 'c-', label='Original Real Skew')\n",
    "plt.plot(reconstructed_sample[:, 2], 'c--', label='Reconstructed Real Skew')\n",
    "plt.plot(original_sample[:, 3], 'orange', label='Original Real Kurtosis')\n",
    "plt.plot(reconstructed_sample[:, 3], 'orange', label='Reconstructed Real Kurtosis', linestyle='--')\n",
    "\n",
    "# plt.plot(original_sample[:, 4], 'g-', label='Original Imaginary Part')\n",
    "# plt.plot(reconstructed_sample[:, 4], 'g--', label='Reconstructed Imaginary Part')\n",
    "# plt.plot(original_sample[:, 5], 'purple', label='Original Imaginary STD')\n",
    "# plt.plot(reconstructed_sample[:, 5], 'purple', label='Reconstructed Imaginary STD', linestyle='--')\n",
    "plt.plot(original_sample[:, 6], 'brown', label='Original Imaginary Skew')\n",
    "plt.plot(reconstructed_sample[:, 6], 'brown', label='Reconstructed Imaginary Skew', linestyle='--')\n",
    "plt.plot(original_sample[:, 7], 'pink', label='Original Imaginary Kurtosis')\n",
    "plt.plot(reconstructed_sample[:, 7], 'pink', label='Reconstructed Imaginary Kurtosis', linestyle='--')\n",
    "plt.title('Original vs Reconstructed IQ Data')\n",
    "plt.xlabel('Time Steps')\n",
    "plt.ylabel('Amplitude')\n",
    "\n",
    "# Place the legend outside the plot area\n",
    "plt.legend(loc='upper left', bbox_to_anchor=(1, 1), fontsize='small', title='Legend')\n",
    "\n",
    "# Adjust layout to make room for the legend\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f704988c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define the number of sequences to plot together\n",
    "n = 2  # Change this to desired number of sequences\n",
    "sample_length = sequence_length * n\n",
    "\n",
    "# Select a random starting sequence for plotting\n",
    "sequence_index = np.random.choice(len(X_chunk_test) - n + 1)\n",
    "\n",
    "# Extract and concatenate the original and reconstructed samples\n",
    "original_sample = np.concatenate(X_chunk_test[sequence_index:sequence_index + n])\n",
    "reconstructed_sample = np.concatenate(X_chunk_pred[sequence_index:sequence_index + n])\n",
    "\n",
    "# Plot concatenated sequences\n",
    "plt.figure(figsize=(14, 6))\n",
    "plt.plot(original_sample[:, 0], 'b-', label='Original Real Part')\n",
    "plt.plot(reconstructed_sample[:, 0], 'r--', label='Reconstructed Real Part')\n",
    "plt.plot(original_sample[:, 1], 'orange', label='Original Real STD')\n",
    "plt.plot(reconstructed_sample[:, 1], 'orange', label='Reconstructed Real STD', linestyle='--')\n",
    "\n",
    "plt.plot(original_sample[:, 4], 'y-', label='Original Imaginary Part')\n",
    "plt.plot(reconstructed_sample[:, 4], 'g--', label='Reconstructed Imaginary Part')\n",
    "plt.plot(original_sample[:, 5], 'pink', label='Original Imaginary STD')\n",
    "plt.plot(reconstructed_sample[:, 5], 'pink', label='Reconstructed Imaginary STD', linestyle='--')\n",
    "plt.title(f'Original vs Reconstructed IQ Data for {n} Sequences of Length {sequence_length}')\n",
    "plt.xlabel('Time Steps')\n",
    "plt.ylabel('Amplitude')\n",
    "plt.legend()\n",
    "# plt.savefig('9-Original vs Reconstructed IQ Data for {n} Sequences of Length {sequence_length}.png')\n",
    "# plt.close()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef63d687",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Repeat for n = 9\n",
    "n = 4  # Change this to desired number of sequences\n",
    "sequence_index = np.random.choice(len(X_chunk_test) - n + 1)\n",
    "original_sample = np.concatenate(X_chunk_test[sequence_index:sequence_index + n])\n",
    "reconstructed_sample = np.concatenate(X_chunk_pred[sequence_index:sequence_index + n])\n",
    "\n",
    "plt.figure(figsize=(14, 6))\n",
    "plt.plot(original_sample[:, 2], 'b-', label='Original Real Part Skew')\n",
    "plt.plot(reconstructed_sample[:, 2], 'r--', label='Reconstructed Real Part Skew')\n",
    "plt.plot(original_sample[:, 6], 'g-', label='Original Imaginary Part Skew')\n",
    "plt.plot(reconstructed_sample[:, 6], 'y--', label='Reconstructed Imaginary Part Skew')\n",
    "plt.title(f'Original vs Reconstructed IQ Data for {n} Sequences of Length {sequence_length}')\n",
    "plt.xlabel('Time Steps')\n",
    "plt.ylabel('Amplitude')\n",
    "plt.legend()\n",
    "# plt.savefig('11-Original vs Reconstructed IQ Data for {n} Sequences of Length {sequence_length}.png')\n",
    "# plt.close()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0bf2a7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Repeat for n = 9\n",
    "n = 4  # Change this to desired number of sequences\n",
    "sequence_index = np.random.choice(len(X_chunk_test) - n + 1)\n",
    "original_sample = np.concatenate(X_chunk_test[sequence_index:sequence_index + n])\n",
    "reconstructed_sample = np.concatenate(X_chunk_pred[sequence_index:sequence_index + n])\n",
    "\n",
    "plt.figure(figsize=(14, 6))\n",
    "plt.plot(original_sample[:, 3], 'b-', label='Original Real Part Kurtosis')\n",
    "plt.plot(reconstructed_sample[:, 3], 'r--', label='Reconstructed Real Part Kurtosis')\n",
    "plt.plot(original_sample[:, 7], 'g-', label='Original Imaginary Part Kurtosis')\n",
    "plt.plot(reconstructed_sample[:, 7], 'y--', label='Reconstructed Imaginary Part Kurtosis')\n",
    "plt.title(f'Original vs Reconstructed IQ Data for {n} Sequences of Length {sequence_length}')\n",
    "plt.xlabel('Time Steps')\n",
    "plt.ylabel('Amplitude')\n",
    "plt.legend()\n",
    "# plt.savefig('11-Original vs Reconstructed IQ Data for {n} Sequences of Length {sequence_length}.png')\n",
    "# plt.close()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "671af7c6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
